{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wbZI8kPkqcoi"
      },
      "outputs": [],
      "source": [
        "from gtfs_new_functions import * # i should have modified the gtfs_functions package due to an error\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import geopandas as gpd\n",
        "import folium\n",
        "from shapely.geometry import Point, LineString, Polygon, shape\n",
        "from folium.plugins import MarkerCluster\n",
        "from google.colab import output\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "import plotly.graph_objs as go\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.cm as cm\n",
        "from geopy.distance import geodesic\n",
        "from math import radians, sin, cos, sqrt, atan2\n",
        "import numpy as np\n",
        "import shapefile as shp\n",
        "import seaborn as sns\n",
        "import os\n",
        "import copy\n",
        "import fiona\n",
        "import csv\n",
        "import math\n",
        "import colorsys\n",
        "from folium.plugins import HeatMap\n",
        "import branca.colormap as cmb\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import partridge as ptg\n",
        "from shapely.geometry import mapping\n",
        "import heapq\n",
        "from folium.features import DivIcon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOozEGComA6C"
      },
      "source": [
        "#####BKK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjGocdufqjnb"
      },
      "outputs": [],
      "source": [
        "feed_bkk = Feed(\"budapest_gtfs.zip\", time_windows=[6, 7]) #itt változtathatunk az időintervallumon,\n",
        "#feed = Feed(\"budapest_gtfs.zip\")\n",
        "routes_bkk = feed_bkk.routes\n",
        "trips_bkk = feed_bkk.trips\n",
        "stops_bkk = feed_bkk.stops\n",
        "stop_times_bkk = feed_bkk.stop_times\n",
        "shapes_bkk = feed_bkk.shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwsioW_axDe8"
      },
      "outputs": [],
      "source": [
        "segments_gdf_bkk = feed_bkk.segments\n",
        "segments_gdf_bkk.head(2)\n",
        "#segments_gdf_bkk.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVbjxFKgxD8M"
      },
      "outputs": [],
      "source": [
        "segments_freq_bkk = feed_bkk.segments_freq\n",
        "segments_freq_bkk.head(2)\n",
        "#print(type(segments_freq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJ3KdPOUOSBZ"
      },
      "outputs": [],
      "source": [
        "colnames=['route_name', 'capacity']\n",
        "df = pd.read_csv('bkk_capacity.csv', names=colnames, header=None)\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouG-1nlhRuvS"
      },
      "outputs": [],
      "source": [
        "segments_freq_bkk = segments_freq_bkk.merge(df, on='route_name')\n",
        "segments_freq_bkk.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayhT_vZMABYj"
      },
      "outputs": [],
      "source": [
        "segments_freq_bkk = segments_freq_bkk.drop(columns = ['route_id','segment_name','start_stop_name','end_stop_name','window','min_per_trip','segment_id'])\n",
        "segments_freq_bkk.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgcXojQQmETL"
      },
      "source": [
        "###VOLÁN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2hp7tpMgb8P"
      },
      "outputs": [],
      "source": [
        "# Read the feed for the specified view\n",
        "feed = ptg.load_feed('volanbusz_gtfs.zip')\n",
        "\n",
        "# Access different components of the feed\n",
        "routes_volan = feed.routes\n",
        "trips_volan = feed.trips\n",
        "stops_volan = feed.stops\n",
        "stop_times_volan = feed.stop_times\n",
        "shapes_volan = feed.shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-UeFdFpYj3c"
      },
      "outputs": [],
      "source": [
        "# Convert time to seconds since midnight\n",
        "def time_to_seconds(t):\n",
        "    try:\n",
        "        # Handle numeric times directly (e.g., 23100.0 seconds)\n",
        "        if isinstance(t, (int, float)):\n",
        "            return int(t)\n",
        "        # Handle HH:MM:SS format\n",
        "        h, m, s = map(int, t.split(':'))\n",
        "        return h * 3600 + m * 60 + s\n",
        "    except Exception as e:\n",
        "        return None  # Return None if there is an issue with conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzoLI0dHvxJF"
      },
      "outputs": [],
      "source": [
        "# Apply the conversion function to the departure_time column\n",
        "stop_times_volan['time_in_seconds'] = stop_times_volan['departure_time'].apply(time_to_seconds)\n",
        "\n",
        "# Define your desired time window in seconds\n",
        "start_time = time_to_seconds('06:00:00')\n",
        "end_time = time_to_seconds('07:00:00')\n",
        "\n",
        "# Filter the stop_times DataFrame based on the time window\n",
        "stop_times_volan = stop_times_volan[\n",
        "    (stop_times_volan['time_in_seconds'] >= start_time) &\n",
        "    (stop_times_volan['time_in_seconds'] <= end_time)\n",
        "]\n",
        "\n",
        "# Drop the helper column if you don't need it anymore\n",
        "stop_times_volan = stop_times_volan.drop(columns=['time_in_seconds'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufNEztxbvThV"
      },
      "outputs": [],
      "source": [
        "def create_segments(stop_times, stops, trips, routes, shapes):\n",
        "    # Merge stop_times with stops to get stop coordinates\n",
        "    stop_times = stop_times.merge(stops[['stop_id', 'stop_lat', 'stop_lon']], on='stop_id')\n",
        "\n",
        "    # Merge stop_times with trips to get trip and route information\n",
        "    stop_times = stop_times.merge(trips[['trip_id', 'route_id', 'direction_id', 'shape_id']], on='trip_id')\n",
        "\n",
        "    # Merge stop_times with routes to get route_name\n",
        "    stop_times = stop_times.merge(routes[['route_id', 'route_short_name']], on='route_id')\n",
        "\n",
        "    # Sort stop_times by trip_id and stop_sequence\n",
        "    stop_times = stop_times.sort_values(['trip_id', 'stop_sequence'])\n",
        "\n",
        "    # Prepare shapes data\n",
        "    shapes = shapes.sort_values(['shape_id', 'shape_pt_sequence'])\n",
        "    shapes_grouped = shapes.groupby('shape_id')\n",
        "\n",
        "    segments = []\n",
        "    for trip_id, group in stop_times.groupby('trip_id'):\n",
        "        shape_id = group.iloc[0]['shape_id']\n",
        "        if shape_id in shapes_grouped.groups:\n",
        "            shape_points = shapes_grouped.get_group(shape_id)[['shape_pt_lon', 'shape_pt_lat']].values\n",
        "            for i in range(len(group) - 1):\n",
        "                from_stop = group.iloc[i]\n",
        "                to_stop = group.iloc[i + 1]\n",
        "                from_index = from_stop['stop_sequence']\n",
        "                to_index = to_stop['stop_sequence']\n",
        "                segment_shape_points = shape_points[from_index:to_index + 1]\n",
        "\n",
        "                if len(segment_shape_points) < 2:\n",
        "                    segment_shape_points = [(from_stop['stop_lon'], from_stop['stop_lat']),\n",
        "                                            (to_stop['stop_lon'], to_stop['stop_lat'])]\n",
        "\n",
        "                segment = {\n",
        "                    'trip_id': trip_id,\n",
        "                    'route_id': from_stop['route_id'],\n",
        "                    'route_short_name': from_stop['route_short_name'],\n",
        "                    'direction_id': from_stop['direction_id'],\n",
        "                    'from_stop_id': from_stop['stop_id'],\n",
        "                    'to_stop_id': to_stop['stop_id'],\n",
        "                    'geometry': LineString(segment_shape_points)\n",
        "                }\n",
        "                segments.append(segment)\n",
        "\n",
        "    segments_df = pd.DataFrame(segments)\n",
        "    return gpd.GeoDataFrame(segments_df, geometry='geometry')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJfUrd8DCz6S"
      },
      "outputs": [],
      "source": [
        "segments_gdf_volan = create_segments(stop_times_volan, stops_volan, trips_volan, routes_volan, shapes_volan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNCj4KS5gb-l"
      },
      "outputs": [],
      "source": [
        "def create_segments_freq(segments_gdf):\n",
        "    # Group by from_stop_id, to_stop_id, route_id, route_short_name, direction_id and count the occurrences\n",
        "    segments_freq = segments_gdf.groupby(['from_stop_id', 'to_stop_id', 'route_id', 'route_short_name', 'direction_id']).size().reset_index(name='frequency')\n",
        "\n",
        "    # Merge with the segments to get the geometries\n",
        "    segments_freq = segments_freq.merge(segments_gdf[['from_stop_id', 'to_stop_id', 'route_id', 'route_short_name', 'direction_id', 'geometry']],\n",
        "                                        on=['from_stop_id', 'to_stop_id', 'route_id', 'route_short_name', 'direction_id']).drop_duplicates()\n",
        "\n",
        "    # Rename columns as requested\n",
        "    segments_freq = segments_freq.rename(columns={\n",
        "        'from_stop_id': 'start_stop_id',\n",
        "        'to_stop_id': 'end_stop_id',\n",
        "        'route_short_name': 'route_name',\n",
        "        'frequency': 'ntrips'\n",
        "    })\n",
        "\n",
        "    segments_freq = segments_freq.drop(columns=['route_id'])\n",
        "\n",
        "    return gpd.GeoDataFrame(segments_freq, geometry='geometry')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk2eopFbMzJd"
      },
      "outputs": [],
      "source": [
        "# Create segment frequencies GeoDataFrame\n",
        "segments_freq_volan = create_segments_freq(segments_gdf_volan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1UMaPOyldYj"
      },
      "outputs": [],
      "source": [
        "colnames=['route_name', 'capacity']\n",
        "df = pd.read_csv('volan_capacity.csv', names=colnames, header=None)\n",
        "\n",
        "df['route_name'] = df['route_name'].astype(str)\n",
        "\n",
        "# Extract route names and capacity values\n",
        "route_names = df['route_name'].tolist()\n",
        "capacity_dict = dict(zip(df['route_name'], df['capacity']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgMmB38mqLCd"
      },
      "outputs": [],
      "source": [
        "def filter_segment_freq(segments_freq,route_names,capacity_dict):\n",
        "  segments_freq['route_name'] = segments_freq['route_name'].astype(str).str.strip()\n",
        "\n",
        "  # Filter the GeoDataFrame based on route names\n",
        "  segments_freq = segments_freq[segments_freq['route_name'].isin(route_names)]\n",
        "  segments_freq['capacity'] = segments_freq['route_name'].map(capacity_dict)\n",
        "  # Ensure the CRS for the geometry GeoDataFrame if not set\n",
        "  if segments_freq.crs is None:\n",
        "      segments_freq.set_crs(epsg=4326, inplace=True)\n",
        "\n",
        "  # Define the path to the shapefile\n",
        "  shapefile_path = 'zone_zone.SHP'\n",
        "\n",
        "  # Check if the shapefile exists\n",
        "  if not os.path.exists(shapefile_path):\n",
        "      raise FileNotFoundError(f\"Shapefile not found: {shapefile_path}\")\n",
        "\n",
        "  # Load the shapefile using Fiona\n",
        "  with fiona.open(shapefile_path) as shapefile:\n",
        "      # Ensure shapefile has a CRS\n",
        "      if shapefile.crs is None:\n",
        "          # Replace 'EPSG:4326' with the correct CRS if known\n",
        "          shapefile_crs = 'EPSG:4326'\n",
        "      else:\n",
        "          shapefile_crs = shapefile.crs\n",
        "\n",
        "      # Convert the shapefile features to GeoDataFrame\n",
        "      shapefile_gdf = gpd.GeoDataFrame.from_features(shapefile, crs=shapefile_crs)\n",
        "\n",
        "      # Filter the shapefile GeoDataFrame based on the 'NO' attribute\n",
        "      shapefile_filtered = shapefile_gdf[(shapefile_gdf['NO'] >= 1000) & (shapefile_gdf['NO'] <= 23999)]\n",
        "      #shapefile_filtered = shapefile_gdf\n",
        "\n",
        "      # Ensure CRS match\n",
        "      if shapefile_filtered.crs != segments_freq.crs:\n",
        "          shapefile_filtered = shapefile_filtered.to_crs(segments_freq.crs)\n",
        "\n",
        "      # Convert filtered shapes to Shapely geometries\n",
        "      filtered_shapes = [shape(geom_shape) for geom_shape in shapefile_filtered.geometry]\n",
        "\n",
        "      # Function to check if a point is within any of the city shapes\n",
        "      def point_in_city_shapes(point):\n",
        "          return any(city_shape.contains(point) for city_shape in filtered_shapes)\n",
        "\n",
        "      # Create masks to filter rows\n",
        "      mask_first_in_city = segments_freq.geometry.apply(lambda line: point_in_city_shapes(Point(line.coords[0])))\n",
        "      mask_last_in_city = segments_freq.geometry.apply(lambda line: point_in_city_shapes(Point(line.coords[-1])))\n",
        "\n",
        "      # Filter rows where either the first or last point is not in the city shapes\n",
        "      segments_freq = segments_freq[mask_first_in_city & mask_last_in_city]\n",
        "\n",
        "      return segments_freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEG33kCf1A-s"
      },
      "outputs": [],
      "source": [
        "segments_freq_volan = filter_segment_freq(segments_freq_volan, route_names, capacity_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXJEAVZAmJvr"
      },
      "source": [
        "###MÁV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hgbma8YpCWWL"
      },
      "outputs": [],
      "source": [
        "# Read the feed for the specified view\n",
        "feed_mav = ptg.load_feed('mav_gtfs.zip')\n",
        "\n",
        "# Access different components of the feed\n",
        "routes_mav = feed_mav.routes\n",
        "trips_mav = feed_mav.trips\n",
        "stops_mav = feed_mav.stops\n",
        "stop_times_mav = feed_mav.stop_times\n",
        "shapes_mav = feed_mav.shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiLe9x3NYrxd"
      },
      "outputs": [],
      "source": [
        "# Apply the conversion function to the departure_time column\n",
        "stop_times_mav['time_in_seconds'] = stop_times_mav['departure_time'].apply(time_to_seconds)\n",
        "\n",
        "# Define your desired time window in seconds\n",
        "start_time = time_to_seconds('06:00:00')\n",
        "end_time = time_to_seconds('07:00:00')\n",
        "\n",
        "# Filter the stop_times DataFrame based on the time window\n",
        "stop_times_mav = stop_times_mav[\n",
        "    (stop_times_mav['time_in_seconds'] >= start_time) &\n",
        "    (stop_times_mav['time_in_seconds'] <= end_time)\n",
        "]\n",
        "\n",
        "# Drop the helper column if you don't need it anymore\n",
        "stop_times_mav = stop_times_mav.drop(columns=['time_in_seconds'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6b2GGdQDTpj"
      },
      "outputs": [],
      "source": [
        "def create_segments2(stop_times, stops, trips, routes, shapes):\n",
        "    # Merge stop_times with stops to get stop coordinates\n",
        "    stop_times = stop_times.merge(stops[['stop_id', 'stop_lat', 'stop_lon']], on='stop_id')\n",
        "\n",
        "    # Merge stop_times with trips to get trip and route information\n",
        "    stop_times = stop_times.merge(trips[['trip_id', 'route_id', 'shape_id']], on='trip_id')\n",
        "\n",
        "    # Merge stop_times with routes to get route_name\n",
        "    stop_times = stop_times.merge(routes[['route_id', 'route_short_name']], on='route_id')\n",
        "\n",
        "    # Sort stop_times by trip_id and stop_sequence\n",
        "    stop_times = stop_times.sort_values(['trip_id', 'stop_sequence'])\n",
        "\n",
        "    # Prepare shapes data\n",
        "    shapes = shapes.sort_values(['shape_id', 'shape_pt_sequence'])\n",
        "    shapes_grouped = shapes.groupby('shape_id')\n",
        "\n",
        "    segments = []\n",
        "    for trip_id, group in stop_times.groupby('trip_id'):\n",
        "        shape_id = group.iloc[0]['shape_id']\n",
        "        if shape_id in shapes_grouped.groups:\n",
        "            shape_points = shapes_grouped.get_group(shape_id)[['shape_pt_lon', 'shape_pt_lat']].values.tolist()\n",
        "            for i in range(len(group) - 1):\n",
        "                from_stop = group.iloc[i]\n",
        "                to_stop = group.iloc[i + 1]\n",
        "\n",
        "                # Find the indices of the stops in the shape points\n",
        "                from_index = next((idx for idx, point in enumerate(shape_points) if (point[0], point[1]) == (from_stop['stop_lon'], from_stop['stop_lat'])), None)\n",
        "                to_index = next((idx for idx, point in enumerate(shape_points) if (point[0], point[1]) == (to_stop['stop_lon'], to_stop['stop_lat'])), None)\n",
        "\n",
        "                # If either index is not found, default to direct line between stops\n",
        "                if from_index is None or to_index is None or from_index >= to_index:\n",
        "                    segment_shape_points = [(from_stop['stop_lon'], from_stop['stop_lat']),\n",
        "                                            (to_stop['stop_lon'], to_stop['stop_lat'])]\n",
        "                else:\n",
        "                    segment_shape_points = shape_points[from_index:to_index + 1]\n",
        "\n",
        "                segment = {\n",
        "                    'trip_id': trip_id,\n",
        "                    'route_id': from_stop['route_id'],\n",
        "                    'route_short_name': from_stop['route_short_name'],\n",
        "                    'from_stop_id': from_stop['stop_id'],\n",
        "                    'to_stop_id': to_stop['stop_id'],\n",
        "                    'geometry': LineString(segment_shape_points)\n",
        "                }\n",
        "                segments.append(segment)\n",
        "\n",
        "    segments_df = pd.DataFrame(segments)\n",
        "    return gpd.GeoDataFrame(segments_df, geometry='geometry')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t53f7R4CWZi"
      },
      "outputs": [],
      "source": [
        "segments_mav = create_segments2(stop_times_mav, stops_mav, trips_mav, routes_mav, shapes_mav)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCYvPPvTcvs1"
      },
      "outputs": [],
      "source": [
        "segments_mav['direction_id'] = 0\n",
        "segments_mav.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDviVVbncign"
      },
      "outputs": [],
      "source": [
        "segments_freq_mav = create_segments_freq(segments_mav)\n",
        "segments_freq_mav.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtPUGvlj-zsR"
      },
      "outputs": [],
      "source": [
        "colnames=['route_name', 'capacity']\n",
        "df2 = pd.read_csv('mav_capacity.csv', names=colnames, header=None)\n",
        "\n",
        "df2['route_name'] = df2['route_name'].astype(str)\n",
        "\n",
        "# Extract route names and capacity values\n",
        "route_names2 = df2['route_name'].tolist()\n",
        "capacity_dict2 = dict(zip(df2['route_name'], df2['capacity']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsT3AKNJ-zyx"
      },
      "outputs": [],
      "source": [
        "segments_freq_mav = filter_segment_freq(segments_freq_mav, route_names2, capacity_dict2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xusvaSePldbL"
      },
      "outputs": [],
      "source": [
        "original_crs = 'EPSG:4326'\n",
        "if segments_freq_bkk.crs is None:\n",
        "    segments_freq_bkk = segments_freq_bkk.set_crs(original_crs)\n",
        "\n",
        "if segments_freq_volan.crs is None:\n",
        "    segments_freq_volan = segments_freq_volan.set_crs(original_crs)\n",
        "\n",
        "if segments_freq_mav.crs is None:\n",
        "    segments_freq_mav = segments_freq_mav.set_crs(original_crs)\n",
        "\n",
        "# Now you can transform to the desired CRS\n",
        "desired_crs = 'EPSG:4326'\n",
        "segments_freq_bkk = segments_freq_bkk.to_crs(desired_crs)\n",
        "segments_freq_volan = segments_freq_volan.to_crs(desired_crs)\n",
        "segments_freq_mav = segments_freq_mav.to_crs(desired_crs)\n",
        "\n",
        "# Concatenate the GeoDataFrames\n",
        "segments_freq = pd.concat([segments_freq_bkk, segments_freq_volan, segments_freq_mav], ignore_index=True)\n",
        "\n",
        "# Ensure the result is still a GeoDataFrame\n",
        "segments_freq = gpd.GeoDataFrame(segments_freq, geometry='geometry')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vnIpKk3mUe7"
      },
      "source": [
        "###Joint heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIpEzwzqHniT"
      },
      "outputs": [],
      "source": [
        "seq_copy = copy.deepcopy(segments_freq_bkk)\n",
        "seq_copy['weight'] = seq_copy['ntrips'] * seq_copy['capacity']\n",
        "\n",
        "# Step 2: Normalize the weights for visualization\n",
        "seq_copy['normalized_weight'] = seq_copy['weight'] / seq_copy['weight'].max()\n",
        "\n",
        "# Step 3: Create a folium map centered on the mean coordinates of the data\n",
        "mean_x, mean_y = seq_copy.geometry.centroid.x.mean(), seq_copy.geometry.centroid.y.mean()\n",
        "m = folium.Map(location=[mean_y, mean_x], zoom_start=12)\n",
        "\n",
        "# Step 4: Add the lines to the map\n",
        "colormap = cmb.LinearColormap(colors=['blue', 'green', 'yellow', 'red'], vmin=0, vmax=1)\n",
        "for _, row in seq_copy.iterrows():\n",
        "    weight = row['normalized_weight']\n",
        "    # Extracting coordinates directly\n",
        "    locations = [(coord[1], coord[0]) for coord in row['geometry'].coords]\n",
        "    folium.PolyLine(\n",
        "        locations=locations,\n",
        "        weight=weight * 10 + 0.1,  # Adjust thickness\n",
        "        color=colormap(weight),  # Color based on weight\n",
        "        opacity=0.7\n",
        "    ).add_to(m)\n",
        "\n",
        "# Step 5: Add colormap to the map\n",
        "colormap.caption = 'Heatmap Intensity'\n",
        "m.add_child(colormap)\n",
        "#m.save('heatmap.html')\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StqEOSykz5A5"
      },
      "source": [
        "###Multigraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dupY6QR232s6"
      },
      "outputs": [],
      "source": [
        "def add_edges(G, gdf):\n",
        "    for idx, row in gdf.iterrows():\n",
        "        # Extract start and end coordinates\n",
        "        start_coords = (row['geometry'].coords[0][1], row['geometry'].coords[0][0])\n",
        "        end_coords = (row['geometry'].coords[-1][1], row['geometry'].coords[-1][0])\n",
        "\n",
        "        # Calculate the product of ntrips and capacity\n",
        "        ntrips_capacity = row['ntrips'] * row['capacity']\n",
        "\n",
        "        # Create a unique key for the edge\n",
        "        key = f\"{row['route_name']}_{ntrips_capacity}\"\n",
        "\n",
        "        # Add edge to the graph with specified keys and additional attributes\n",
        "        if row['direction_id'] == 0:\n",
        "          G.add_edge(start_coords, end_coords, key=key, route_name=row['route_name'], ntrips_capacity=ntrips_capacity)\n",
        "        elif row['direction_id'] == 1:\n",
        "          G.add_edge(end_coords, start_coords, key=key, route_name=row['route_name'], ntrips_capacity=ntrips_capacity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQnHmUeO37s8"
      },
      "outputs": [],
      "source": [
        "G = nx.MultiDiGraph()\n",
        "# Add edges from segments_freq to the graph\n",
        "add_edges(G, segments_freq)\n",
        "\n",
        "# Display basic information about the graph\n",
        "#print(info(G))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxKV_siP4afb"
      },
      "outputs": [],
      "source": [
        "def create_folium_map(G):\n",
        "    # Create a folium map centered around Budapest\n",
        "    m = folium.Map(location=[47.4979, 19.0402], zoom_start=12)\n",
        "\n",
        "    # Add edges to the folium map with tooltips\n",
        "    for u, v, data in G.edges(data=True):\n",
        "        # Create the coordinates list for the PolyLine\n",
        "        coords = [u, v]\n",
        "\n",
        "        # Create the tooltip text\n",
        "        tooltip_text = f\"Route: {data['route_name']}<br>Capacity: {data['ntrips_capacity']}\"\n",
        "\n",
        "        # Add the PolyLine to the map with the tooltip\n",
        "        folium.PolyLine(coords, color='blue', weight=2.5, opacity=1, tooltip=tooltip_text).add_to(m)\n",
        "\n",
        "    # Add nodes to the folium map\n",
        "    for node in G.nodes():\n",
        "        folium.CircleMarker(location=node, radius=2, color='red').add_to(m)\n",
        "\n",
        "    return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XqplBQqIS1l"
      },
      "outputs": [],
      "source": [
        "def create_folium_map_multicolor(G):\n",
        "    # Create a folium map centered around Budapest\n",
        "    m = folium.Map(location=[47.4979, 19.0402], zoom_start=12)\n",
        "\n",
        "    # Generate a list of distinct colors\n",
        "    colors = list(mcolors.CSS4_COLORS.values())\n",
        "    max_colors = min(len(colors), 50)  # Limit to 30 distinct colors if there are fewer colors available\n",
        "    colors = colors[:max_colors]\n",
        "\n",
        "    # Create a dictionary to track the number of parallel edges\n",
        "    edge_count = {}\n",
        "\n",
        "    # Add edges to the folium map with tooltips\n",
        "    for u, v, key, data in G.edges(keys=True, data=True):\n",
        "        # Create the coordinates list for the PolyLine\n",
        "        coords = [u, v]\n",
        "\n",
        "        # Create a unique identifier for the edge pair (u, v)\n",
        "        edge_id = (u, v)\n",
        "        if edge_id not in edge_count:\n",
        "            edge_count[edge_id] = 0\n",
        "        edge_index = edge_count[edge_id]\n",
        "        edge_count[edge_id] += 1\n",
        "\n",
        "        # Assign a color based on the edge index\n",
        "        color = colors[edge_index % len(colors)]\n",
        "\n",
        "        # Create the tooltip text\n",
        "        tooltip_text = f\"Route: {data['route_name']}<br>Capacity: {data['ntrips_capacity']}\"\n",
        "\n",
        "        # Add the PolyLine to the map with the tooltip\n",
        "        folium.PolyLine(coords, color=color, weight=2.5, opacity=1, tooltip=tooltip_text).add_to(m)\n",
        "\n",
        "    # Add nodes to the folium map\n",
        "    for node in G.nodes():\n",
        "        folium.CircleMarker(location=node, radius=2, color='red').add_to(m)\n",
        "\n",
        "    return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0OX4iig4ah0"
      },
      "outputs": [],
      "source": [
        "s = create_folium_map(G)#_multicolor(G)\n",
        "\n",
        "# Display the map\n",
        "#m.save(\"multidigraph_map.html\")\n",
        "s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iQDJRoOz_7Z"
      },
      "source": [
        "###Normal graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTU8O8F-WSUT"
      },
      "outputs": [],
      "source": [
        "def add_edges_with_weights2(G, gdf):\n",
        "    for idx, row in gdf.iterrows():\n",
        "        # Extract start and end coordinates\n",
        "        start_coords = (row['geometry'].coords[0][1], row['geometry'].coords[0][0])\n",
        "        end_coords = (row['geometry'].coords[-1][1], row['geometry'].coords[-1][0])\n",
        "\n",
        "        ntrips_capacity = row['ntrips'] * row['capacity']\n",
        "\n",
        "        if row['direction_id'] == 0:\n",
        "            if G.has_edge(start_coords, end_coords):\n",
        "                # If the edge exists, update the weight\n",
        "                G[start_coords][end_coords]['weight'] += ntrips_capacity\n",
        "                G[start_coords][end_coords]['ntrips_capacity'] += ntrips_capacity\n",
        "            else:\n",
        "                # If the edge does not exist, add it with the new weight and attributes\n",
        "                G.add_edge(start_coords, end_coords, weight=ntrips_capacity, route_name=row['route_name'], ntrips_capacity=ntrips_capacity)\n",
        "        elif row['direction_id'] == 1:\n",
        "            if G.has_edge(end_coords, start_coords):\n",
        "                # If the edge exists, update the weight\n",
        "                G[end_coords][start_coords]['weight'] += ntrips_capacity\n",
        "                G[end_coords][start_coords]['ntrips_capacity'] += ntrips_capacity\n",
        "            else:\n",
        "                # If the edge does not exist, add it with the new weight and attributes\n",
        "                G.add_edge(end_coords, start_coords, weight=ntrips_capacity, route_name=row['route_name'], ntrips_capacity=ntrips_capacity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE6nusLiVSHT"
      },
      "outputs": [],
      "source": [
        "def create_folium_map2(G):\n",
        "    # Create a folium map centered around Budapest\n",
        "    m = folium.Map(location=[47.4979, 19.0402], zoom_start=12)\n",
        "\n",
        "    # Add edges to the folium map with tooltips\n",
        "    for u, v, data in G.edges(data=True):\n",
        "        # Create the coordinates list for the PolyLine\n",
        "        coords = [u, v]\n",
        "\n",
        "        # Create the tooltip text\n",
        "        capacity = data.get('ntrips_capacity', 'N/A')\n",
        "        tooltip_text = f\"Capacity: {capacity}\"\n",
        "\n",
        "        # Add the PolyLine to the map with the tooltip\n",
        "        folium.PolyLine(coords, color='blue', weight=2.5, opacity=1, tooltip=tooltip_text).add_to(m)\n",
        "\n",
        "    # Add nodes to the folium map\n",
        "    for node in G.nodes():\n",
        "        folium.CircleMarker(location=node, radius=2, color='red').add_to(m)\n",
        "\n",
        "    return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgg46SJ_t7fT"
      },
      "outputs": [],
      "source": [
        "def create_folium_heatmap(G):\n",
        "    # Create a folium map centered around Budapest\n",
        "    m = folium.Map(location=[47.4979, 19.0402], zoom_start=12)\n",
        "\n",
        "    # Normalize the edge weights for color scaling\n",
        "    edge_weights = [data.get('ntrips_capacity', 1) for u, v, data in G.edges(data=True)]\n",
        "    min_weight = min(edge_weights)\n",
        "    max_weight = max(edge_weights)\n",
        "\n",
        "    def get_color(weight):\n",
        "        # Normalize the weight to the range [0, 1]\n",
        "        norm_weight = (weight - min_weight) / (max_weight - min_weight) if max_weight != min_weight else 0.5\n",
        "        # Get a color from the 'rainbow' colormap\n",
        "        color = cm.rainbow(norm_weight)\n",
        "        # Convert color to hex\n",
        "        return mcolors.to_hex(color)\n",
        "\n",
        "    # Add edges to the folium map with tooltips\n",
        "    for u, v, data in G.edges(data=True):\n",
        "        # Create the coordinates list for the PolyLine\n",
        "        coords = [u, v]\n",
        "\n",
        "        # Get the weight and color for the edge\n",
        "        capacity = data.get('ntrips_capacity', 1)\n",
        "        color = get_color(capacity)\n",
        "\n",
        "        # Create the tooltip text\n",
        "        tooltip_text = f\"Capacity: {capacity}\"\n",
        "\n",
        "        # Add the PolyLine to the map with the tooltip\n",
        "        folium.PolyLine(coords, color=color, weight=2.5, opacity=1, tooltip=tooltip_text).add_to(m)\n",
        "\n",
        "    # Add nodes to the folium map\n",
        "    for node in G.nodes():\n",
        "        folium.CircleMarker(location=node, radius=2, color='gray').add_to(m)\n",
        "\n",
        "    return m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsBYzEP50IET"
      },
      "source": [
        "###Combining nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQ5x4B0WQnK6"
      },
      "outputs": [],
      "source": [
        "def filter_stops(stops, shapefile_path='zone_zone.SHP'):\n",
        "    # Convert stops to GeoDataFrame if it's not already one\n",
        "    if not isinstance(stops, gpd.GeoDataFrame):\n",
        "        if 'geometry' not in stops.columns:\n",
        "            raise ValueError(\"The stops DataFrame must have a 'geometry' column with Point geometries.\")\n",
        "        stops = gpd.GeoDataFrame(stops, geometry='geometry')\n",
        "\n",
        "    # Ensure the CRS for the stops GeoDataFrame if not set\n",
        "    if stops.crs is None:\n",
        "        stops.set_crs(epsg=4326, inplace=True)\n",
        "\n",
        "    # Check if the shapefile exists\n",
        "    if not os.path.exists(shapefile_path):\n",
        "        raise FileNotFoundError(f\"Shapefile not found: {shapefile_path}\")\n",
        "\n",
        "    # Load the shapefile using Fiona\n",
        "    with fiona.open(shapefile_path) as shapefile:\n",
        "        # Ensure shapefile has a CRS\n",
        "        if shapefile.crs is None:\n",
        "            shapefile_crs = 'EPSG:4326'\n",
        "        else:\n",
        "            shapefile_crs = shapefile.crs\n",
        "\n",
        "        # Convert the shapefile features to a GeoDataFrame\n",
        "        shapefile_gdf = gpd.GeoDataFrame.from_features(shapefile, crs=shapefile_crs)\n",
        "\n",
        "        # Filter the shapefile GeoDataFrame based on the 'NO' attribute\n",
        "        shapefile_filtered = shapefile_gdf[(shapefile_gdf['NO'] >= 1000) & (shapefile_gdf['NO'] <= 23999)]\n",
        "\n",
        "        # Ensure CRS match\n",
        "        if shapefile_filtered.crs != stops.crs:\n",
        "            shapefile_filtered = shapefile_filtered.to_crs(stops.crs)\n",
        "\n",
        "        # Convert filtered shapes to Shapely geometries\n",
        "        filtered_shapes = [shape(geom_shape) for geom_shape in shapefile_filtered.geometry]\n",
        "\n",
        "        # Function to check if a point (stop) is within any of the city shapes\n",
        "        def point_in_city_shapes(point):\n",
        "            return any(city_shape.contains(point) for city_shape in filtered_shapes)\n",
        "\n",
        "        # Create a mask to filter stops within the city shapes\n",
        "        mask_in_city = stops.geometry.apply(lambda point: point_in_city_shapes(point))\n",
        "\n",
        "        # Filter stops within the city shapes\n",
        "        filtered_stops = stops[mask_in_city]\n",
        "\n",
        "        return filtered_stops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YUw5YgrRKvh"
      },
      "outputs": [],
      "source": [
        "stops_bkk = stops_bkk.drop(columns = ['stop_code','location_sub_type','wheelchair_boarding'])\n",
        "stops_volan = stops_volan.drop(columns = ['platform_code'])\n",
        "stops_volan['geometry'] = stops_volan.apply(lambda row: Point(row['stop_lon'], row['stop_lat']), axis=1)\n",
        "stops_volan = gpd.GeoDataFrame(stops_volan, geometry='geometry')\n",
        "stops_volan = filter_stops(stops_volan)\n",
        "stops_mav = stops_mav.drop(columns = ['stop_code','stop_desc','wheelchair_boarding','stop_timezone'])\n",
        "stops_mav['geometry'] = stops_mav.apply(lambda row: Point(row['stop_lon'], row['stop_lat']), axis=1)\n",
        "stops_mav = gpd.GeoDataFrame(stops_mav, geometry='geometry')\n",
        "stops_mav = filter_stops(stops_mav)\n",
        "stops = pd.concat([stops_bkk, stops_volan, stops_mav], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHElCShkEQFI"
      },
      "outputs": [],
      "source": [
        "# Assuming 'stops' is the GeoDataFrame and has columns 'stop_name', 'stop_id', 'geometry'\n",
        "stops_list = []\n",
        "\n",
        "# Step 1: Extract necessary data into a list of [stop_name, stop_id, coordinate]\n",
        "for _, row in stops.iterrows():\n",
        "    stop_name = row['stop_name']\n",
        "    stop_id = row['stop_id']\n",
        "    coordinate = (row['stop_lon'], row['stop_lat'])\n",
        "    stops_list.append([stop_name, stop_id, coordinate])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pIRviVtRhjj"
      },
      "outputs": [],
      "source": [
        "def remove_brackets_and_extras(stop_name):\n",
        "    stop_name = re.sub(r'\\(.*?\\)', '', stop_name)         # Remove text in parentheses\n",
        "    stop_name = re.sub(r'\\[.*?\\]', '', stop_name)         # Remove text in square brackets\n",
        "    stop_name = re.sub(r'\\bH\\b', '', stop_name)           # Remove standalone 'H'\n",
        "    stop_name = re.sub(r'\\bM\\b', '', stop_name)           # Remove standalone 'M'\n",
        "    stop_name = stop_name.replace('+', '')                # Remove the '+' sign\n",
        "    stop_name = re.sub(r'\\bBudapest,\\s*', '', stop_name)  # Remove 'Budapest, '\n",
        "    return stop_name.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PRDuBz8Rsii"
      },
      "outputs": [],
      "source": [
        "# Clean stop names in the stops_list\n",
        "for stop in stops_list:\n",
        "    stop[0] = remove_brackets_and_extras(stop[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvQzcZmwdfWE"
      },
      "outputs": [],
      "source": [
        "# Step 3: Create a dictionary with stop_name as keys and lists of [stop_id, coordinate] as values\n",
        "stops_dict = defaultdict(list)\n",
        "for stop_name, stop_id, coord in stops_list:\n",
        "    stops_dict[stop_name].append([stop_id, coord])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAyuyZip_Y8R"
      },
      "outputs": [],
      "source": [
        "def haversine(coord1, coord2):\n",
        "    # Radius of the Earth in meters\n",
        "    R = 6371000\n",
        "\n",
        "    lat1, lon1 = coord1\n",
        "    lat2, lon2 = coord2\n",
        "\n",
        "    # Convert latitude and longitude from degrees to radians\n",
        "    phi1 = math.radians(lat1)\n",
        "    phi2 = math.radians(lat2)\n",
        "    delta_phi = math.radians(lat2 - lat1)\n",
        "    delta_lambda = math.radians(lon2 - lon1)\n",
        "\n",
        "    # Haversine formula\n",
        "    a = math.sin(delta_phi / 2) ** 2 + math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda / 2) ** 2\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
        "\n",
        "    # Distance in meters\n",
        "    distance = R * c\n",
        "    return distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0NZMxBwIOTm"
      },
      "outputs": [],
      "source": [
        "# Step 4: Group by coordinates within each stop name\n",
        "def group_coordinates_by_distance(entries, threshold=1000):\n",
        "    groups = []\n",
        "    for stop_id, coord in entries:\n",
        "        added = False\n",
        "        for group in groups:\n",
        "            if all(haversine(coord, existing_coord[1]) < threshold for existing_coord in group):\n",
        "                group.append([stop_id, coord])\n",
        "                added = True\n",
        "                break\n",
        "        if not added:\n",
        "            groups.append([[stop_id, coord]])\n",
        "    return groups\n",
        "\n",
        "# Create a list to hold all groups of stop_ids and coordinates\n",
        "grouped_by_stop = []\n",
        "\n",
        "# Group coordinates for each stop name\n",
        "for stop_name, entries in stops_dict.items():\n",
        "    groups = group_coordinates_by_distance(entries)\n",
        "    grouped_by_stop.extend(groups)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ekwadYdgTi9C"
      },
      "outputs": [],
      "source": [
        "# Step 5: Calculate average coordinates for each group and replace them in the grouped list\n",
        "def calculate_average_coordinates(groups):\n",
        "    for group in groups:\n",
        "        if not group:\n",
        "            continue\n",
        "        avg_lat = sum(coord[1][1] for coord in group) / len(group)\n",
        "        avg_lon = sum(coord[1][0] for coord in group) / len(group)\n",
        "        average_coord = (avg_lon, avg_lat)\n",
        "        for item in group:\n",
        "            item[1] = average_coord\n",
        "\n",
        "# Apply the average calculation to all groups\n",
        "calculate_average_coordinates(grouped_by_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip9M-feJ2yJR"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary to store stop_id as keys and average coordinates as values\n",
        "average_coords_dict = {}\n",
        "\n",
        "# Iterate through the grouped_by_stop list to fill the dictionary\n",
        "for group in grouped_by_stop:\n",
        "    for stop_id, avg_coord in group:\n",
        "        average_coords_dict[stop_id] = avg_coord\n",
        "\n",
        "to_get_red = {}\n",
        "\n",
        "# Iterate through grouped_by_stop\n",
        "for sublist in grouped_by_stop:\n",
        "    for id_coord_pair in sublist:\n",
        "        id_ = id_coord_pair[0]       # First element is the ID\n",
        "        coord = id_coord_pair[1]     # Second element is the coordinate\n",
        "        # Switch the two parts of the coordinate\n",
        "        reversed_coord = (coord[1], coord[0])  # Swap the lat and long\n",
        "        # Add the id to the list corresponding to the reversed coordinate in result_dict\n",
        "        if reversed_coord not in to_get_red:\n",
        "            to_get_red[reversed_coord] = []\n",
        "        to_get_red[reversed_coord].append(id_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIP3X2oM0gEh"
      },
      "outputs": [],
      "source": [
        "# Function to update the geometry of each row\n",
        "def update_geometry(row):\n",
        "    # Get the original LineString\n",
        "    line = row['geometry']\n",
        "\n",
        "    # Look up the average coordinates using the stop IDs\n",
        "    start_coord = average_coords_dict.get(row['start_stop_id'])\n",
        "    end_coord = average_coords_dict.get(row['end_stop_id'])\n",
        "\n",
        "    # If coordinates are found, update the LineString\n",
        "    if start_coord and end_coord:\n",
        "        # Create a new LineString with updated start and end coordinates\n",
        "        new_coords = [start_coord] + list(line.coords[1:-1]) + [end_coord]\n",
        "        return LineString(new_coords)\n",
        "    else:\n",
        "        # If not found, return the original LineString\n",
        "        return line\n",
        "\n",
        "seq_copy = copy.deepcopy(segments_freq)\n",
        "seq_copy['geometry'] = seq_copy.apply(update_geometry, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9YyoNaIhEiq"
      },
      "outputs": [],
      "source": [
        "G4 = nx.DiGraph()\n",
        "add_edges_with_weights2(G4, seq_copy)\n",
        "#G4.nodes()\n",
        "r = create_folium_map2(G4)\n",
        "#r.save('osszevont.html')\n",
        "r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRg2VBcNqfmJ"
      },
      "outputs": [],
      "source": [
        "Ghelp = nx.MultiDiGraph()\n",
        "add_edges(Ghelp, seq_copy)\n",
        "#G4.nodes()\n",
        "#r = create_folium_map(Ghelp)\n",
        "#r.save('osszevont.html')\n",
        "#r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOGgPufjDSt3"
      },
      "source": [
        "###Zones and OD matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEy5w_tpDVxW"
      },
      "outputs": [],
      "source": [
        "sf = shp.Reader(\"zone_zone.shp\")\n",
        "#print(type(sf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ7lxezXDwj3"
      },
      "outputs": [],
      "source": [
        "def read_shapefile_df(sf):\n",
        "    \"\"\"\n",
        "    Read a shapefile into a Pandas dataframe with a 'coords'\n",
        "    column holding the geometry information. This uses the pyshp\n",
        "    package\n",
        "    \"\"\"\n",
        "    fields = [x[0] for x in sf.fields][1:]\n",
        "    records = sf.records()\n",
        "    shps = [s.points for s in sf.shapes()]\n",
        "    df = pd.DataFrame(columns=fields, data=records)\n",
        "    df = df.assign(coords=shps)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t626mk74Ig-7"
      },
      "outputs": [],
      "source": [
        "def read_shapefile_gdf(sf):\n",
        "    \"\"\"\n",
        "    Read a shapefile into a GeoDataFrame.\n",
        "    \"\"\"\n",
        "    # Extract the fields and records\n",
        "    fields = [x[0] for x in sf.fields][1:]\n",
        "    records = sf.records()\n",
        "\n",
        "    # Extract the shapes and convert them to shapely geometries\n",
        "    geometries = []\n",
        "    for shape in sf.shapes():\n",
        "        if len(shape.points) == 1:  # Point\n",
        "            geometries.append(Point(shape.points[0]))\n",
        "        else:  # Polygon\n",
        "            geometries.append(Polygon(shape.points))\n",
        "\n",
        "    # Create a GeoDataFrame\n",
        "    df = gpd.GeoDataFrame(columns=fields, data=records)\n",
        "    df = df.assign(geometry=geometries)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj8oqxVUDwmG"
      },
      "outputs": [],
      "source": [
        "df_shape = read_shapefile_gdf(sf)\n",
        "df_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clwYiHYhDwoW"
      },
      "outputs": [],
      "source": [
        "def plot_map(sf, df, x_lim=None, y_lim=None, figsize=(33, 25), filtered_ids=None):\n",
        "    '''\n",
        "    Plot map with lim coordinates and colored shapes based on 'NO' attribute.\n",
        "    '''\n",
        "    plt.figure(figsize=figsize)\n",
        "    id = 0\n",
        "\n",
        "    cmap = plt.get_cmap('viridis')\n",
        "    norm = plt.Normalize(df['NO'].min(), df['NO'].max())\n",
        "\n",
        "    for shape in sf.shapeRecords():\n",
        "        if filtered_ids is not None and id not in filtered_ids:\n",
        "            id += 1\n",
        "            continue\n",
        "        x = [i[0] for i in shape.shape.points[:]]\n",
        "        y = [i[1] for i in shape.shape.points[:]]\n",
        "        color = cmap(norm(df.loc[id, 'NO']))\n",
        "        plt.fill(x, y, color=color, edgecolor='k')\n",
        "\n",
        "        if x_lim is None and y_lim is None:\n",
        "            x0 = np.mean(x)\n",
        "            y0 = np.mean(y)\n",
        "            plt.text(x0, y0, id, fontsize=10)\n",
        "        id += 1\n",
        "\n",
        "    if x_lim is not None and y_lim is not None:\n",
        "        plt.xlim(x_lim)\n",
        "        plt.ylim(y_lim)\n",
        "\n",
        "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "    sm.set_array([])\n",
        "    plt.colorbar(sm, orientation='vertical', label='NO Attribute')\n",
        "    #plt.savefig('zonak.png')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoI_HFPmDwq-"
      },
      "outputs": [],
      "source": [
        "# Filter dataframe based on 'NO' attribute\n",
        "filtered_df = df_shape[(df_shape['NO'] >= 1000) & (df_shape['NO'] <= 23999)]\n",
        "filtered_ids = filtered_df.index.tolist()\n",
        "\n",
        "#print(filtered_df.shape)  # Print the shape of the filtered dataframe\n",
        "\n",
        "# Plot the filtered map with colors\n",
        "plot_map(sf, filtered_df, filtered_ids=filtered_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X55_iFZrHY7e"
      },
      "outputs": [],
      "source": [
        "geom = copy.deepcopy(seq_copy)\n",
        "geom.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ25gdpkHY9m"
      },
      "outputs": [],
      "source": [
        "if geom.crs is None:\n",
        "    # Replace 'EPSG:4326' with the correct CRS if known\n",
        "    geom.set_crs(epsg=4326, inplace=True)\n",
        "\n",
        "# Define the path to the shapefile\n",
        "shapefile_path = 'zone_zone.SHP'\n",
        "\n",
        "# Check if the shapefile exists\n",
        "if not os.path.exists(shapefile_path):\n",
        "    raise FileNotFoundError(f\"Shapefile not found: {shapefile_path}\")\n",
        "\n",
        "# Load the shapefile using Fiona\n",
        "with fiona.open(shapefile_path) as shapefile:\n",
        "    # Ensure shapefile has a CRS\n",
        "    if shapefile.crs is None:\n",
        "        # Replace 'EPSG:4326' with the correct CRS if known\n",
        "        shapefile_crs = 'EPSG:4326'\n",
        "    else:\n",
        "        shapefile_crs = shapefile.crs\n",
        "\n",
        "    # Convert the shapefile features to GeoDataFrame\n",
        "    shapefile_gdf = gpd.GeoDataFrame.from_features(shapefile, crs=shapefile_crs)\n",
        "\n",
        "    # Filter the shapefile GeoDataFrame based on the 'NO' attribute\n",
        "    shapefile_filtered = shapefile_gdf[(shapefile_gdf['NO'] >= 1000) & (shapefile_gdf['NO'] <= 23999)]\n",
        "\n",
        "    # Ensure CRS match\n",
        "    if shapefile_filtered.crs != geom.crs:\n",
        "        shapefile_filtered = shapefile_filtered.to_crs(geom.crs)\n",
        "\n",
        "    # Initialize the new columns with None values\n",
        "    geom['first_coord_shape_index'] = None\n",
        "    geom['second_coord_shape_index'] = None\n",
        "\n",
        "    # Convert filtered shapes to Shapely geometries\n",
        "    filtered_shapes = [(shape(geom_shape), shape_id) for geom_shape, shape_id in zip(shapefile_filtered.geometry, shapefile_filtered.index)]\n",
        "\n",
        "    # Iterate through the rows of the GeoDataFrame\n",
        "    for idx, row in geom.iterrows():\n",
        "        # Extract the coordinates from the geometry column\n",
        "        coords = list(row.geometry.coords)\n",
        "        first_coord = Point(coords[0])\n",
        "        second_coord = Point(coords[-1])\n",
        "        \"\"\"if idx <= 10:\n",
        "          print(coords)\n",
        "          print(first_coord)\n",
        "          print(second_coord)\"\"\"\n",
        "\n",
        "        # Check which shape the first coordinate falls within\n",
        "        first_match = next((shape_id for geom_shape, shape_id in filtered_shapes if geom_shape.contains(first_coord)), None)\n",
        "        geom.at[idx, 'first_coord_shape_index'] = first_match\n",
        "\n",
        "        # Check which shape the second coordinate falls within\n",
        "        second_match = next((shape_id for geom_shape, shape_id in filtered_shapes if geom_shape.contains(second_coord)), None)\n",
        "        geom.at[idx, 'second_coord_shape_index'] = second_match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqIp8en_EIgB"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty dictionary to store the results.\n",
        "node_shape_dict = {}\n",
        "\n",
        "# Iterate over each row in the GeoDataFrame.\n",
        "for index, row in geom.iterrows():\n",
        "    # Extract the geometry (assuming it's a LineString).\n",
        "    line = row['geometry']\n",
        "\n",
        "    # Get the first and last elements of the LineString.\n",
        "    first_point = line.coords[0]\n",
        "    last_point = line.coords[-1]\n",
        "\n",
        "    # Swap the longitude and latitude\n",
        "    first_point_swapped = (first_point[1], first_point[0])\n",
        "    last_point_swapped = (last_point[1], last_point[0])\n",
        "\n",
        "    # Check if the first point is not already in the dictionary.\n",
        "    if first_point_swapped not in node_shape_dict:\n",
        "        # Add the swapped first point as a key with the first_shape_index as the value.\n",
        "        node_shape_dict[first_point_swapped] = row['first_coord_shape_index']\n",
        "\n",
        "    # Check if the last point is not already in the dictionary.\n",
        "    if last_point_swapped not in node_shape_dict:\n",
        "        # Add the swapped last point as a key with the second_shape_index as the value.\n",
        "        node_shape_dict[last_point_swapped] = row['second_coord_shape_index']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh3Cq3Y1SaQ5"
      },
      "outputs": [],
      "source": [
        "# Initialize zone_dict as an empty dictionary\n",
        "zone_dict = {}\n",
        "\n",
        "# Iterate through node_shape_dict\n",
        "for node, zone in node_shape_dict.items():\n",
        "    # If zone is already in zone_dict, increment its value by 1\n",
        "    if zone in zone_dict:\n",
        "        zone_dict[zone] += 1\n",
        "    # If zone is not in zone_dict, add it with a value of 1\n",
        "    else:\n",
        "        zone_dict[zone] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFenQQkPXxef"
      },
      "outputs": [],
      "source": [
        "# Initialize the matrix\n",
        "matrix_size = len(shapefile_filtered)\n",
        "matrix_cap = np.zeros((matrix_size, matrix_size))\n",
        "\n",
        "# Populate the matrix based on geom rows\n",
        "for idx, row in geom.iterrows():\n",
        "    first_index = row['first_coord_shape_index']\n",
        "    second_index = row['second_coord_shape_index']\n",
        "    if first_index is not None and second_index is not None and first_index <= 921 and second_index <= 921:\n",
        "      matrix_cap[first_index, second_index] += row['ntrips'] * row['capacity']\n",
        "matrix_trip = np.zeros((matrix_size, matrix_size))\n",
        "for idx, row in geom.iterrows():\n",
        "    first_index = row['first_coord_shape_index']\n",
        "    second_index = row['second_coord_shape_index']\n",
        "    if first_index is not None and second_index is not None and first_index <= 921 and second_index <= 921:\n",
        "      matrix_trip[first_index, second_index] += row['ntrips']\n",
        "matrix_cap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj_M22R_MQ5L"
      },
      "source": [
        "###Incoming and outgoing weights, OD comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgEKThh0Ttif"
      },
      "outputs": [],
      "source": [
        "file_path = 'OD_tk.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "third_column_tk = df.iloc[2:924, 2].reset_index(drop=True)\n",
        "third_row_tk = df.iloc[1, 3:925].reset_index(drop=True)\n",
        "\n",
        "file_path2 = 'OD_szgk.xlsx'\n",
        "df2 = pd.read_excel(file_path2)\n",
        "third_column_szgk = df2.iloc[2:924, 2].reset_index(drop=True)\n",
        "third_row_szgk = df2.iloc[1, 3:925].reset_index(drop=True)\n",
        "\n",
        "third_column_tk = third_column_tk / 24 * 1.1\n",
        "third_row_tk = third_row_tk / 24 * 1.1\n",
        "third_column_szgk = third_column_szgk / 24 * 1.1 * 1.5\n",
        "third_row_szgk = third_row_szgk / 24 * 1.1 * 1.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXY7vm9p2ppr"
      },
      "outputs": [],
      "source": [
        "row_sums_without_diagonal = []\n",
        "for i in range(matrix_cap.shape[0]):\n",
        "    row_sum = np.sum(matrix_cap[i]) - matrix_cap[i, i]\n",
        "    row_sums_without_diagonal.append(row_sum)\n",
        "\n",
        "ki_tk = pd.DataFrame({'index': third_column_tk.index, 'OD_outvalue': third_column_tk, 'calculated_outvalue': row_sums_without_diagonal})\n",
        "\n",
        "col_sums_without_diagonal = []\n",
        "for i in range(matrix_cap.shape[1]):\n",
        "    col_sum = np.sum(matrix_cap[:, i]) - matrix_cap[i, i]\n",
        "    col_sums_without_diagonal.append(col_sum)\n",
        "\n",
        "# 3. Append the computed sums as a new column to the `third_column` DataFrame\n",
        "#third_row['calculated_invalue'] = col_sums_without_diagonal\n",
        "be_tk = pd.DataFrame({'index': third_row_tk.index, 'OD_invalue': third_row_tk, 'calculated_invalue': col_sums_without_diagonal})\n",
        "\n",
        "ki_tk.set_index('index', inplace=True)\n",
        "be_tk.set_index('index', inplace=True)\n",
        "\n",
        "be_tk.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06Eq0JnH7xKG"
      },
      "outputs": [],
      "source": [
        "row_sums_without_diagonal2 = []\n",
        "for i in range(matrix_cap.shape[0]):\n",
        "    row_sum = np.sum(matrix_cap[i]) - matrix_cap[i, i]\n",
        "    row_sums_without_diagonal2.append(row_sum)\n",
        "\n",
        "ki_szgk = pd.DataFrame({'index': third_column_szgk.index, 'OD_outvalue': third_column_szgk, 'calculated_outvalue': row_sums_without_diagonal})\n",
        "\n",
        "col_sums_without_diagonal2 = []\n",
        "for i in range(matrix_cap.shape[1]):\n",
        "    col_sum = np.sum(matrix_cap[:, i]) - matrix_cap[i, i]\n",
        "    col_sums_without_diagonal2.append(col_sum)\n",
        "\n",
        "# 3. Append the computed sums as a new column to the `third_column` DataFrame\n",
        "#third_row['calculated_invalue'] = col_sums_without_diagonal\n",
        "be_szgk = pd.DataFrame({'index': third_row_szgk.index, 'OD_invalue': third_row_szgk, 'calculated_invalue': col_sums_without_diagonal})\n",
        "\n",
        "ki_szgk.set_index('index', inplace=True)\n",
        "be_szgk.set_index('index', inplace=True)\n",
        "\n",
        "be_szgk.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9ZmJcNH4RQd"
      },
      "outputs": [],
      "source": [
        "ki_tk['OD_outvalue'] = pd.to_numeric(ki_tk['OD_outvalue'], errors='coerce')\n",
        "ki_tk['calculated_outvalue'] = pd.to_numeric(ki_tk['calculated_outvalue'], errors='coerce')\n",
        "ki_tk['Out_difference'] = - ki_tk['OD_outvalue'] + ki_tk['calculated_outvalue']\n",
        "be_tk['OD_invalue'] = pd.to_numeric(be_tk['OD_invalue'], errors='coerce')\n",
        "be_tk['calculated_invalue'] = pd.to_numeric(be_tk['calculated_invalue'], errors='coerce')\n",
        "be_tk['In_difference'] = -be_tk['OD_invalue'] + be_tk['calculated_invalue']\n",
        "\n",
        "ki_szgk['OD_outvalue'] = pd.to_numeric(ki_szgk['OD_outvalue'], errors='coerce')\n",
        "ki_szgk['calculated_outvalue'] = pd.to_numeric(ki_szgk['calculated_outvalue'], errors='coerce')\n",
        "ki_szgk['Out_difference'] = - ki_szgk['OD_outvalue'] + ki_szgk['calculated_outvalue']\n",
        "be_szgk['OD_invalue'] = pd.to_numeric(be_szgk['OD_invalue'], errors='coerce')\n",
        "be_szgk['calculated_invalue'] = pd.to_numeric(be_szgk['calculated_invalue'], errors='coerce')\n",
        "be_szgk['In_difference'] = -be_szgk['OD_invalue'] + be_szgk['calculated_invalue']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNM--gpoInfR"
      },
      "outputs": [],
      "source": [
        "min_index_ki_szgk = ki_szgk['Out_difference'].idxmin()  # Get index of the min value\n",
        "max_index_ki_szgk = ki_szgk['Out_difference'].idxmax()  # Get index of the max value\n",
        "min_index_be_szgk = be_szgk['In_difference'].idxmin()\n",
        "max_index_be_szgk = be_szgk['In_difference'].idxmax()\n",
        "\n",
        "min_ki_szgk = ki_szgk['Out_difference'].min()  # Get the min value\n",
        "max_ki_szgk = ki_szgk['Out_difference'].max()  # Get the max value\n",
        "min_be_szgk = be_szgk['In_difference'].min()\n",
        "max_be_szgk = be_szgk['In_difference'].max()\n",
        "\n",
        "min_index_ki_tk = ki_tk['Out_difference'].idxmin()  # Get index of the min value\n",
        "max_index_ki_tk = ki_tk['Out_difference'].idxmax()  # Get index of the max value\n",
        "min_index_be_tk = be_tk['In_difference'].idxmin()\n",
        "max_index_be_tk = be_tk['In_difference'].idxmax()\n",
        "\n",
        "min_ki_tk = ki_tk['Out_difference'].min()  # Get the min value\n",
        "max_ki_tk = ki_tk['Out_difference'].max()  # Get the max value\n",
        "min_be_tk = be_tk['In_difference'].min()\n",
        "max_be_tk = be_tk['In_difference'].max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE5zJvkoiOEC"
      },
      "source": [
        "###Difference maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiraJMcobgFr"
      },
      "outputs": [],
      "source": [
        "gdf = gpd.read_file('zone_zone.SHP')\n",
        "gdf = gdf[(gdf['NO'] >= 1000) & (gdf['NO'] <= 23999)]\n",
        "# Convert the GeoDataFrame to the same CRS as Folium (WGS84 - EPSG:4326)\n",
        "gdf = gdf.to_crs(epsg=4326)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZsaoiSVInhu"
      },
      "outputs": [],
      "source": [
        "def percent(values):\n",
        "  # Reverse the colormap to \"red_white_blue\"\n",
        "  cmap = mcolors.LinearSegmentedColormap.from_list(\"red_blue_cmap\", [\"red\", \"white\", \"blue\"])\n",
        "\n",
        "  # Function to assign colors based on the percentile range\n",
        "  def get_percentile_color(value, percentiles, cmap):\n",
        "      for i in range(len(percentiles) - 1):\n",
        "          if percentiles[i] <= value < percentiles[i + 1]:\n",
        "              color = cmap(i / (len(percentiles) - 1))  # Use red-to-blue colormap\n",
        "              return mcolors.to_hex(color)\n",
        "      return mcolors.to_hex(cmap(1.0))  # For the top 100th percentile\n",
        "\n",
        "  # Calculate percentiles\n",
        "  percentiles = np.percentile(values, np.arange(0, 101, 10))  # Divide into 10 percentiles (0-10, 10-20, ..., 90-100)\n",
        "\n",
        "  # Find the indices of min and max values\n",
        "  min_index_ki_tk = values.idxmin()\n",
        "  max_index_ki_tk = values.idxmax()\n",
        "  min_ki_tk = values.min()\n",
        "  max_ki_tk = values.max()\n",
        "\n",
        "  # Convert GeoDataFrame to WGS84 (EPSG:4326)\n",
        "  gdf_filtered = gdf.to_crs(epsg=4326)\n",
        "  gdf_filtered['percentile'] = np.searchsorted(percentiles, values, side='right')\n",
        "\n",
        "  # Create a base map\n",
        "  m = folium.Map(location=[gdf_filtered.geometry.centroid.y.mean(), gdf_filtered.geometry.centroid.x.mean()], zoom_start=10)\n",
        "\n",
        "  # Add shapes to the map with colors based on percentiles and handle min/max values\n",
        "  for idx, row in gdf_filtered.iterrows():\n",
        "      value = values[idx]\n",
        "      color = get_percentile_color(value, percentiles, cmap)  # Get the color based on percentile\n",
        "\n",
        "      # Assign tooltip for the zone\n",
        "      tooltip = f\"Value: {value}, Percentile: {gdf_filtered['percentile'][idx]}\"\n",
        "\n",
        "      # Add GeoJson for each shape\n",
        "      geojson = folium.GeoJson(\n",
        "          data={\n",
        "              'type': 'Feature',\n",
        "              'geometry': mapping(row['geometry']),\n",
        "              'properties': {'idx': idx}\n",
        "          },\n",
        "          style_function=lambda x, color=color: {\n",
        "              'fillColor': color,\n",
        "              'color': 'black',\n",
        "              'weight': 1,\n",
        "              'fillOpacity': 0.7\n",
        "          },\n",
        "          tooltip=tooltip  # Attach the tooltip with the value and percentile\n",
        "      )\n",
        "      geojson.add_to(m)\n",
        "\n",
        "  # Add a red circle for the min value zone\n",
        "  folium.CircleMarker(\n",
        "      location=[gdf_filtered.loc[min_index_ki_tk].geometry.centroid.y, gdf_filtered.loc[min_index_ki_tk].geometry.centroid.x],\n",
        "      radius=8,\n",
        "      color='red',\n",
        "      fill=True,\n",
        "      fill_color='red',\n",
        "      fill_opacity=1.0,\n",
        "      popup=f\"Min Value: {min_ki_tk}\"\n",
        "  ).add_to(m)\n",
        "\n",
        "  # Add a blue circle for the max value zone\n",
        "  folium.CircleMarker(\n",
        "      location=[gdf_filtered.loc[max_index_ki_tk].geometry.centroid.y, gdf_filtered.loc[max_index_ki_tk].geometry.centroid.x],\n",
        "      radius=8,\n",
        "      color='blue',\n",
        "      fill=True,\n",
        "      fill_color='blue',\n",
        "      fill_opacity=1.0,\n",
        "      popup=f\"Max Value: {max_ki_tk}\"\n",
        "  ).add_to(m)\n",
        "\n",
        "  # Create a reversed colormap legend using branca\n",
        "  colormap = cmb.LinearColormap(\n",
        "      colors=['red', 'white', 'blue'],\n",
        "      vmin=percentiles[0], vmax=percentiles[-1],\n",
        "      caption='Out Percentile Range'\n",
        "  )\n",
        "  colormap.add_to(m)  # Add the colormap to the map\n",
        "\n",
        "  # Display the map\n",
        "  #m.save('minmax_percentile_ki.html')\n",
        "  return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnonGd5kb6Lb"
      },
      "outputs": [],
      "source": [
        "values_ki = ki_tk.iloc[:, 2]\n",
        "percent(values_ki)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19yD4k_RAj-P"
      },
      "outputs": [],
      "source": [
        "values_be = be_tk.iloc[:, 2]\n",
        "percent(values_be)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkQ2B_WBC3pC"
      },
      "outputs": [],
      "source": [
        "# Define color ranges and corresponding intervals\n",
        "def get_custom_color(idx, value):\n",
        "    # Check if the shape would be colored purple or blue and if be_tk.iloc[idx, 1] is 0\n",
        "    if value < -450:\n",
        "        if ki_tk.iloc[idx, 1] == 0:\n",
        "            return '#000000'  # Black if be_tk.iloc[idx, 1] is 0\n",
        "        return '#800080'  # Purple for values < -450\n",
        "    elif -450 <= value < 0:\n",
        "        if ki_tk.iloc[idx, 1] == 0:\n",
        "            return '#000000'  # Black if be_tk.iloc[idx, 1] is 0\n",
        "        return '#0000FF'  # Blue for values between -450 and 0\n",
        "    elif value == 0:\n",
        "        return '#FFFF00'  # Yellow for values equal to 0\n",
        "    elif 0 < value <= 450:\n",
        "        return '#FFA500'  # Orange for values between 0 and 450\n",
        "    elif value > 450:\n",
        "        return '#FF0000'  # Red for values greater than 450\n",
        "    return '#FFFFFF'  # Default to white if something goes wrong\n",
        "\n",
        "def szines(values,get_custom_color):# Create a base map\n",
        "  gdf_filtered = gdf.to_crs(epsg=4326)\n",
        "  m = folium.Map(location=[gdf_filtered.geometry.centroid.y.mean(), gdf_filtered.geometry.centroid.x.mean()], zoom_start=10)\n",
        "\n",
        "  # Add each shape to the map with its color based on the value interval\n",
        "  for idx, row in gdf_filtered.iterrows():\n",
        "      value = values.iloc[idx]  # Get the value\n",
        "      color = get_custom_color(idx, value)  # Get the color based on the defined intervals and conditions\n",
        "\n",
        "      tooltip = f\"Value: {value}\"  # Tooltip with value\n",
        "\n",
        "      geojson = folium.GeoJson(\n",
        "          data={\n",
        "              'type': 'Feature',\n",
        "              'geometry': mapping(row['geometry']),\n",
        "              'properties': {'idx': idx}\n",
        "          },\n",
        "          style_function=lambda x, color=color: {\n",
        "              'fillColor': color,\n",
        "              'color': 'black',\n",
        "              'weight': 1,\n",
        "              'fillOpacity': 0.7\n",
        "          },\n",
        "          tooltip=tooltip  # Attach the tooltip with the value\n",
        "      )\n",
        "      geojson.add_to(m)\n",
        "\n",
        "  # Display the map\n",
        "  #m.save('szines_elotte_ki.html')\n",
        "  return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvQjuLjocz1T"
      },
      "outputs": [],
      "source": [
        "values_elotte_ki = ki_tk.iloc[:, 2]\n",
        "szines(values_elotte_ki,get_custom_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKWRCR8HC4E_"
      },
      "outputs": [],
      "source": [
        "values_elotte_be = be_tk.iloc[:, 2]\n",
        "szines(values_elotte_be,get_custom_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHoTq-Odv0Yt"
      },
      "outputs": [],
      "source": [
        "values_utana_ki = ki_tk.iloc[:, 1] - ki_tk.iloc[:, 0] - ki_szgk.iloc[:, 0]\n",
        "szines(values_utana_ki,get_custom_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHtL56r4v0dF"
      },
      "outputs": [],
      "source": [
        "values_utana_be = be_tk.iloc[:, 1] - be_tk.iloc[:, 0] - be_szgk.iloc[:, 0]\n",
        "szines(values_utana_be,get_custom_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrO7NWkGyUpK"
      },
      "outputs": [],
      "source": [
        "def get_custom_color2(idx, value):\n",
        "    if value < -450:\n",
        "        return '#800080'  # Purple\n",
        "    elif -450 <= value < 0:\n",
        "        return '#0000FF'  # Blue\n",
        "    elif value == 0:\n",
        "        return '#FFFF00'  # Yellow\n",
        "    elif 0 < value <= 450:\n",
        "        return '#FFA500'  # Orange\n",
        "    elif value > 450:\n",
        "        return '#FF0000'  # Red\n",
        "    return '#FFFFFF'  # Default to white if something goes wrong\n",
        "values_szgk_ki  = ki_szgk.iloc[:, 0]\n",
        "szines(values_szgk_ki,get_custom_color2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFXL0NtdE2E4"
      },
      "outputs": [],
      "source": [
        "values_szgk_be  = be_szgk.iloc[:, 0]\n",
        "szines(values_szgk_be,get_custom_color2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b2ktWYMi73h"
      },
      "source": [
        "### Fraction maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LstIxlBvi8UB"
      },
      "outputs": [],
      "source": [
        "ki_tk['Out_fraction'] =  ki_tk['OD_outvalue']/ ki_tk['calculated_outvalue']\n",
        "be_tk['In_fraction'] =  be_tk['OD_invalue']/ be_tk['calculated_invalue']\n",
        "\n",
        "ki_szgk['Out_fraction'] =  ki_szgk['OD_outvalue'] / ki_szgk['calculated_outvalue']\n",
        "be_szgk['In_fraction'] =  be_szgk['OD_invalue'] / be_szgk['calculated_invalue']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YhDC3-GjaNB"
      },
      "outputs": [],
      "source": [
        "min_index_ki_tk = ki_tk['Out_fraction'].idxmin()  # Get index of the min value\n",
        "max_index_ki_tk = ki_tk['Out_fraction'].idxmax()  # Get index of the max value\n",
        "min_index_be_tk = be_tk['In_fraction'].idxmin()\n",
        "max_index_be_tk = be_tk['In_fraction'].idxmax()\n",
        "\n",
        "min_ki_tk = ki_tk['Out_fraction'].min()  # Get the min value\n",
        "max_ki_tk = ki_tk['Out_fraction'].max()  # Get the max value\n",
        "min_be_tk = be_tk['In_fraction'].min()\n",
        "max_be_tk = be_tk['In_fraction'].max()\n",
        "\n",
        "min_index_ki_szgk = ki_szgk['Out_fraction'].idxmin()  # Get index of the min value\n",
        "max_index_ki_szgk = ki_szgk['Out_fraction'].idxmax()  # Get index of the max value\n",
        "min_index_be_szgk = be_szgk['In_fraction'].idxmin()\n",
        "max_index_be_szgk = be_szgk['In_fraction'].idxmax()\n",
        "\n",
        "min_ki_szgk = ki_szgk['Out_fraction'].min()  # Get the min value\n",
        "max_ki_szgk = ki_szgk['Out_fraction'].max()  # Get the max value\n",
        "min_be_szgk = be_szgk['In_fraction'].min()\n",
        "max_be_szgk = be_szgk['In_fraction'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtO23-hpcA9B"
      },
      "outputs": [],
      "source": [
        "def get_color(value):\n",
        "    if value < 1:\n",
        "        return '#00FF00'  # Green\n",
        "    elif value == 1:\n",
        "        return '#FFFF00'  # Yellow\n",
        "    else:\n",
        "        return '#FF0000'  # Red\n",
        "\n",
        "def aranyos(values,get_color):\n",
        "  gdf_filtered = gdf.to_crs(epsg=4326)\n",
        "  # Create a base map\n",
        "  m = folium.Map(location=[gdf_filtered.geometry.centroid.y.mean(), gdf_filtered.geometry.centroid.x.mean()], zoom_start=10)\n",
        "\n",
        "  # Add each shape to the map with its color based on the value interval\n",
        "  for idx, row in gdf_filtered.iterrows():\n",
        "      value = values.iloc[idx]  # Get the value\n",
        "      color = get_color(value)  # Get the color based on the defined intervals and conditions\n",
        "\n",
        "      tooltip = f\"Value: {value}\"  # Tooltip with value\n",
        "\n",
        "      geojson = folium.GeoJson(\n",
        "          data={\n",
        "              'type': 'Feature',\n",
        "              'geometry': mapping(row['geometry']),\n",
        "              'properties': {'idx': idx}\n",
        "          },\n",
        "          style_function=lambda x, color=color: {\n",
        "              'fillColor': color,\n",
        "              'color': 'black',\n",
        "              'weight': 1,\n",
        "              'fillOpacity': 0.7\n",
        "          },\n",
        "          tooltip=tooltip  # Attach the tooltip with the value\n",
        "      )\n",
        "      geojson.add_to(m)\n",
        "\n",
        "  # Display the map\n",
        "  #m.save('szines_arany_elotte_ki.html')\n",
        "  return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FKAphbVegRD"
      },
      "outputs": [],
      "source": [
        "values_arany_elotte_ki = ki_tk.iloc[:, 3]\n",
        "aranyos(values_arany_elotte_ki,get_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIZk3SlGcBIa"
      },
      "outputs": [],
      "source": [
        "values_arany_elotte_be = be_tk.iloc[:, 3]\n",
        "aranyos(values_arany_elotte_be,get_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzTuOkmmcBj4"
      },
      "outputs": [],
      "source": [
        "values_arany_utana_ki =  (ki_tk.iloc[:, 0] + ki_szgk.iloc[:, 0]) / ki_tk.iloc[:, 1]\n",
        "aranyos(values_arany_utana_ki,get_color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU3Q386WcBrS"
      },
      "outputs": [],
      "source": [
        "values_arany_utana_be =  (be_tk.iloc[:, 0] + be_szgk.iloc[:, 0]) / be_tk.iloc[:, 1]\n",
        "aranyos(values_arany_utana_be,get_color)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGKhjqmRIQZx"
      },
      "source": [
        "###Every_OD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxvp1eNy9DTB"
      },
      "outputs": [],
      "source": [
        "eredeti_OD_tk = df.iloc[2:924, 3:925].reset_index(drop=True)\n",
        "eredeti_OD_tk = eredeti_OD_tk / 24 * 1.1 # traffic in rush hours\n",
        "eredeti_OD_tk\n",
        "\n",
        "eredeti_OD_szgk = df2.iloc[2:924, 3:925].reset_index(drop=True)\n",
        "eredeti_OD_szgk = eredeti_OD_szgk * 1.5 / 24 * 1.1 # average number of people in a car\n",
        "eredeti_OD_szgk\n",
        "\n",
        "igeny_OD = eredeti_OD_tk + eredeti_OD_szgk\n",
        "igeny_OD.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7i38cnAinEy6"
      },
      "outputs": [],
      "source": [
        "new_index = np.arange(0, 922)\n",
        "igeny_OD.columns = new_index\n",
        "#eredeti_OD_tk.columns = new_index\n",
        "#igeny_OD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0JT91Tj8gG2"
      },
      "outputs": [],
      "source": [
        "def dijkstra_max_capacity(G, source):\n",
        "    \"\"\"Modified Dijkstra's algorithm to find paths with the maximum minimum edge weight, ensuring positive capacity.\"\"\"\n",
        "    # Priority queue: stores (-capacity, path length, current node, path)\n",
        "    queue = [(-float('inf'), 0, source, [])]\n",
        "    visited = {source: (-float('inf'), 0)}  # Stores the best (capacity, path length) found so far\n",
        "    paths = {}  # Store the path to reach each node\n",
        "\n",
        "    while queue:\n",
        "        # Pop the node with the highest capacity path\n",
        "        max_cap, length, node, path = heapq.heappop(queue)\n",
        "        max_cap = -max_cap\n",
        "        current_path = path + [node]\n",
        "\n",
        "        if node in paths:\n",
        "            continue\n",
        "\n",
        "        paths[node] = current_path\n",
        "\n",
        "        # Explore neighbors\n",
        "        for neighbor in G.neighbors(node):\n",
        "            edge_weight = G[node][neighbor]['weight']\n",
        "\n",
        "            # Skip edges with non-positive capacity\n",
        "            if edge_weight <= 0:\n",
        "                continue\n",
        "\n",
        "            # The capacity of the new path is the min of the current path capacity and the new edge weight\n",
        "            new_cap = min(max_cap, edge_weight)\n",
        "            new_length = length + 1\n",
        "\n",
        "            # If this path to neighbor has better capacity or is shorter with the same capacity\n",
        "            if neighbor not in visited or (new_cap > visited[neighbor][0]) or (new_cap == visited[neighbor][0] and new_length < visited[neighbor][1]):\n",
        "                visited[neighbor] = (new_cap, new_length)\n",
        "                heapq.heappush(queue, (-new_cap, new_length, neighbor, current_path))\n",
        "\n",
        "    return paths, visited"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBehLQRX8gNt"
      },
      "outputs": [],
      "source": [
        "# Initialize node_capacity_dict\n",
        "node_capacity_dict = {node: 0 for node in G4.nodes()}  # Initialize capacity counts for each node\n",
        "\n",
        "# List to store the final results\n",
        "all_routes = []\n",
        "\n",
        "# Get all nodes in the graph\n",
        "nodes = list(G4.nodes())\n",
        "\n",
        "# Iterate over all pairs of nodes\n",
        "for start_node in nodes:\n",
        "    # Run the modified Dijkstra's algorithm from the start node\n",
        "    paths, capacities = dijkstra_max_capacity(G4, start_node)\n",
        "\n",
        "    # Store the result for each end node\n",
        "    for end_node in nodes:\n",
        "        if end_node != start_node:\n",
        "            # First, consider the first and last nodes\n",
        "            if end_node in capacities:\n",
        "                # Path exists between start_node and end_node\n",
        "                path = paths[end_node]\n",
        "\n",
        "                # Get the first and last node in the path\n",
        "                first_node = path[0]\n",
        "                last_node = path[-1]\n",
        "\n",
        "                # Get first_zone and last_zone using node_shape_dict\n",
        "                try:\n",
        "                    first_zone = node_shape_dict[first_node]\n",
        "                    last_zone = node_shape_dict[last_node]\n",
        "\n",
        "                    # Fetch the OD value for this zone pair using a try-except block\n",
        "                    try:\n",
        "                        od_value = igeny_OD.at[first_zone, last_zone]\n",
        "                        #od_value = eredeti_OD_tk.at[first_zone, last_zone]\n",
        "                    except KeyError:\n",
        "                        od_value = 0  # Handle the case where the key is not found\n",
        "                        #print(f\"KeyError: Invalid zone combination: first_zone={first_zone}, last_zone={last_zone}\")\n",
        "\n",
        "                    # Increment the capacity in node_capacity_dict for each node in the path\n",
        "                    for node in path:\n",
        "                        node_capacity_dict[node] += (od_value)/(zone_dict[node_shape_dict[node]])\n",
        "\n",
        "                    # Get the max capacity for this path\n",
        "                    max_capacity = capacities[end_node][0]\n",
        "                    all_routes.append([first_node, last_node, max_capacity])\n",
        "\n",
        "                except KeyError:\n",
        "                    #print(f\"KeyError: Invalid node in node_shape_dict for first_node={first_node} or last_node={last_node}\")\n",
        "                    continue\n",
        "\n",
        "            else:\n",
        "                # Handle the case where there is no path from start_node to end_node\n",
        "                max_capacity = 0  # or some other default value indicating no path\n",
        "                all_routes.append([start_node, end_node, max_capacity])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LjLVkfjswFj"
      },
      "outputs": [],
      "source": [
        "# Calculate outgoing and incoming weights\n",
        "outgoing_weights = {}\n",
        "incoming_weights = {}\n",
        "\n",
        "for node in G4.nodes():\n",
        "    outgoing_weights[node] = sum(data['weight'] for _, _, data in G4.out_edges(node, data=True))\n",
        "    incoming_weights[node] = sum(data['weight'] for _, _, data in G4.in_edges(node, data=True))\n",
        "print(outgoing_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU761Mw4swKq"
      },
      "outputs": [],
      "source": [
        "# Normalize the weights for visualization\n",
        "max_weight1 = max(outgoing_weights.values())\n",
        "normalized_weights1 = {node: weight / max_weight1 for node, weight in outgoing_weights.items()}\n",
        "max_weight2 = max(incoming_weights.values())\n",
        "normalized_weights2 = {node: weight / max_weight2 for node, weight in incoming_weights.items()}\n",
        "print(normalized_weights1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfhnbGiAswWh"
      },
      "outputs": [],
      "source": [
        "def get_color2(weight):\n",
        "    cmap = mcolors.LinearSegmentedColormap.from_list(\"weight_cmap\", ['blue','green','yellow', 'orange', 'red'])\n",
        "    return mcolors.to_hex(cmap(weight))\n",
        "\n",
        "def buborek(normalized_weights, get_color):\n",
        "  m = folium.Map(location=[47.4979, 19.0402], zoom_start=12)\n",
        "  # Determine the maximum and minimum of the normalized weights\n",
        "  max_weight = max(normalized_weights.values())\n",
        "  min_weight = min(normalized_weights.values())\n",
        "\n",
        "  # Add nodes with sizes and colors based on normalized outgoing weights\n",
        "  for node, normalized_weight in normalized_weights.items():\n",
        "      # Normalize size by mapping weights linearly from the smallest size to the largest\n",
        "      size_factor = 10  # Base size factor, adjust to scale all circles appropriately\n",
        "      radius = size_factor * (normalized_weight - min_weight) / (max_weight - min_weight) + 5\n",
        "\n",
        "      folium.CircleMarker(\n",
        "          location=node,\n",
        "          radius=radius,\n",
        "          color=get_color(normalized_weight),\n",
        "          fill=True,\n",
        "          fill_color=get_color(normalized_weight),\n",
        "          fill_opacity=0.6,\n",
        "          popup=f'Outgoing weight: {outgoing_weights[node]} (Normalized: {normalized_weight:.2f})'\n",
        "      ).add_to(m)\n",
        "\n",
        "  # Add color map to the folium map\n",
        "  colormap = folium.StepColormap(\n",
        "      colors=['blue', 'green', 'yellow', 'orange', 'red'],\n",
        "      vmin=0, vmax=1,\n",
        "      index=[0, 0.25, 0.5, 0.75, 1],\n",
        "      caption='Heatmap Intensity'\n",
        "  )\n",
        "  m.add_child(colormap)\n",
        "  #m.save('buborek.html')\n",
        "  # Display the map\n",
        "  return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lkntZKTgZfC"
      },
      "outputs": [],
      "source": [
        "buborek(normalized_weights1,get_color2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajEauVbLgfmr"
      },
      "outputs": [],
      "source": [
        "buborek(normalized_weights2,get_color2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P6FCzHJuIVI"
      },
      "outputs": [],
      "source": [
        "difference_dict = {key: incoming_weights[key] - node_capacity_dict[key] for key in incoming_weights}\n",
        "max_weight = max(incoming_weights.values())\n",
        "normalized_weights_diff = {node: weight / max_weight for node, weight in difference_dict.items()}\n",
        "difference_dict2 = {key: outgoing_weights[key] - node_capacity_dict[key] for key in outgoing_weights}\n",
        "max_weight2 = max(outgoing_weights.values())\n",
        "normalized_weights_diff2 = {node: weight / max_weight2 for node, weight in difference_dict2.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLeV3VJpvzV_"
      },
      "outputs": [],
      "source": [
        "def get_color3(value):\n",
        "    if value > 0:\n",
        "        return '#00FF00'  # Green for positive values\n",
        "    else:\n",
        "        return '#FF0000'  # Red for negative values\n",
        "\n",
        "def buborek_diff (normalized_weights, get_color):\n",
        "  m = folium.Map(location=[47.4979, 19.0402], zoom_start=12)\n",
        "  # Get the minimum and maximum values of the normalized weights\n",
        "  min_weight = min(normalized_weights.values())\n",
        "  max_weight = max(normalized_weights.values())\n",
        "\n",
        "  # Create a custom colormap with red for negative and green for positive values\n",
        "  # Here we scale the colormap so that 0 is the midpoint (transition between red and green)\n",
        "  colormap = cmb.LinearColormap(\n",
        "      colors=['#FF0000', '#FFFFFF', '#00FF00'],  # Red -> White -> Green\n",
        "      vmin=min_weight,  # Minimum value is the minimum weight\n",
        "      vmax=max_weight,  # Maximum value is the maximum weight\n",
        "  ).to_step(n=2)  # You can adjust the number of steps for smoother transitions\n",
        "\n",
        "  # Add nodes with colors based on normalized outgoing weights\n",
        "  for node, normalized_weight in normalized_weights.items():\n",
        "      folium.CircleMarker(\n",
        "          location=node,\n",
        "          radius=8,  # Adjust the size of the markers as needed\n",
        "          color=get_color(normalized_weight),  # Use the color based on positive/negative\n",
        "          fill=True,\n",
        "          fill_color=get_color(normalized_weight),  # Fill color based on the same logic\n",
        "          fill_opacity=0.6,\n",
        "      ).add_to(m)\n",
        "\n",
        "  # Add the colormap to the map\n",
        "  colormap.add_to(m)\n",
        "\n",
        "  # Display the map\n",
        "  #m.save('megallos_utana_ki.html')\n",
        "  return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EhXqWuVhWCj"
      },
      "outputs": [],
      "source": [
        "buborek_diff(normalized_weights_diff,get_color3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXXgDZiWj-01"
      },
      "outputs": [],
      "source": [
        "buborek_diff(normalized_weights_diff2,get_color3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ib64U3JhyPC"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty list for red_stops\n",
        "red_stops = []\n",
        "\n",
        "# Iterate through difference_dict\n",
        "for coord, value in difference_dict.items():\n",
        "    if value < 0:  # Check if the value is negative\n",
        "        # Append the IDs from result_dict corresponding to the coordinate\n",
        "        #if coord in to_get_red:  # Check if the coordinate exists in result_dict\n",
        "            #red_stops.extend(to_get_red[coord])\n",
        "        red_stops.append(coord)\n",
        "\n",
        "# Print the resulting list of red_stops\n",
        "print(red_stops)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHhKAPhzsBBE"
      },
      "outputs": [],
      "source": [
        "# A set to store the route names\n",
        "matching_route_names = set()\n",
        "\n",
        "# Iterate through nodes in Ghelp\n",
        "for node in Ghelp.nodes:\n",
        "    # Check if the node's coordinates are in red_stops\n",
        "    if node in red_stops:\n",
        "        # Check outgoing edges\n",
        "        for _, end_node, key, data in Ghelp.out_edges(node, keys=True, data=True):\n",
        "            route_name = data.get('route_name')\n",
        "            if route_name:\n",
        "                matching_route_names.add(route_name)\n",
        "\n",
        "        # Check ingoing edges\n",
        "        for start_node, _, key, data in Ghelp.in_edges(node, keys=True, data=True):\n",
        "            route_name = data.get('route_name')\n",
        "            if route_name:\n",
        "                matching_route_names.add(route_name)\n",
        "\n",
        "# Print or return the set of route names\n",
        "print(matching_route_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5KmNmjC9Dxf"
      },
      "source": [
        "### Arányok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UReNdOYsyLqO"
      },
      "outputs": [],
      "source": [
        "# Initialize the stops_at_routes dictionary with sets to ensure uniqueness\n",
        "stops_at_routes = {}\n",
        "\n",
        "# Iterate through all the nodes in Ghelp (nodes are coordinates)\n",
        "for node in Ghelp.nodes:\n",
        "    # For all incoming edges\n",
        "    for u, v, data in Ghelp.in_edges(node, data=True):\n",
        "        route_name = data.get('route_name')  # Extract route name from edge data\n",
        "        if route_name:\n",
        "            if route_name in stops_at_routes:\n",
        "                # Add the current node (coordinate) to the set of this route\n",
        "                stops_at_routes[route_name].add(node)\n",
        "            else:\n",
        "                # Start a new set for this route with the current node (coordinate)\n",
        "                stops_at_routes[route_name] = {node}\n",
        "\n",
        "    # For all outgoing edges\n",
        "    for u, v, data in Ghelp.out_edges(node, data=True):\n",
        "        route_name = data.get('route_name')  # Extract route name from edge data\n",
        "        if route_name:\n",
        "            if route_name in stops_at_routes:\n",
        "                # Add the current node (coordinate) to the set of this route\n",
        "                stops_at_routes[route_name].add(node)\n",
        "            else:\n",
        "                # Start a new set for this route with the current node (coordinate)\n",
        "                stops_at_routes[route_name] = {node}\n",
        "\n",
        "# Convert the sets to lists to match the original specification\n",
        "stops_at_routes = {route: list(coords) for route, coords in stops_at_routes.items()}\n",
        "\n",
        "# Now stops_at_routes contains route names as keys and unique lists of coordinates as values\n",
        "print(stops_at_routes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCWQ9qc-4w85"
      },
      "outputs": [],
      "source": [
        "print(len(stops_at_routes['125']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss6Ln5rZ6hnF"
      },
      "outputs": [],
      "source": [
        "rate_dict = {\n",
        "    key: (node_capacity_dict[key] *100 / outgoing_weights[key]) if outgoing_weights[key] != 0 else float('inf')\n",
        "    for key in outgoing_weights\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLSmNQwV658J"
      },
      "outputs": [],
      "source": [
        "print(rate_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YucWqbXm8wqa"
      },
      "outputs": [],
      "source": [
        "def get_color4(value):\n",
        "    if value < 100:\n",
        "        return '#00FF00'  # Green for values less than 100\n",
        "    elif 100 <= value < 200:\n",
        "        return '#FFFF00'  # Yellow for values between 100 and 200\n",
        "    elif 200 <= value < 400:\n",
        "        return '#FFA500'  # Orange for values between 200 and 400\n",
        "    else:\n",
        "        return '#FF0000'  # Red for values above 400\n",
        "\n",
        "def arany_szines(rate_dict,get_color):\n",
        "  m = folium.Map(location=[47.4979, 19.0402], zoom_start=12)\n",
        "  # Get the minimum and maximum values of the difference_dict3\n",
        "  min_weight = min(rate_dict.values())\n",
        "  max_weight = max(rate_dict.values())\n",
        "\n",
        "  # Add nodes (coordinates) with colors based on the difference_dict3 values\n",
        "  for node, value in rate_dict.items():\n",
        "      folium.CircleMarker(\n",
        "          location=node,  # Node is the coordinate (latitude, longitude)\n",
        "          radius=8,  # Adjust the size of the markers as needed\n",
        "          color=get_color(value),  # Use the color based on the value\n",
        "          fill=True,\n",
        "          fill_color=get_color(value),  # Fill color based on the same logic\n",
        "          fill_opacity=0.6,\n",
        "      ).add_to(m)\n",
        "\n",
        "  # Display the map\n",
        "  #m.save('aranyok.html')\n",
        "  return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ck3yHSjh5GU"
      },
      "outputs": [],
      "source": [
        "arany_szines(rate_dict,get_color4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsVt8rQP9ORq"
      },
      "outputs": [],
      "source": [
        "stops_at_routes_modified = copy.deepcopy(stops_at_routes)\n",
        "\n",
        "# Iterate through the stops_at_routes dictionary\n",
        "for route_name, coords_list in stops_at_routes_modified.items():\n",
        "    # Replace each coordinate in the list with the corresponding value from difference_dict3\n",
        "    stops_at_routes_modified[route_name] = [\n",
        "        rate_dict.get(coord, coord)  # Replace if the coordinate is in difference_dict3, else keep it as is\n",
        "        for coord in coords_list\n",
        "    ]\n",
        "\n",
        "# Now stops_at_routes_modified contains routes with values from difference_dict3 replacing coordinates\n",
        "print(stops_at_routes_modified)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RftTKLIH9wKp"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "\n",
        "# Iterate through the stops_at_routes_modified dictionary\n",
        "for route_name, values_list in stops_at_routes_modified.items():\n",
        "    if values_list:  # Ensure the list is not empty\n",
        "        # Calculate min, max, average, and median for the list\n",
        "        min_val = min(values_list)\n",
        "        max_val = max(values_list)\n",
        "        avg_val = np.mean(values_list)\n",
        "        median_val = np.median(values_list)\n",
        "\n",
        "        # Append the data for this route\n",
        "        data.append([route_name, min_val, max_val, avg_val, median_val])\n",
        "\n",
        "# Create a DataFrame with the collected data\n",
        "usage_rate = pd.DataFrame(data, columns=['route_name', 'min', 'max', 'average', 'median'])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(usage_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhKgRBJQVCdJ"
      },
      "outputs": [],
      "source": [
        "usage_rate2 = usage_rate[(usage_rate['average'] != 0) & (~np.isinf(usage_rate['average']))]\n",
        "\n",
        "# Sort by 'average' to get the top 5 highest averages\n",
        "usage_rate2.sort_values(by='average', ascending=False).head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lauxz5bebUN5"
      },
      "outputs": [],
      "source": [
        "# Sort by 'average' to get the top 5 lowest averages\n",
        "usage_rate2.sort_values(by='average', ascending=True).head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVq56nTilgO8"
      },
      "outputs": [],
      "source": [
        "usage_rate.to_csv('aranyok.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "StqEOSykz5A5",
        "e2sGgdi--hHv"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
