{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wbZI8kPkqcoi"
      },
      "outputs": [],
      "source": [
        "#!pip install gtfs_functions\n",
        "#!pip install KeplerGl\n",
        "#!pip install keplergl jupyter\n",
        "#!jupyter nbextension install --py --symlink --sys-prefix keplergl\n",
        "#!jupyter nbextension enable --py --sys-prefix keplergl\n",
        "#!jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
        "#!pip install gtfs_functions matplotlib basemap\n",
        "#!pip install geopy\n",
        "#!pip install pyshp\n",
        "#!pip install partridge\n",
        "import matplotlib.pyplot as plt\n",
        "#from mpl_toolkits.basemap import Basemap\n",
        "import gtfs_functions as gtfs\n",
        "from gtfs_functions import Feed\n",
        "import networkx as nx\n",
        "import geopandas as gpd\n",
        "import folium\n",
        "from shapely.geometry import Point, LineString, Polygon, shape\n",
        "from folium.plugins import MarkerCluster\n",
        "#import keplergl as kp\n",
        "from google.colab import output\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "from gtfs_functions.gtfs_plots import map_gdf\n",
        "import plotly.graph_objs as go\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.cm as cm\n",
        "from geopy.distance import geodesic\n",
        "from math import radians, sin, cos, sqrt, atan2\n",
        "from itertools import combinations\n",
        "import numpy as np\n",
        "import shapefile as shp\n",
        "import seaborn as sns\n",
        "import os\n",
        "import copy\n",
        "import fiona\n",
        "import csv\n",
        "import math\n",
        "import colorsys\n",
        "from difflib import SequenceMatcher\n",
        "from folium.plugins import HeatMap\n",
        "import branca.colormap as cmb\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import partridge as ptg\n",
        "from shapely.ops import nearest_points\n",
        "from scipy.spatial import cKDTree\n",
        "from shapely.geometry import mapping\n",
        "import heapq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOozEGComA6C"
      },
      "source": [
        "###BKK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjGocdufqjnb"
      },
      "outputs": [],
      "source": [
        "feed_bkk = Feed(\"budapest_gtfs.zip\", time_windows=[6, 7]) #itt változtathatunk az időintervallumon\n",
        "#feed = Feed(\"budapest_gtfs.zip\")\n",
        "routes_bkk = feed_bkk.routes\n",
        "trips_bkk = feed_bkk.trips\n",
        "stops_bkk = feed_bkk.stops\n",
        "stop_times_bkk = feed_bkk.stop_times\n",
        "shapes_bkk = feed_bkk.shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwsioW_axDe8"
      },
      "outputs": [],
      "source": [
        "segments_gdf_bkk = feed_bkk.segments\n",
        "segments_gdf_bkk.head(2)\n",
        "#segments_gdf.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVbjxFKgxD8M"
      },
      "outputs": [],
      "source": [
        "segments_freq_bkk = feed_bkk.segments_freq\n",
        "segments_freq_bkk.head(2)\n",
        "#print(type(segments_freq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJ3KdPOUOSBZ"
      },
      "outputs": [],
      "source": [
        "colnames=['route_name', 'capacity']\n",
        "df = pd.read_csv('bkk_capacity.csv', names=colnames, header=None)\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouG-1nlhRuvS"
      },
      "outputs": [],
      "source": [
        "segments_freq_bkk = segments_freq_bkk.merge(df, on='route_name')\n",
        "segments_freq_bkk.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayhT_vZMABYj"
      },
      "outputs": [],
      "source": [
        "segments_freq_bkk = segments_freq_bkk.drop(columns = ['route_id','segment_name','start_stop_name','end_stop_name','window','min_per_trip','segment_id'])\n",
        "segments_freq_bkk.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgcXojQQmETL"
      },
      "source": [
        "###VOLÁN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2hp7tpMgb8P"
      },
      "outputs": [],
      "source": [
        "# Read the feed for the specified view\n",
        "feed = ptg.load_feed('volanbusz_gtfs.zip')\n",
        "\n",
        "# Access different components of the feed\n",
        "routes_volan = feed.routes\n",
        "trips_volan = feed.trips\n",
        "stops_volan = feed.stops\n",
        "stop_times_volan = feed.stop_times\n",
        "shapes_volan = feed.shapes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_times_volan.info()"
      ],
      "metadata": {
        "id": "dr75Vpo8UDcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert time to seconds since midnight\n",
        "def time_to_seconds(t):\n",
        "    try:\n",
        "        # Handle numeric times directly (e.g., 23100.0 seconds)\n",
        "        if isinstance(t, (int, float)):\n",
        "            return int(t)\n",
        "        # Handle HH:MM:SS format\n",
        "        h, m, s = map(int, t.split(':'))\n",
        "        return h * 3600 + m * 60 + s\n",
        "    except Exception as e:\n",
        "        return None  # Return None if there is an issue with conversion"
      ],
      "metadata": {
        "id": "Z-UeFdFpYj3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the conversion function to the departure_time column\n",
        "stop_times_volan['time_in_seconds'] = stop_times_volan['departure_time'].apply(time_to_seconds)\n",
        "\n",
        "# Define your desired time window in seconds\n",
        "start_time = time_to_seconds('06:00:00')\n",
        "end_time = time_to_seconds('07:00:00')\n",
        "\n",
        "# Filter the stop_times DataFrame based on the time window\n",
        "stop_times_volan = stop_times_volan[\n",
        "    (stop_times_volan['time_in_seconds'] >= start_time) &\n",
        "    (stop_times_volan['time_in_seconds'] <= end_time)\n",
        "]\n",
        "\n",
        "# Drop the helper column if you don't need it anymore\n",
        "stop_times_volan = stop_times_volan.drop(columns=['time_in_seconds'])"
      ],
      "metadata": {
        "id": "bzoLI0dHvxJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_times_volan.info()"
      ],
      "metadata": {
        "id": "gj6M0GORUk8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufNEztxbvThV"
      },
      "outputs": [],
      "source": [
        "def create_segments(stop_times, stops, trips, routes, shapes):\n",
        "    # Merge stop_times with stops to get stop coordinates\n",
        "    stop_times = stop_times.merge(stops[['stop_id', 'stop_lat', 'stop_lon']], on='stop_id')\n",
        "\n",
        "    # Merge stop_times with trips to get trip and route information\n",
        "    stop_times = stop_times.merge(trips[['trip_id', 'route_id', 'direction_id', 'shape_id']], on='trip_id')\n",
        "\n",
        "    # Merge stop_times with routes to get route_name\n",
        "    stop_times = stop_times.merge(routes[['route_id', 'route_short_name']], on='route_id')\n",
        "\n",
        "    # Sort stop_times by trip_id and stop_sequence\n",
        "    stop_times = stop_times.sort_values(['trip_id', 'stop_sequence'])\n",
        "\n",
        "    # Prepare shapes data\n",
        "    shapes = shapes.sort_values(['shape_id', 'shape_pt_sequence'])\n",
        "    shapes_grouped = shapes.groupby('shape_id')\n",
        "\n",
        "    segments = []\n",
        "    for trip_id, group in stop_times.groupby('trip_id'):\n",
        "        shape_id = group.iloc[0]['shape_id']\n",
        "        if shape_id in shapes_grouped.groups:\n",
        "            shape_points = shapes_grouped.get_group(shape_id)[['shape_pt_lon', 'shape_pt_lat']].values\n",
        "            for i in range(len(group) - 1):\n",
        "                from_stop = group.iloc[i]\n",
        "                to_stop = group.iloc[i + 1]\n",
        "                from_index = from_stop['stop_sequence']\n",
        "                to_index = to_stop['stop_sequence']\n",
        "                segment_shape_points = shape_points[from_index:to_index + 1]\n",
        "\n",
        "                if len(segment_shape_points) < 2:\n",
        "                    segment_shape_points = [(from_stop['stop_lon'], from_stop['stop_lat']),\n",
        "                                            (to_stop['stop_lon'], to_stop['stop_lat'])]\n",
        "\n",
        "                segment = {\n",
        "                    'trip_id': trip_id,\n",
        "                    'route_id': from_stop['route_id'],\n",
        "                    'route_short_name': from_stop['route_short_name'],\n",
        "                    'direction_id': from_stop['direction_id'],\n",
        "                    'from_stop_id': from_stop['stop_id'],\n",
        "                    'to_stop_id': to_stop['stop_id'],\n",
        "                    'geometry': LineString(segment_shape_points)\n",
        "                }\n",
        "                segments.append(segment)\n",
        "\n",
        "    segments_df = pd.DataFrame(segments)\n",
        "    return gpd.GeoDataFrame(segments_df, geometry='geometry')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJfUrd8DCz6S"
      },
      "outputs": [],
      "source": [
        "segments_gdf_volan = create_segments(stop_times_volan, stops_volan, trips_volan, routes_volan, shapes_volan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNCj4KS5gb-l"
      },
      "outputs": [],
      "source": [
        "def create_segments_freq(segments_gdf):\n",
        "    # Group by from_stop_id, to_stop_id, route_id, route_short_name, direction_id and count the occurrences\n",
        "    segments_freq = segments_gdf.groupby(['from_stop_id', 'to_stop_id', 'route_id', 'route_short_name', 'direction_id']).size().reset_index(name='frequency')\n",
        "\n",
        "    # Merge with the segments to get the geometries\n",
        "    segments_freq = segments_freq.merge(segments_gdf[['from_stop_id', 'to_stop_id', 'route_id', 'route_short_name', 'direction_id', 'geometry']],\n",
        "                                        on=['from_stop_id', 'to_stop_id', 'route_id', 'route_short_name', 'direction_id']).drop_duplicates()\n",
        "\n",
        "    # Rename columns as requested\n",
        "    segments_freq = segments_freq.rename(columns={\n",
        "        'from_stop_id': 'start_stop_id',\n",
        "        'to_stop_id': 'end_stop_id',\n",
        "        'route_short_name': 'route_name',\n",
        "        'frequency': 'ntrips'\n",
        "    })\n",
        "\n",
        "    segments_freq = segments_freq.drop(columns=['route_id'])\n",
        "\n",
        "    return gpd.GeoDataFrame(segments_freq, geometry='geometry')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk2eopFbMzJd"
      },
      "outputs": [],
      "source": [
        "# Create segment frequencies GeoDataFrame\n",
        "segments_freq_volan = create_segments_freq(segments_gdf_volan)\n",
        "\n",
        "# Display segment frequencies\n",
        "#print(segments_freq_volan.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5IuvptBHq-A"
      },
      "outputs": [],
      "source": [
        "# Filter segments_freq_gdf where route_name is 315\n",
        "filtered_segments_freq = segments_freq_volan[segments_freq_volan['route_name'] == '315']\n",
        "\n",
        "# Display filtered segment frequencies\n",
        "#print(filtered_segments_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1UMaPOyldYj"
      },
      "outputs": [],
      "source": [
        "colnames=['route_name', 'capacity']\n",
        "df = pd.read_csv('volan_capacity.csv', names=colnames, header=None)\n",
        "\n",
        "df['route_name'] = df['route_name'].astype(str)\n",
        "\n",
        "# Extract route names and capacity values\n",
        "route_names = df['route_name'].tolist()\n",
        "capacity_dict = dict(zip(df['route_name'], df['capacity']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgMmB38mqLCd"
      },
      "outputs": [],
      "source": [
        "def filter_segment_freq(segments_freq,route_names,capacity_dict):\n",
        "  segments_freq['route_name'] = segments_freq['route_name'].astype(str).str.strip()\n",
        "\n",
        "  # Filter the GeoDataFrame based on route names\n",
        "  segments_freq = segments_freq[segments_freq['route_name'].isin(route_names)]\n",
        "  segments_freq['capacity'] = segments_freq['route_name'].map(capacity_dict)\n",
        "  # Ensure the CRS for the geometry GeoDataFrame if not set\n",
        "  if segments_freq.crs is None:\n",
        "      segments_freq.set_crs(epsg=4326, inplace=True)\n",
        "\n",
        "  # Define the path to the shapefile\n",
        "  shapefile_path = 'zone_zone.SHP'\n",
        "\n",
        "  # Check if the shapefile exists\n",
        "  if not os.path.exists(shapefile_path):\n",
        "      raise FileNotFoundError(f\"Shapefile not found: {shapefile_path}\")\n",
        "\n",
        "  # Load the shapefile using Fiona\n",
        "  with fiona.open(shapefile_path) as shapefile:\n",
        "      # Ensure shapefile has a CRS\n",
        "      if shapefile.crs is None:\n",
        "          # Replace 'EPSG:4326' with the correct CRS if known\n",
        "          shapefile_crs = 'EPSG:4326'\n",
        "      else:\n",
        "          shapefile_crs = shapefile.crs\n",
        "\n",
        "      # Convert the shapefile features to GeoDataFrame\n",
        "      shapefile_gdf = gpd.GeoDataFrame.from_features(shapefile, crs=shapefile_crs)\n",
        "\n",
        "      # Filter the shapefile GeoDataFrame based on the 'NO' attribute\n",
        "      shapefile_filtered = shapefile_gdf[(shapefile_gdf['NO'] >= 1000) & (shapefile_gdf['NO'] <= 23999)]\n",
        "      #shapefile_filtered = shapefile_gdf\n",
        "\n",
        "      # Ensure CRS match\n",
        "      if shapefile_filtered.crs != segments_freq.crs:\n",
        "          shapefile_filtered = shapefile_filtered.to_crs(segments_freq.crs)\n",
        "\n",
        "      # Convert filtered shapes to Shapely geometries\n",
        "      filtered_shapes = [shape(geom_shape) for geom_shape in shapefile_filtered.geometry]\n",
        "\n",
        "      # Function to check if a point is within any of the city shapes\n",
        "      def point_in_city_shapes(point):\n",
        "          return any(city_shape.contains(point) for city_shape in filtered_shapes)\n",
        "\n",
        "      # Create masks to filter rows\n",
        "      mask_first_in_city = segments_freq.geometry.apply(lambda line: point_in_city_shapes(Point(line.coords[0])))\n",
        "      mask_last_in_city = segments_freq.geometry.apply(lambda line: point_in_city_shapes(Point(line.coords[-1])))\n",
        "\n",
        "      # Filter rows where either the first or last point is not in the city shapes\n",
        "      segments_freq = segments_freq[mask_first_in_city & mask_last_in_city]\n",
        "\n",
        "      return segments_freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEG33kCf1A-s"
      },
      "outputs": [],
      "source": [
        "segments_freq_volan = filter_segment_freq(segments_freq_volan, route_names, capacity_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33eFd2jTMXHB"
      },
      "outputs": [],
      "source": [
        "segments_freq_volan.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXJEAVZAmJvr"
      },
      "source": [
        "###MÁV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hgbma8YpCWWL"
      },
      "outputs": [],
      "source": [
        "# Read the feed for the specified view\n",
        "feed_mav = ptg.load_feed('mav_gtfs.zip')\n",
        "\n",
        "# Access different components of the feed\n",
        "routes_mav = feed_mav.routes\n",
        "trips_mav = feed_mav.trips\n",
        "stops_mav = feed_mav.stops\n",
        "stop_times_mav = feed_mav.stop_times\n",
        "shapes_mav = feed_mav.shapes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the conversion function to the departure_time column\n",
        "stop_times_mav['time_in_seconds'] = stop_times_mav['departure_time'].apply(time_to_seconds)\n",
        "\n",
        "# Define your desired time window in seconds\n",
        "start_time = time_to_seconds('06:00:00')\n",
        "end_time = time_to_seconds('07:00:00')\n",
        "\n",
        "# Filter the stop_times DataFrame based on the time window\n",
        "stop_times_mav = stop_times_mav[\n",
        "    (stop_times_mav['time_in_seconds'] >= start_time) &\n",
        "    (stop_times_mav['time_in_seconds'] <= end_time)\n",
        "]\n",
        "\n",
        "# Drop the helper column if you don't need it anymore\n",
        "stop_times_mav = stop_times_mav.drop(columns=['time_in_seconds'])"
      ],
      "metadata": {
        "id": "xiLe9x3NYrxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tam71bXR3d_K"
      },
      "outputs": [],
      "source": [
        "def adjust_coordinates(stops, shapes, decimal_places=2):\n",
        "    # Round coordinates in stops and shapes to the same number of decimal places\n",
        "    stops['stop_lat'] = stops['stop_lat'].round(decimal_places)\n",
        "    stops['stop_lon'] = stops['stop_lon'].round(decimal_places)\n",
        "    shapes['shape_pt_lat'] = shapes['shape_pt_lat'].round(decimal_places)\n",
        "    shapes['shape_pt_lon'] = shapes['shape_pt_lon'].round(decimal_places)\n",
        "\n",
        "    # Create a cKDTree for fast nearest-neighbor lookup\n",
        "    shape_coords = shapes[['shape_pt_lon', 'shape_pt_lat']].values\n",
        "    tree = cKDTree(shape_coords)\n",
        "\n",
        "    # Adjust stop coordinates to match closest shape point if necessary\n",
        "    stop_points = stops[['stop_lon', 'stop_lat']].values\n",
        "    distances, indices = tree.query(stop_points)\n",
        "\n",
        "    # Update stops with the nearest shape point coordinates\n",
        "    nearest_points = shape_coords[indices]\n",
        "    stops['adjusted_stop_lon'] = nearest_points[:, 0]\n",
        "    stops['adjusted_stop_lat'] = nearest_points[:, 1]\n",
        "\n",
        "    return stops\n",
        "\n",
        "# Adjust coordinates in the stops dataframe\n",
        "stops_mav = adjust_coordinates(stops_mav, shapes_mav)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6b2GGdQDTpj"
      },
      "outputs": [],
      "source": [
        "def create_segments2(stop_times, stops, trips, routes, shapes):\n",
        "    # Merge stop_times with stops to get stop coordinates\n",
        "    stop_times = stop_times.merge(stops[['stop_id', 'stop_lat', 'stop_lon']], on='stop_id')\n",
        "\n",
        "    # Merge stop_times with trips to get trip and route information\n",
        "    stop_times = stop_times.merge(trips[['trip_id', 'route_id', 'shape_id']], on='trip_id')\n",
        "\n",
        "    # Merge stop_times with routes to get route_name\n",
        "    stop_times = stop_times.merge(routes[['route_id', 'route_short_name']], on='route_id')\n",
        "\n",
        "    # Sort stop_times by trip_id and stop_sequence\n",
        "    stop_times = stop_times.sort_values(['trip_id', 'stop_sequence'])\n",
        "\n",
        "    # Prepare shapes data\n",
        "    shapes = shapes.sort_values(['shape_id', 'shape_pt_sequence'])\n",
        "    shapes_grouped = shapes.groupby('shape_id')\n",
        "\n",
        "    segments = []\n",
        "    for trip_id, group in stop_times.groupby('trip_id'):\n",
        "        shape_id = group.iloc[0]['shape_id']\n",
        "        if shape_id in shapes_grouped.groups:\n",
        "            shape_points = shapes_grouped.get_group(shape_id)[['shape_pt_lon', 'shape_pt_lat']].values.tolist()\n",
        "            for i in range(len(group) - 1):\n",
        "                from_stop = group.iloc[i]\n",
        "                to_stop = group.iloc[i + 1]\n",
        "\n",
        "                # Find the indices of the stops in the shape points\n",
        "                from_index = next((idx for idx, point in enumerate(shape_points) if (point[0], point[1]) == (from_stop['stop_lon'], from_stop['stop_lat'])), None)\n",
        "                to_index = next((idx for idx, point in enumerate(shape_points) if (point[0], point[1]) == (to_stop['stop_lon'], to_stop['stop_lat'])), None)\n",
        "\n",
        "                # If either index is not found, default to direct line between stops\n",
        "                if from_index is None or to_index is None or from_index >= to_index:\n",
        "                    segment_shape_points = [(from_stop['stop_lon'], from_stop['stop_lat']),\n",
        "                                            (to_stop['stop_lon'], to_stop['stop_lat'])]\n",
        "                else:\n",
        "                    segment_shape_points = shape_points[from_index:to_index + 1]\n",
        "\n",
        "                segment = {\n",
        "                    'trip_id': trip_id,\n",
        "                    'route_id': from_stop['route_id'],\n",
        "                    'route_short_name': from_stop['route_short_name'],\n",
        "                    'from_stop_id': from_stop['stop_id'],\n",
        "                    'to_stop_id': to_stop['stop_id'],\n",
        "                    'geometry': LineString(segment_shape_points)\n",
        "                }\n",
        "                segments.append(segment)\n",
        "\n",
        "    segments_df = pd.DataFrame(segments)\n",
        "    return gpd.GeoDataFrame(segments_df, geometry='geometry')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9nz3ieDi6WV"
      },
      "outputs": [],
      "source": [
        "def create_segments3(stop_times, stops, trips, routes, shapes, decimal_places=6):\n",
        "    # Round coordinates in stops and shapes to the same number of decimal places\n",
        "    stops['stop_lat'] = stops['stop_lat'].round(decimal_places)\n",
        "    stops['stop_lon'] = stops['stop_lon'].round(decimal_places)\n",
        "    shapes['shape_pt_lat'] = shapes['shape_pt_lat'].round(decimal_places)\n",
        "    shapes['shape_pt_lon'] = shapes['shape_pt_lon'].round(decimal_places)\n",
        "    # Merge stop_times with stops to get stop coordinates\n",
        "    stop_times = stop_times.merge(stops[['stop_id', 'stop_lat', 'stop_lon']], on='stop_id')\n",
        "\n",
        "    # Merge stop_times with trips to get trip and route information\n",
        "    stop_times = stop_times.merge(trips[['trip_id', 'route_id', 'shape_id']], on='trip_id')\n",
        "\n",
        "    # Merge stop_times with routes to get route_name\n",
        "    stop_times = stop_times.merge(routes[['route_id', 'route_short_name']], on='route_id')\n",
        "\n",
        "    # Sort stop_times by trip_id and stop_sequence\n",
        "    stop_times = stop_times.sort_values(['trip_id', 'stop_sequence'])\n",
        "\n",
        "    # Prepare shapes data\n",
        "    shapes = shapes.sort_values(['shape_id', 'shape_pt_sequence'])\n",
        "    shapes_grouped = shapes.groupby('shape_id')\n",
        "\n",
        "    segments = []\n",
        "    for trip_id, group in stop_times.groupby('trip_id'):\n",
        "        shape_id = group.iloc[0]['shape_id']\n",
        "        if shape_id in shapes_grouped.groups:\n",
        "            shape_points = shapes_grouped.get_group(shape_id)[['shape_pt_lon', 'shape_pt_lat']].values.tolist()\n",
        "            for i in range(len(group) - 1):\n",
        "                from_stop = group.iloc[i]\n",
        "                to_stop = group.iloc[i + 1]\n",
        "\n",
        "                # Find the indices of the stops in the shape points\n",
        "                from_index = next((idx for idx, point in enumerate(shape_points) if (point[0], point[1]) == (from_stop['stop_lon'], from_stop['stop_lat'])), None)\n",
        "                to_index = next((idx for idx, point in enumerate(shape_points) if (point[0], point[1]) == (to_stop['stop_lon'], to_stop['stop_lat'])), None)\n",
        "\n",
        "                # If either index is not found, default to direct line between stops\n",
        "                if from_index is None or to_index is None or from_index >= to_index:\n",
        "                    segment_shape_points = [(from_stop['stop_lon'], from_stop['stop_lat']),\n",
        "                                            (to_stop['stop_lon'], to_stop['stop_lat'])]\n",
        "                else:\n",
        "                    segment_shape_points = shape_points[from_index:to_index + 1]\n",
        "\n",
        "                # Ensure segment_shape_points has more than 2 points for the LineString\n",
        "                if len(segment_shape_points) < 2:\n",
        "                    segment_shape_points = [(from_stop['stop_lon'], from_stop['stop_lat']),\n",
        "                                            (to_stop['stop_lon'], to_stop['stop_lat'])]\n",
        "\n",
        "                segment = {\n",
        "                    'trip_id': trip_id,\n",
        "                    'route_id': from_stop['route_id'],\n",
        "                    'route_short_name': from_stop['route_short_name'],\n",
        "                    'from_stop_id': from_stop['stop_id'],\n",
        "                    'to_stop_id': to_stop['stop_id'],\n",
        "                    'geometry': LineString(segment_shape_points)\n",
        "                }\n",
        "                segments.append(segment)\n",
        "\n",
        "    segments_df = pd.DataFrame(segments)\n",
        "    return gpd.GeoDataFrame(segments_df, geometry='geometry')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_segments4(stop_times, stops, trips, routes, shapes):\n",
        "    # Merge stop_times with stops to get adjusted stop coordinates\n",
        "    stop_times = stop_times.merge(stops[['stop_id', 'adjusted_stop_lat', 'adjusted_stop_lon']], on='stop_id')\n",
        "\n",
        "    # Merge stop_times with trips to get trip and route information\n",
        "    stop_times = stop_times.merge(trips[['trip_id', 'route_id', 'shape_id']], on='trip_id')\n",
        "\n",
        "    # Merge stop_times with routes to get route_name\n",
        "    stop_times = stop_times.merge(routes[['route_id', 'route_short_name']], on='route_id')\n",
        "\n",
        "    # Sort stop_times by trip_id and stop_sequence\n",
        "    stop_times = stop_times.sort_values(['trip_id', 'stop_sequence'])\n",
        "\n",
        "    # Prepare shapes data\n",
        "    shapes = shapes.sort_values(['shape_id', 'shape_pt_sequence'])\n",
        "    shapes_grouped = shapes.groupby('shape_id')\n",
        "\n",
        "    segments = []\n",
        "    for trip_id, group in stop_times.groupby('trip_id'):\n",
        "        shape_id = group.iloc[0]['shape_id']\n",
        "        if shape_id in shapes_grouped.groups:\n",
        "            shape_points = shapes_grouped.get_group(shape_id)[['shape_pt_lon', 'shape_pt_lat']].values.tolist()\n",
        "\n",
        "            for i in range(len(group) - 1):\n",
        "                from_stop = group.iloc[i]\n",
        "                to_stop = group.iloc[i + 1]\n",
        "\n",
        "                from_point = (from_stop['adjusted_stop_lon'], from_stop['adjusted_stop_lat'])\n",
        "                to_point = (to_stop['adjusted_stop_lon'], to_stop['adjusted_stop_lat'])\n",
        "\n",
        "                # Find the indices of the stops in the shape points\n",
        "                from_index = next((idx for idx, point in enumerate(shape_points) if np.allclose(point, from_point, atol=1e-6)), None)\n",
        "                to_index = next((idx for idx, point in enumerate(shape_points) if np.allclose(point, to_point, atol=1e-6)), None)\n",
        "\n",
        "                if from_index is not None and to_index is not None and from_index < to_index:\n",
        "                    segment_shape_points = shape_points[from_index:to_index + 1]\n",
        "                else:\n",
        "                    segment_shape_points = [from_point, to_point]\n",
        "\n",
        "                segment = {\n",
        "                    'trip_id': trip_id,\n",
        "                    'route_id': from_stop['route_id'],\n",
        "                    'route_short_name': from_stop['route_short_name'],\n",
        "                    'from_stop_id': from_stop['stop_id'],\n",
        "                    'to_stop_id': to_stop['stop_id'],\n",
        "                    'geometry': LineString(segment_shape_points)\n",
        "                }\n",
        "                segments.append(segment)\n",
        "\n",
        "    segments_df = pd.DataFrame(segments)\n",
        "    return gpd.GeoDataFrame(segments_df, geometry='geometry')"
      ],
      "metadata": {
        "id": "nUTGyDwW-4lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OhICpb0monTq"
      },
      "outputs": [],
      "source": [
        "segments_mav = create_segments(stop_times_mav, stops_mav, trips_mav, routes_mav, shapes_mav)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t53f7R4CWZi"
      },
      "outputs": [],
      "source": [
        "segments_mav = create_segments2(stop_times_mav, stops_mav, trips_mav, routes_mav, shapes_mav)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCYvPPvTcvs1"
      },
      "outputs": [],
      "source": [
        "segments_mav['direction_id'] = 0\n",
        "segments_mav.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jC5CVC0ERqOS"
      },
      "outputs": [],
      "source": [
        "print(segments_mav.geometry.iloc[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhTAUIWymw-L"
      },
      "outputs": [],
      "source": [
        "def create_segments_freq2(segments_gdf, stop_times):\n",
        "    # Check and print columns of the segments_gdf for verification\n",
        "    print(\"Segments DataFrame columns:\", segments_gdf.columns)\n",
        "\n",
        "    # Calculate trip frequencies\n",
        "    trip_frequencies = stop_times.groupby(['trip_id']).size().reset_index(name='trip_count')\n",
        "\n",
        "    # Merge trip frequencies with segments\n",
        "    segments_with_freq = segments_gdf.merge(trip_frequencies, on='trip_id', how='left')\n",
        "\n",
        "    # Check and print columns of the merged DataFrame for verification\n",
        "    print(\"Merged DataFrame columns:\", segments_with_freq.columns)\n",
        "\n",
        "    # Group by the necessary keys and aggregate trip counts\n",
        "    segment_keys = ['from_stop_id', 'to_stop_id', 'route_id', 'route_short_name', 'geometry']\n",
        "    segments_freq = segments_with_freq.groupby(segment_keys).agg({'trip_count': 'sum'}).reset_index()\n",
        "\n",
        "    # Rename columns as requested\n",
        "    segments_freq = segments_freq.rename(columns={\n",
        "        'from_stop_id': 'start_stop_id',\n",
        "        'to_stop_id': 'end_stop_id',\n",
        "        'route_short_name': 'route_name',\n",
        "        'trip_count': 'ntrips'\n",
        "    })\n",
        "\n",
        "    # Ensure that segments_freq is a GeoDataFrame\n",
        "    segments_freq_gdf = gpd.GeoDataFrame(segments_freq, geometry='geometry')\n",
        "\n",
        "    return segments_freq_gdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDviVVbncign"
      },
      "outputs": [],
      "source": [
        "segments_freq_mav = create_segments_freq(segments_mav)\n",
        "#segments_freq_mav = create_segments_freq2(segments_mav,stop_times_mav)\n",
        "segments_freq_mav.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtPUGvlj-zsR"
      },
      "outputs": [],
      "source": [
        "colnames=['route_name', 'capacity']\n",
        "df2 = pd.read_csv('mav_capacity.csv', names=colnames, header=None)\n",
        "\n",
        "df2['route_name'] = df2['route_name'].astype(str)\n",
        "\n",
        "# Extract route names and capacity values\n",
        "route_names2 = df2['route_name'].tolist()\n",
        "capacity_dict2 = dict(zip(df2['route_name'], df2['capacity']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsT3AKNJ-zyx"
      },
      "outputs": [],
      "source": [
        "segments_freq_mav = filter_segment_freq(segments_freq_mav, route_names2, capacity_dict2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xusvaSePldbL"
      },
      "outputs": [],
      "source": [
        "desired_crs = 'EPSG:4326'\n",
        "\n",
        "# Ensure both GeoDataFrames have the same CRS\n",
        "if segments_freq_bkk.crs != desired_crs:\n",
        "    segments_freq_bkk = segments_freq_bkk.to_crs(desired_crs)\n",
        "if segments_freq_volan.crs != desired_crs:\n",
        "    segments_freq_volan = segments_freq_volan.to_crs(desired_crs)\n",
        "if segments_freq_mav.crs != desired_crs:\n",
        "    segments_freq_mav = segments_freq_mav.to_crs(desired_crs)\n",
        "\n",
        "# Concatenate the GeoDataFrames\n",
        "segments_freq = pd.concat([segments_freq_bkk, segments_freq_volan, segments_freq_mav], ignore_index=True)\n",
        "\n",
        "# Ensure the result is still a GeoDataFrame\n",
        "segments_freq = gpd.GeoDataFrame(segments_freq, geometry='geometry')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grGLUde3InIv"
      },
      "outputs": [],
      "source": [
        "print(segments_freq.geometry.iloc[12])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vnIpKk3mUe7"
      },
      "source": [
        "###Joint heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIpEzwzqHniT"
      },
      "outputs": [],
      "source": [
        "seq_copy = copy.deepcopy(segments_freq)\n",
        "seq_copy['weight'] = seq_copy['ntrips'] * seq_copy['capacity']\n",
        "\n",
        "# Step 2: Normalize the weights for visualization\n",
        "seq_copy['normalized_weight'] = seq_copy['weight'] / seq_copy['weight'].max()\n",
        "\n",
        "# Step 3: Create a folium map centered on the mean coordinates of the data\n",
        "mean_x, mean_y = seq_copy.geometry.centroid.x.mean(), seq_copy.geometry.centroid.y.mean()\n",
        "m = folium.Map(location=[mean_y, mean_x], zoom_start=12)\n",
        "\n",
        "# Step 4: Add the lines to the map\n",
        "colormap = cmb.LinearColormap(colors=['blue', 'green', 'yellow', 'red'], vmin=0, vmax=1)\n",
        "for _, row in seq_copy.iterrows():\n",
        "    weight = row['normalized_weight']\n",
        "    # Extracting coordinates directly\n",
        "    locations = [(coord[1], coord[0]) for coord in row['geometry'].coords]\n",
        "    folium.PolyLine(\n",
        "        locations=locations,\n",
        "        weight=weight * 10 + 0.1,  # Adjust thickness\n",
        "        color=colormap(weight),  # Color based on weight\n",
        "        opacity=0.7\n",
        "    ).add_to(m)\n",
        "\n",
        "# Step 5: Add colormap to the map\n",
        "colormap.caption = 'Heatmap Intensity'\n",
        "m.add_child(colormap)\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StqEOSykz5A5"
      },
      "source": [
        "###Multigraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dupY6QR232s6"
      },
      "outputs": [],
      "source": [
        "def add_edges(G, gdf):\n",
        "    for idx, row in gdf.iterrows():\n",
        "        # Extract start and end coordinates\n",
        "        start_coords = (row['geometry'].coords[0][1], row['geometry'].coords[0][0])\n",
        "        end_coords = (row['geometry'].coords[-1][1], row['geometry'].coords[-1][0])\n",
        "\n",
        "        # Calculate the product of ntrips and capacity\n",
        "        ntrips_capacity = row['ntrips'] * row['capacity']\n",
        "\n",
        "        # Create a unique key for the edge\n",
        "        key = f\"{row['route_name']}_{ntrips_capacity}\"\n",
        "\n",
        "        # Add edge to the graph with specified keys and additional attributes\n",
        "        if row['direction_id'] == 0:\n",
        "          G.add_edge(start_coords, end_coords, key=key, route_name=row['route_name'], ntrips_capacity=ntrips_capacity)\n",
        "        elif row['direction_id'] == 1:\n",
        "          G.add_edge(end_coords, start_coords, key=key, route_name=row['route_name'], ntrips_capacity=ntrips_capacity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQnHmUeO37s8"
      },
      "outputs": [],
      "source": [
        "G = nx.MultiDiGraph()\n",
        "# Add edges from segments_freq to the graph\n",
        "add_edges(G, segments_freq)\n",
        "\n",
        "# Display basic information about the graph\n",
        "#print(info(G))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxKV_siP4afb"
      },
      "outputs": [],
      "source": [
        "def create_folium_map(G):\n",
        "    # Create a folium map centered around Budapest\n",
        "    m = folium.Map(location=[47.4979, 19.0402], zoom_start=12)\n",
        "\n",
        "    # Add edges to the folium map with tooltips\n",
        "    for u, v, data in G.edges(data=True):\n",
        "        # Create the coordinates list for the PolyLine\n",
        "        coords = [u, v]\n",
        "\n",
        "        # Create the tooltip text\n",
        "        tooltip_text = f\"Route: {data['route_name']}<br>Capacity: {data['ntrips_capacity']}\"\n",
        "\n",
        "        # Add the PolyLine to the map with the tooltip\n",
        "        folium.PolyLine(coords, color='blue', weight=2.5, opacity=1, tooltip=tooltip_text).add_to(m)\n",
        "\n",
        "    # Add nodes to the folium map\n",
        "    for node in G.nodes():\n",
        "        folium.CircleMarker(location=node, radius=2, color='red').add_to(m)\n",
        "\n",
        "    return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XqplBQqIS1l"
      },
      "outputs": [],
      "source": [
        "def create_folium_map_multicolor(G):\n",
        "    # Create a folium map centered around Budapest\n",
        "    m = folium.Map(location=[47.4979, 19.0402], zoom_start=12)\n",
        "\n",
        "    # Generate a list of distinct colors\n",
        "    colors = list(mcolors.CSS4_COLORS.values())\n",
        "    max_colors = min(len(colors), 50)  # Limit to 30 distinct colors if there are fewer colors available\n",
        "    colors = colors[:max_colors]\n",
        "\n",
        "    # Create a dictionary to track the number of parallel edges\n",
        "    edge_count = {}\n",
        "\n",
        "    # Add edges to the folium map with tooltips\n",
        "    for u, v, key, data in G.edges(keys=True, data=True):\n",
        "        # Create the coordinates list for the PolyLine\n",
        "        coords = [u, v]\n",
        "\n",
        "        # Create a unique identifier for the edge pair (u, v)\n",
        "        edge_id = (u, v)\n",
        "        if edge_id not in edge_count:\n",
        "            edge_count[edge_id] = 0\n",
        "        edge_index = edge_count[edge_id]\n",
        "        edge_count[edge_id] += 1\n",
        "\n",
        "        # Assign a color based on the edge index\n",
        "        color = colors[edge_index % len(colors)]\n",
        "\n",
        "        # Create the tooltip text\n",
        "        tooltip_text = f\"Route: {data['route_name']}<br>Capacity: {data['ntrips_capacity']}\"\n",
        "\n",
        "        # Add the PolyLine to the map with the tooltip\n",
        "        folium.PolyLine(coords, color=color, weight=2.5, opacity=1, tooltip=tooltip_text).add_to(m)\n",
        "\n",
        "    # Add nodes to the folium map\n",
        "    for node in G.nodes():\n",
        "        folium.CircleMarker(location=node, radius=2, color='red').add_to(m)\n",
        "\n",
        "    return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0OX4iig4ah0"
      },
      "outputs": [],
      "source": [
        "s = create_folium_map(G)#_multicolor(G)\n",
        "\n",
        "# Display the map\n",
        "#m.save(\"multidigraph_map.html\")\n",
        "s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iQDJRoOz_7Z"
      },
      "source": [
        "###Normal graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrwQwHS7OHGr"
      },
      "outputs": [],
      "source": [
        "def add_edges_with_weights(G, gdf):\n",
        "    for idx, row in gdf.iterrows():\n",
        "        # Extract start and end coordinates\n",
        "        start_coords = (row['geometry'].coords[0][1], row['geometry'].coords[0][0])\n",
        "        end_coords = (row['geometry'].coords[-1][1], row['geometry'].coords[-1][0])\n",
        "\n",
        "        ntrips_capacity = row['ntrips'] * row['capacity']\n",
        "\n",
        "        # Add edge to the graph with ntrips as weight\n",
        "        if row['direction_id'] == 0:\n",
        "          G.add_edge(start_coords, end_coords, weight=ntrips_capacity, attr_dict=row.to_dict())\n",
        "        elif row['direction_id'] == 1:\n",
        "          G.add_edge(end_coords, start_coords, weight=ntrips_capacity, attr_dict=row.to_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTU8O8F-WSUT"
      },
      "outputs": [],
      "source": [
        "def add_edges_with_weights2(G, gdf):\n",
        "    for idx, row in gdf.iterrows():\n",
        "        # Extract start and end coordinates\n",
        "        start_coords = (row['geometry'].coords[0][1], row['geometry'].coords[0][0])\n",
        "        end_coords = (row['geometry'].coords[-1][1], row['geometry'].coords[-1][0])\n",
        "\n",
        "        ntrips_capacity = row['ntrips'] * row['capacity']\n",
        "\n",
        "        if row['direction_id'] == 0:\n",
        "            if G.has_edge(start_coords, end_coords):\n",
        "                # If the edge exists, update the weight\n",
        "                G[start_coords][end_coords]['weight'] += ntrips_capacity\n",
        "                G[start_coords][end_coords]['ntrips_capacity'] += ntrips_capacity\n",
        "            else:\n",
        "                # If the edge does not exist, add it with the new weight and attributes\n",
        "                G.add_edge(start_coords, end_coords, weight=ntrips_capacity, route_name=row['route_name'], ntrips_capacity=ntrips_capacity)\n",
        "        elif row['direction_id'] == 1:\n",
        "            if G.has_edge(end_coords, start_coords):\n",
        "                # If the edge exists, update the weight\n",
        "                G[end_coords][start_coords]['weight'] += ntrips_capacity\n",
        "                G[end_coords][start_coords]['ntrips_capacity'] += ntrips_capacity\n",
        "            else:\n",
        "                # If the edge does not exist, add it with the new weight and attributes\n",
        "                G.add_edge(end_coords, start_coords, weight=ntrips_capacity, route_name=row['route_name'], ntrips_capacity=ntrips_capacity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdDtxFCIOHKK"
      },
      "outputs": [],
      "source": [
        "G2 = nx.DiGraph()\n",
        "add_edges_with_weights2(G2, segments_freq_mav)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE6nusLiVSHT"
      },
      "outputs": [],
      "source": [
        "def create_folium_map2(G):\n",
        "    # Create a folium map centered around Budapest\n",
        "    m = folium.Map(location=[47.4979, 19.0402], zoom_start=12)\n",
        "\n",
        "    # Add edges to the folium map with tooltips\n",
        "    for u, v, data in G.edges(data=True):\n",
        "        # Create the coordinates list for the PolyLine\n",
        "        coords = [u, v]\n",
        "\n",
        "        # Create the tooltip text\n",
        "        capacity = data.get('ntrips_capacity', 'N/A')\n",
        "        tooltip_text = f\"Capacity: {capacity}\"\n",
        "\n",
        "        # Add the PolyLine to the map with the tooltip\n",
        "        folium.PolyLine(coords, color='blue', weight=2.5, opacity=1, tooltip=tooltip_text).add_to(m)\n",
        "\n",
        "    # Add nodes to the folium map\n",
        "    for node in G.nodes():\n",
        "        folium.CircleMarker(location=node, radius=2, color='red').add_to(m)\n",
        "\n",
        "    return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgg46SJ_t7fT"
      },
      "outputs": [],
      "source": [
        "def create_folium_heatmap(G):\n",
        "    # Create a folium map centered around Budapest\n",
        "    m = folium.Map(location=[47.4979, 19.0402], zoom_start=12)\n",
        "\n",
        "    # Normalize the edge weights for color scaling\n",
        "    edge_weights = [data.get('ntrips_capacity', 1) for u, v, data in G.edges(data=True)]\n",
        "    min_weight = min(edge_weights)\n",
        "    max_weight = max(edge_weights)\n",
        "\n",
        "    def get_color(weight):\n",
        "        # Normalize the weight to the range [0, 1]\n",
        "        norm_weight = (weight - min_weight) / (max_weight - min_weight) if max_weight != min_weight else 0.5\n",
        "        # Get a color from the 'rainbow' colormap\n",
        "        color = cm.rainbow(norm_weight)\n",
        "        # Convert color to hex\n",
        "        return mcolors.to_hex(color)\n",
        "\n",
        "    # Add edges to the folium map with tooltips\n",
        "    for u, v, data in G.edges(data=True):\n",
        "        # Create the coordinates list for the PolyLine\n",
        "        coords = [u, v]\n",
        "\n",
        "        # Get the weight and color for the edge\n",
        "        capacity = data.get('ntrips_capacity', 1)\n",
        "        color = get_color(capacity)\n",
        "\n",
        "        # Create the tooltip text\n",
        "        tooltip_text = f\"Capacity: {capacity}\"\n",
        "\n",
        "        # Add the PolyLine to the map with the tooltip\n",
        "        folium.PolyLine(coords, color=color, weight=2.5, opacity=1, tooltip=tooltip_text).add_to(m)\n",
        "\n",
        "    # Add nodes to the folium map\n",
        "    for node in G.nodes():\n",
        "        folium.CircleMarker(location=node, radius=2, color='gray').add_to(m)\n",
        "\n",
        "    return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR6ixbzjP3F8"
      },
      "outputs": [],
      "source": [
        "r = create_folium_heatmap(G2)\n",
        "r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa28Pr3AD9Xs"
      },
      "outputs": [],
      "source": [
        "r = create_folium_map2(G2)\n",
        "r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF6mW_cHKgem"
      },
      "outputs": [],
      "source": [
        "G3 = nx.DiGraph()\n",
        "add_edges_with_weights2(G3, segments_freq_bkk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tr3GawYNKlVN"
      },
      "outputs": [],
      "source": [
        "r = create_folium_map2(G3)\n",
        "r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsBYzEP50IET"
      },
      "source": [
        "###Combining nodes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_stops(stops, shapefile_path='zone_zone.SHP'):\n",
        "    # Convert stops to GeoDataFrame if it's not already one\n",
        "    if not isinstance(stops, gpd.GeoDataFrame):\n",
        "        if 'geometry' not in stops.columns:\n",
        "            raise ValueError(\"The stops DataFrame must have a 'geometry' column with Point geometries.\")\n",
        "        stops = gpd.GeoDataFrame(stops, geometry='geometry')\n",
        "\n",
        "    # Ensure the CRS for the stops GeoDataFrame if not set\n",
        "    if stops.crs is None:\n",
        "        stops.set_crs(epsg=4326, inplace=True)\n",
        "\n",
        "    # Check if the shapefile exists\n",
        "    if not os.path.exists(shapefile_path):\n",
        "        raise FileNotFoundError(f\"Shapefile not found: {shapefile_path}\")\n",
        "\n",
        "    # Load the shapefile using Fiona\n",
        "    with fiona.open(shapefile_path) as shapefile:\n",
        "        # Ensure shapefile has a CRS\n",
        "        if shapefile.crs is None:\n",
        "            shapefile_crs = 'EPSG:4326'\n",
        "        else:\n",
        "            shapefile_crs = shapefile.crs\n",
        "\n",
        "        # Convert the shapefile features to a GeoDataFrame\n",
        "        shapefile_gdf = gpd.GeoDataFrame.from_features(shapefile, crs=shapefile_crs)\n",
        "\n",
        "        # Filter the shapefile GeoDataFrame based on the 'NO' attribute\n",
        "        shapefile_filtered = shapefile_gdf[(shapefile_gdf['NO'] >= 1000) & (shapefile_gdf['NO'] <= 23999)]\n",
        "\n",
        "        # Ensure CRS match\n",
        "        if shapefile_filtered.crs != stops.crs:\n",
        "            shapefile_filtered = shapefile_filtered.to_crs(stops.crs)\n",
        "\n",
        "        # Convert filtered shapes to Shapely geometries\n",
        "        filtered_shapes = [shape(geom_shape) for geom_shape in shapefile_filtered.geometry]\n",
        "\n",
        "        # Function to check if a point (stop) is within any of the city shapes\n",
        "        def point_in_city_shapes(point):\n",
        "            return any(city_shape.contains(point) for city_shape in filtered_shapes)\n",
        "\n",
        "        # Create a mask to filter stops within the city shapes\n",
        "        mask_in_city = stops.geometry.apply(lambda point: point_in_city_shapes(point))\n",
        "\n",
        "        # Filter stops within the city shapes\n",
        "        filtered_stops = stops[mask_in_city]\n",
        "\n",
        "        return filtered_stops"
      ],
      "metadata": {
        "id": "fQ5x4B0WQnK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YUw5YgrRKvh"
      },
      "outputs": [],
      "source": [
        "stops_bkk = stops_bkk.drop(columns = ['stop_code','location_sub_type','wheelchair_boarding'])\n",
        "stops_volan = stops_volan.drop(columns = ['platform_code'])\n",
        "stops_volan['geometry'] = stops_volan.apply(lambda row: Point(row['stop_lon'], row['stop_lat']), axis=1)\n",
        "stops_volan = gpd.GeoDataFrame(stops_volan, geometry='geometry')\n",
        "stops_volan = filter_stops(stops_volan)\n",
        "stops_mav = stops_mav.drop(columns = ['stop_code','stop_desc','wheelchair_boarding','stop_timezone'])\n",
        "stops_mav['geometry'] = stops_mav.apply(lambda row: Point(row['stop_lon'], row['stop_lat']), axis=1)\n",
        "stops_mav = gpd.GeoDataFrame(stops_mav, geometry='geometry')\n",
        "stops_mav = filter_stops(stops_mav)\n",
        "stops = pd.concat([stops_bkk, stops_volan], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHElCShkEQFI"
      },
      "outputs": [],
      "source": [
        "# Assuming 'stops' is the GeoDataFrame and has columns 'stop_name', 'stop_id', 'geometry'\n",
        "stops_list = []\n",
        "\n",
        "# Step 1: Extract necessary data into a list of [stop_name, stop_id, coordinate]\n",
        "for _, row in stops.iterrows():\n",
        "    stop_name = row['stop_name']\n",
        "    stop_id = row['stop_id']\n",
        "    coordinate = (row['stop_lon'], row['stop_lat'])\n",
        "    stops_list.append([stop_name, stop_id, coordinate])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pIRviVtRhjj"
      },
      "outputs": [],
      "source": [
        "def remove_brackets_and_extras(stop_name):\n",
        "    stop_name = re.sub(r'\\(.*?\\)', '', stop_name)         # Remove text in parentheses\n",
        "    stop_name = re.sub(r'\\[.*?\\]', '', stop_name)         # Remove text in square brackets\n",
        "    stop_name = re.sub(r'\\bH\\b', '', stop_name)           # Remove standalone 'H'\n",
        "    stop_name = re.sub(r'\\bM\\b', '', stop_name)           # Remove standalone 'M'\n",
        "    stop_name = stop_name.replace('+', '')                # Remove the '+' sign\n",
        "    stop_name = re.sub(r'\\bBudapest,\\s*', '', stop_name)  # Remove 'Budapest, '\n",
        "    return stop_name.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PRDuBz8Rsii"
      },
      "outputs": [],
      "source": [
        "# Clean stop names in the stops_list\n",
        "for stop in stops_list:\n",
        "    stop[0] = remove_brackets_and_extras(stop[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvQzcZmwdfWE"
      },
      "outputs": [],
      "source": [
        "# Step 3: Create a dictionary with stop_name as keys and lists of [stop_id, coordinate] as values\n",
        "stops_dict = defaultdict(list)\n",
        "for stop_name, stop_id, coord in stops_list:\n",
        "    stops_dict[stop_name].append([stop_id, coord])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAyuyZip_Y8R"
      },
      "outputs": [],
      "source": [
        "def haversine(coord1, coord2):\n",
        "    # Radius of the Earth in meters\n",
        "    R = 6371000\n",
        "\n",
        "    lat1, lon1 = coord1\n",
        "    lat2, lon2 = coord2\n",
        "\n",
        "    # Convert latitude and longitude from degrees to radians\n",
        "    phi1 = math.radians(lat1)\n",
        "    phi2 = math.radians(lat2)\n",
        "    delta_phi = math.radians(lat2 - lat1)\n",
        "    delta_lambda = math.radians(lon2 - lon1)\n",
        "\n",
        "    # Haversine formula\n",
        "    a = math.sin(delta_phi / 2) ** 2 + math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda / 2) ** 2\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
        "\n",
        "    # Distance in meters\n",
        "    distance = R * c\n",
        "    return distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0NZMxBwIOTm"
      },
      "outputs": [],
      "source": [
        "# Step 4: Group by coordinates within each stop name\n",
        "def group_coordinates_by_distance(entries, threshold=1000):\n",
        "    groups = []\n",
        "    for stop_id, coord in entries:\n",
        "        added = False\n",
        "        for group in groups:\n",
        "            if all(haversine(coord, existing_coord[1]) < threshold for existing_coord in group):\n",
        "                group.append([stop_id, coord])\n",
        "                added = True\n",
        "                break\n",
        "        if not added:\n",
        "            groups.append([[stop_id, coord]])\n",
        "    return groups\n",
        "\n",
        "# Create a list to hold all groups of stop_ids and coordinates\n",
        "grouped_by_stop = []\n",
        "\n",
        "# Group coordinates for each stop name\n",
        "for stop_name, entries in stops_dict.items():\n",
        "    groups = group_coordinates_by_distance(entries)\n",
        "    grouped_by_stop.extend(groups)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ekwadYdgTi9C"
      },
      "outputs": [],
      "source": [
        "# Step 5: Calculate average coordinates for each group and replace them in the grouped list\n",
        "def calculate_average_coordinates(groups):\n",
        "    for group in groups:\n",
        "        if not group:\n",
        "            continue\n",
        "        avg_lat = sum(coord[1][1] for coord in group) / len(group)\n",
        "        avg_lon = sum(coord[1][0] for coord in group) / len(group)\n",
        "        average_coord = (avg_lon, avg_lat)\n",
        "        for item in group:\n",
        "            item[1] = average_coord\n",
        "\n",
        "# Apply the average calculation to all groups\n",
        "calculate_average_coordinates(grouped_by_stop)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coord_to_coord_dict = {}\n",
        "\n",
        "def calculate_average_coordinates2(groups):\n",
        "    for group in groups:\n",
        "        if not group:\n",
        "            continue\n",
        "        # Calculate the average latitude and longitude\n",
        "        avg_lat = sum(coord[1][1] for coord in group) / len(group)\n",
        "        avg_lon = sum(coord[1][0] for coord in group) / len(group)\n",
        "        average_coord = (avg_lon, avg_lat)\n",
        "\n",
        "        # Replace the coordinates in the original list and update the dictionary\n",
        "        for item in group:\n",
        "            original_coord = item[1]\n",
        "            item[1] = average_coord\n",
        "            coord_to_coord_dict[original_coord] = average_coord\n",
        "# Apply the average calculation to all groups\n",
        "calculate_average_coordinates2(grouped_by_stop)"
      ],
      "metadata": {
        "id": "yW3xqsu_hp1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(coord_to_coord_dict)"
      ],
      "metadata": {
        "id": "EE3jFBlHiwpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1Mqw_HQ8Imw"
      },
      "outputs": [],
      "source": [
        "\"\"\"with open('dict.csv', 'w') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in filtered_stop_coords_dict.items():\n",
        "       writer.writerow([key, value])\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip9M-feJ2yJR"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary to store stop_id as keys and average coordinates as values\n",
        "average_coords_dict = {}\n",
        "\n",
        "# Iterate through the grouped_by_stop list to fill the dictionary\n",
        "for group in grouped_by_stop:\n",
        "    for stop_id, avg_coord in group:\n",
        "        average_coords_dict[stop_id] = avg_coord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIP3X2oM0gEh"
      },
      "outputs": [],
      "source": [
        "# Function to update the geometry of each row\n",
        "def update_geometry(row):\n",
        "    # Get the original LineString\n",
        "    line = row['geometry']\n",
        "\n",
        "    # Look up the average coordinates using the stop IDs\n",
        "    start_coord = average_coords_dict.get(row['start_stop_id'])\n",
        "    end_coord = average_coords_dict.get(row['end_stop_id'])\n",
        "\n",
        "    # If coordinates are found, update the LineString\n",
        "    if start_coord and end_coord:\n",
        "        # Create a new LineString with updated start and end coordinates\n",
        "        new_coords = [start_coord] + list(line.coords[1:-1]) + [end_coord]\n",
        "        return LineString(new_coords)\n",
        "    else:\n",
        "        # If not found, return the original LineString\n",
        "        return line\n",
        "\n",
        "seq_copy = copy.deepcopy(segments_freq)\n",
        "seq_copy['geometry'] = seq_copy.apply(update_geometry, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9YyoNaIhEiq"
      },
      "outputs": [],
      "source": [
        "G4 = nx.DiGraph()\n",
        "add_edges_with_weights2(G4, seq_copy)\n",
        "#G4.nodes()\n",
        "r = create_folium_map2(G4)\n",
        "r"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mav modes combining + heatmap"
      ],
      "metadata": {
        "id": "e2sGgdi--hHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stops_mav = stops_mav.drop(columns = ['stop_code','stop_desc','wheelchair_boarding','stop_timezone'])\n",
        "for _, row in stops_mav.iterrows():\n",
        "    stop_name = row['stop_name']\n",
        "    stop_id = row['stop_id']\n",
        "    coordinate = (row['stop_lon'], row['stop_lat'])\n",
        "    stops_list.append([stop_name, stop_id, coordinate])\n",
        "for stop in stops_list:\n",
        "    stop[0] = remove_brackets_and_extras(stop[0])\n",
        "stops_dict = defaultdict(list)\n",
        "for stop_name, stop_id, coord in stops_list:\n",
        "    stops_dict[stop_name].append([stop_id, coord])\n",
        "grouped_by_stop = []\n",
        "for stop_name, entries in stops_dict.items():\n",
        "    groups = group_coordinates_by_distance(entries)\n",
        "    grouped_by_stop.extend(groups)\n",
        "calculate_average_coordinates(grouped_by_stop)\n",
        "average_coords_dict = {}\n",
        "for group in grouped_by_stop:\n",
        "    for stop_id, avg_coord in group:\n",
        "        average_coords_dict[stop_id] = avg_coord\n",
        "seq_copy2 = copy.deepcopy(segments_freq_mav)\n",
        "seq_copy2['geometry'] = seq_copy2.apply(update_geometry, axis=1)"
      ],
      "metadata": {
        "id": "44VYwDEm55zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_copy2['weight'] = seq_copy2['ntrips'] * seq_copy2['capacity']\n",
        "seq_copy2['normalized_weight'] = seq_copy2['weight'] / seq_copy2['weight'].max()\n",
        "mean_x, mean_y = seq_copy2.geometry.centroid.x.mean(), seq_copy2.geometry.centroid.y.mean()\n",
        "m = folium.Map(location=[mean_y, mean_x], zoom_start=12)\n",
        "colormap = cmb.LinearColormap(colors=['blue', 'green', 'yellow', 'red'], vmin=0, vmax=1)\n",
        "for _, row in seq_copy2.iterrows():\n",
        "    weight = row['normalized_weight']\n",
        "    locations = [(coord[1], coord[0]) for coord in row['geometry'].coords]\n",
        "    folium.PolyLine(\n",
        "        locations=locations,\n",
        "        weight=weight * 10 + 0.1,  # Adjust thickness\n",
        "        color=colormap(weight),  # Color based on weight\n",
        "        opacity=0.7\n",
        "    ).add_to(m)\n",
        "colormap.caption = 'Heatmap Intensity'\n",
        "m.add_child(colormap)\n",
        "m"
      ],
      "metadata": {
        "id": "vuEN8ejZ8PZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOGgPufjDSt3"
      },
      "source": [
        "###Zones and OD matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEy5w_tpDVxW"
      },
      "outputs": [],
      "source": [
        "sf = shp.Reader(\"zone_zone.shp\")\n",
        "#print(type(sf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ7lxezXDwj3"
      },
      "outputs": [],
      "source": [
        "def read_shapefile_df(sf):\n",
        "    \"\"\"\n",
        "    Read a shapefile into a Pandas dataframe with a 'coords'\n",
        "    column holding the geometry information. This uses the pyshp\n",
        "    package\n",
        "    \"\"\"\n",
        "    fields = [x[0] for x in sf.fields][1:]\n",
        "    records = sf.records()\n",
        "    shps = [s.points for s in sf.shapes()]\n",
        "    df = pd.DataFrame(columns=fields, data=records)\n",
        "    df = df.assign(coords=shps)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t626mk74Ig-7"
      },
      "outputs": [],
      "source": [
        "def read_shapefile_gdf(sf):\n",
        "    \"\"\"\n",
        "    Read a shapefile into a GeoDataFrame.\n",
        "    \"\"\"\n",
        "    # Extract the fields and records\n",
        "    fields = [x[0] for x in sf.fields][1:]\n",
        "    records = sf.records()\n",
        "\n",
        "    # Extract the shapes and convert them to shapely geometries\n",
        "    geometries = []\n",
        "    for shape in sf.shapes():\n",
        "        if len(shape.points) == 1:  # Point\n",
        "            geometries.append(Point(shape.points[0]))\n",
        "        else:  # Polygon\n",
        "            geometries.append(Polygon(shape.points))\n",
        "\n",
        "    # Create a GeoDataFrame\n",
        "    df = gpd.GeoDataFrame(columns=fields, data=records)\n",
        "    df = df.assign(geometry=geometries)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj8oqxVUDwmG"
      },
      "outputs": [],
      "source": [
        "df_shape = read_shapefile_gdf(sf)\n",
        "df_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clwYiHYhDwoW"
      },
      "outputs": [],
      "source": [
        "def plot_map(sf, df, x_lim=None, y_lim=None, figsize=(17, 11), filtered_ids=None):\n",
        "    '''\n",
        "    Plot map with lim coordinates and colored shapes based on 'NO' attribute.\n",
        "    '''\n",
        "    plt.figure(figsize=figsize)\n",
        "    id = 0\n",
        "\n",
        "    cmap = plt.get_cmap('viridis')\n",
        "    norm = plt.Normalize(df['NO'].min(), df['NO'].max())\n",
        "\n",
        "    for shape in sf.shapeRecords():\n",
        "        if filtered_ids is not None and id not in filtered_ids:\n",
        "            id += 1\n",
        "            continue\n",
        "        x = [i[0] for i in shape.shape.points[:]]\n",
        "        y = [i[1] for i in shape.shape.points[:]]\n",
        "        color = cmap(norm(df.loc[id, 'NO']))\n",
        "        plt.fill(x, y, color=color, edgecolor='k')\n",
        "\n",
        "        if x_lim is None and y_lim is None:\n",
        "            x0 = np.mean(x)\n",
        "            y0 = np.mean(y)\n",
        "            plt.text(x0, y0, id, fontsize=10)\n",
        "        id += 1\n",
        "\n",
        "    if x_lim is not None and y_lim is not None:\n",
        "        plt.xlim(x_lim)\n",
        "        plt.ylim(y_lim)\n",
        "\n",
        "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "    sm.set_array([])\n",
        "    plt.colorbar(sm, orientation='vertical', label='NO Attribute')\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoI_HFPmDwq-"
      },
      "outputs": [],
      "source": [
        "# Filter dataframe based on 'NO' attribute\n",
        "filtered_df = df_shape[(df_shape['NO'] >= 1000) & (df_shape['NO'] <= 23999)]\n",
        "filtered_ids = filtered_df.index.tolist()\n",
        "\n",
        "#print(filtered_df.shape)  # Print the shape of the filtered dataframe\n",
        "\n",
        "# Plot the filtered map with colors\n",
        "plot_map(sf, filtered_df, filtered_ids=filtered_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iO-VWN4tHY5P"
      },
      "outputs": [],
      "source": [
        "filtered_df\n",
        "print(type(filtered_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X55_iFZrHY7e"
      },
      "outputs": [],
      "source": [
        "geom = copy.deepcopy(seq_copy)\n",
        "geom.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ25gdpkHY9m"
      },
      "outputs": [],
      "source": [
        "if geom.crs is None:\n",
        "    # Replace 'EPSG:4326' with the correct CRS if known\n",
        "    geom.set_crs(epsg=4326, inplace=True)\n",
        "\n",
        "# Define the path to the shapefile\n",
        "shapefile_path = 'zone_zone.SHP'\n",
        "\n",
        "# Check if the shapefile exists\n",
        "if not os.path.exists(shapefile_path):\n",
        "    raise FileNotFoundError(f\"Shapefile not found: {shapefile_path}\")\n",
        "\n",
        "# Load the shapefile using Fiona\n",
        "with fiona.open(shapefile_path) as shapefile:\n",
        "    # Ensure shapefile has a CRS\n",
        "    if shapefile.crs is None:\n",
        "        # Replace 'EPSG:4326' with the correct CRS if known\n",
        "        shapefile_crs = 'EPSG:4326'\n",
        "    else:\n",
        "        shapefile_crs = shapefile.crs\n",
        "\n",
        "    # Convert the shapefile features to GeoDataFrame\n",
        "    shapefile_gdf = gpd.GeoDataFrame.from_features(shapefile, crs=shapefile_crs)\n",
        "\n",
        "    # Filter the shapefile GeoDataFrame based on the 'NO' attribute\n",
        "    shapefile_filtered = shapefile_gdf[(shapefile_gdf['NO'] >= 1000) & (shapefile_gdf['NO'] <= 23999)]\n",
        "\n",
        "    # Ensure CRS match\n",
        "    if shapefile_filtered.crs != geom.crs:\n",
        "        shapefile_filtered = shapefile_filtered.to_crs(geom.crs)\n",
        "\n",
        "    # Initialize the new columns with None values\n",
        "    geom['first_coord_shape_index'] = None\n",
        "    geom['second_coord_shape_index'] = None\n",
        "\n",
        "    # Convert filtered shapes to Shapely geometries\n",
        "    filtered_shapes = [(shape(geom_shape), shape_id) for geom_shape, shape_id in zip(shapefile_filtered.geometry, shapefile_filtered.index)]\n",
        "\n",
        "    # Iterate through the rows of the GeoDataFrame\n",
        "    for idx, row in geom.iterrows():\n",
        "        # Extract the coordinates from the geometry column\n",
        "        coords = list(row.geometry.coords)\n",
        "        first_coord = Point(coords[0])\n",
        "        second_coord = Point(coords[-1])\n",
        "        \"\"\"if idx <= 10:\n",
        "          print(coords)\n",
        "          print(first_coord)\n",
        "          print(second_coord)\"\"\"\n",
        "\n",
        "        # Check which shape the first coordinate falls within\n",
        "        first_match = next((shape_id for geom_shape, shape_id in filtered_shapes if geom_shape.contains(first_coord)), None)\n",
        "        geom.at[idx, 'first_coord_shape_index'] = first_match\n",
        "\n",
        "        # Check which shape the second coordinate falls within\n",
        "        second_match = next((shape_id for geom_shape, shape_id in filtered_shapes if geom_shape.contains(second_coord)), None)\n",
        "        geom.at[idx, 'second_coord_shape_index'] = second_match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6VDqWP1HY_u"
      },
      "outputs": [],
      "source": [
        "geom.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty dictionary to store the results.\n",
        "node_shape_dict = {}\n",
        "\n",
        "# Iterate over each row in the GeoDataFrame.\n",
        "for index, row in geom.iterrows():\n",
        "    # Extract the geometry (assuming it's a LineString).\n",
        "    line = row['geometry']\n",
        "\n",
        "    # Get the first and last elements of the LineString.\n",
        "    first_point = line.coords[0]\n",
        "    last_point = line.coords[-1]\n",
        "\n",
        "    # Check if the first point is not already in the dictionary.\n",
        "    if first_point not in node_shape_dict:\n",
        "        # Add the first point as a key with the first_shape_index as the value.\n",
        "        node_shape_dict[first_point] = row['first_coord_shape_index']\n",
        "\n",
        "    # Check if the last point is not already in the dictionary.\n",
        "    if last_point not in node_shape_dict:\n",
        "        # Add the last point as a key with the second_shape_index as the value.\n",
        "        node_shape_dict[last_point] = row['second_coord_shape_index']"
      ],
      "metadata": {
        "id": "iqIp8en_EIgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh3Cq3Y1SaQ5"
      },
      "outputs": [],
      "source": [
        "#print(len(node_shape_dict))\n",
        "#print(list(node_shape_dict.items())[23])\n",
        "print(node_shape_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFenQQkPXxef"
      },
      "outputs": [],
      "source": [
        "# Initialize the matrix\n",
        "matrix_size = len(shapefile_filtered)\n",
        "matrix_cap = np.zeros((matrix_size, matrix_size))\n",
        "\n",
        "# Populate the matrix based on geom rows\n",
        "for idx, row in geom.iterrows():\n",
        "    first_index = row['first_coord_shape_index']\n",
        "    second_index = row['second_coord_shape_index']\n",
        "    if first_index is not None and second_index is not None and first_index <= 921 and second_index <= 921:\n",
        "      matrix_cap[first_index, second_index] += row['ntrips'] * row['capacity']\n",
        "matrix_cap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTy1vDK6Xxgm"
      },
      "outputs": [],
      "source": [
        "matrix_trip = np.zeros((matrix_size, matrix_size))\n",
        "for idx, row in geom.iterrows():\n",
        "    first_index = row['first_coord_shape_index']\n",
        "    second_index = row['second_coord_shape_index']\n",
        "    if first_index is not None and second_index is not None and first_index <= 921 and second_index <= 921:\n",
        "      matrix_trip[first_index, second_index] += row['ntrips']\n",
        "matrix_trip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_row_with_nan(matrix):\n",
        "    for row in matrix:\n",
        "        if any(math.isnan(value) for value in row):\n",
        "            return row\n",
        "    return None\n",
        "row_with_nan = find_row_with_nan(matrix_cap)\n",
        "print(row_with_nan)"
      ],
      "metadata": {
        "id": "tu-sve8ppcYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj_M22R_MQ5L"
      },
      "source": [
        "###Be- és kimenő súlyok, OD összehasonlítás"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZDTN1x3MiBI"
      },
      "outputs": [],
      "source": [
        "# Calculate outgoing and incoming weights\n",
        "outgoing_weights = {}\n",
        "incoming_weights = {}\n",
        "\n",
        "for node in G4.nodes():\n",
        "    outgoing_weights[node] = sum(data['weight'] for _, _, data in G4.out_edges(node, data=True))\n",
        "    incoming_weights[node] = sum(data['weight'] for _, _, data in G4.in_edges(node, data=True))\n",
        "#print(outgoing_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDfT1G-jQ1XI"
      },
      "outputs": [],
      "source": [
        "# Normalize the weights for visualization\n",
        "max_weight = max(outgoing_weights.values())\n",
        "normalized_weights = {node: weight / max_weight for node, weight in outgoing_weights.items()}\n",
        "#print(normalized_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqA0BHfxMikB"
      },
      "outputs": [],
      "source": [
        "# Create GeoDataFrame for outgoing weights\n",
        "outgoing_gdf = gpd.GeoDataFrame(\n",
        "    [{'geometry': Point(node), 'weight': weight} for node, weight in normalized_weights.items()],\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "# Create GeoDataFrame for incoming weights\n",
        "incoming_gdf = gpd.GeoDataFrame(\n",
        "    [{'geometry': Point(node), 'weight': weight} for node, weight in incoming_weights.items()],\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "#outgoing_gdf.head(3)\n",
        "#incoming_gdf.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "v5y3gYmRMimI"
      },
      "outputs": [],
      "source": [
        "# Initialize map centered on Budapest\n",
        "m = folium.Map(location=[47.4979, 19.0402], zoom_start=12)  # Zoom level can be adjusted\n",
        "\n",
        "# Define a function to choose color based on normalized weight\n",
        "def get_color(weight):\n",
        "    # Define a color map from green (low weight) to red (high weight)\n",
        "    cmap = mcolors.LinearSegmentedColormap.from_list(\"weight_cmap\", ['blue','green','yellow', 'orange', 'red'])\n",
        "    # Normalize the weight and map it to the color\n",
        "    return mcolors.to_hex(cmap(weight))\n",
        "\n",
        "# Add nodes with sizes and colors based on normalized outgoing weights\n",
        "for node, normalized_weight in normalized_weights.items():\n",
        "    folium.CircleMarker(\n",
        "        location=node,\n",
        "        radius=normalized_weight * 20,  # Adjust scaling factor as needed\n",
        "        color=get_color(normalized_weight),  # Border color of the circle\n",
        "        fill=True,\n",
        "        fill_color=get_color(normalized_weight),\n",
        "        fill_opacity=0.6,\n",
        "        popup=f'Outgoing weight: {outgoing_weights[node]} (Normalized: {normalized_weight:.2f})'\n",
        "    ).add_to(m)\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgEKThh0Ttif"
      },
      "outputs": [],
      "source": [
        "file_path = 'OD.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "third_column = df.iloc[2:924, 2].reset_index(drop=True)\n",
        "third_row = df.iloc[1, 3:925].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6ySSnmPUk0m"
      },
      "outputs": [],
      "source": [
        "# Initialize the dictionaries\n",
        "out = {}\n",
        "into = {}\n",
        "\n",
        "# Iterate through each row in the GeoDataFrame\n",
        "for index, row in geom.iterrows():\n",
        "    first_index = row['first_coord_shape_index']\n",
        "    second_index = row['second_coord_shape_index']\n",
        "    load = row['ntrips'] * row['capacity']\n",
        "\n",
        "    if first_index != second_index:\n",
        "      # Update the out dictionary\n",
        "      if first_index in out:\n",
        "        out[first_index] += load\n",
        "      else:\n",
        "        out[first_index] = load\n",
        "\n",
        "      # Update the in dictionary\n",
        "      if second_index in into:\n",
        "        into[second_index] += load\n",
        "      else:\n",
        "        into[second_index] = load\n",
        "# Create a new Series 'out_value' by mapping from the 'out' dictionary using third_column's indices\n",
        "out_value = third_column.index.map(lambda idx: out.get(idx, np.nan))\n",
        "third_column_out = pd.Series(out_value, index=third_column.index, name='out_value')\n",
        "\n",
        "# Create a new Series 'in_value' by mapping from the 'in_' dictionary using third_row's indices\n",
        "in_value = third_row.index.map(lambda idx: into.get(idx, np.nan))\n",
        "third_row_in = pd.Series(in_value, index=third_row.index, name='in_value')\n",
        "\n",
        "# Convert the Series into DataFrames\n",
        "ki = pd.DataFrame({'index': third_column.index, 'OD_outvalue': third_column, 'calculated_outvalue': third_column_out})\n",
        "be = pd.DataFrame({'index': third_row.index, 'OD_invalue': third_row, 'calculated_invalue': third_row_in})\n",
        "\n",
        "# Optionally, set the 'index' as the index of the DataFrame if needed\n",
        "ki.set_index('index', inplace=True)\n",
        "be.set_index('index', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhxNJjw2evBF"
      },
      "outputs": [],
      "source": [
        "ki['Out_difference'] = - ki['OD_outvalue'] + ki['calculated_outvalue']\n",
        "be['In_difference'] = - be['OD_invalue'] + be['calculated_invalue']\n",
        "be.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row_sums_without_diagonal = []\n",
        "for i in range(matrix_cap.shape[0]):\n",
        "    row_sum = np.sum(matrix_cap[i]) - matrix_cap[i, i]\n",
        "    row_sums_without_diagonal.append(row_sum)\n",
        "\n",
        "ki = pd.DataFrame({'index': third_column.index, 'OD_outvalue': third_column, 'calculated_outvalue': row_sums_without_diagonal})\n",
        "\n",
        "col_sums_without_diagonal = []\n",
        "for i in range(matrix_cap.shape[1]):\n",
        "    col_sum = np.sum(matrix_cap[:, i]) - matrix_cap[i, i]\n",
        "    col_sums_without_diagonal.append(col_sum)\n",
        "\n",
        "# 3. Append the computed sums as a new column to the `third_column` DataFrame\n",
        "#third_row['calculated_invalue'] = col_sums_without_diagonal\n",
        "be = pd.DataFrame({'index': third_row.index, 'OD_invalue': third_row, 'calculated_invalue': col_sums_without_diagonal})\n",
        "\n",
        "ki.set_index('index', inplace=True)\n",
        "be.set_index('index', inplace=True)\n",
        "\n",
        "be.head(2)"
      ],
      "metadata": {
        "id": "IXY7vm9p2ppr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(be)"
      ],
      "metadata": {
        "id": "DDugSPqtB0t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "be.index.values[-1] = 921\n",
        "be.at[921, 'OD_invalue'] = 4.565532"
      ],
      "metadata": {
        "id": "06Eq0JnH7xKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ki['OD_outvalue'] = pd.to_numeric(ki['OD_outvalue'], errors='coerce')\n",
        "ki['calculated_outvalue'] = pd.to_numeric(ki['calculated_outvalue'], errors='coerce')\n",
        "ki['Out_difference'] = - ki['OD_outvalue'] + ki['calculated_outvalue']\n",
        "be['OD_invalue'] = pd.to_numeric(be['OD_invalue'], errors='coerce')\n",
        "be['calculated_invalue'] = pd.to_numeric(be['calculated_invalue'], errors='coerce')\n",
        "be['In_difference'] = -be['OD_invalue'] + be['calculated_invalue']"
      ],
      "metadata": {
        "id": "G9ZmJcNH4RQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_index_ki = ki['Out_difference'].idxmin()  # Get index of the min value\n",
        "max_index_ki = ki['Out_difference'].idxmax()  # Get index of the max value\n",
        "min_index_be = be['In_difference'].idxmin()\n",
        "max_index_be = be['In_difference'].idxmax()\n",
        "\n",
        "min_ki = ki['Out_difference'].min()  # Get the min value\n",
        "max_ki = ki['Out_difference'].max()  # Get the max value\n",
        "min_be = be['In_difference'].min()\n",
        "max_be = be['In_difference'].max()"
      ],
      "metadata": {
        "id": "QNM--gpoInfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf = gpd.read_file('zone_zone.SHP')\n",
        "gdf = gdf[(gdf['NO'] >= 1000) & (gdf['NO'] <= 23999)]\n",
        "# Convert the GeoDataFrame to the same CRS as Folium (WGS84 - EPSG:4326)\n",
        "gdf = gdf.to_crs(epsg=4326)\n",
        "\n",
        "# Create a base map\n",
        "m = folium.Map(location=[gdf.geometry.centroid.y.mean(), gdf.geometry.centroid.x.mean()], zoom_start=10)\n",
        "\n",
        "# Function to style the shapes\n",
        "def style_function(feature):\n",
        "    index = feature['properties']['idx']  # Use 'idx' which we'll add to properties\n",
        "    if index == min_index_ki:\n",
        "        return {'fillColor': 'blue', 'color': 'blue', 'weight': 2, 'fillOpacity': 0.7}\n",
        "    elif index == max_index_ki:\n",
        "        return {'fillColor': 'red', 'color': 'red', 'weight': 2, 'fillOpacity': 0.7}\n",
        "    else:\n",
        "        return {'fillColor': 'transparent', 'color': 'black', 'weight': 1, 'fillOpacity': 0.0}\n",
        "\n",
        "# Add each shape to the map with its index in properties\n",
        "for idx, row in gdf.iterrows():\n",
        "    # Add the appropriate tooltip based on whether it's min, max, or neither\n",
        "    if idx == min_index_ki:\n",
        "        tooltip = f\"Min Value: {min_ki}\"\n",
        "    elif idx == max_index_ki:\n",
        "        tooltip = f\"Max Value: {max_ki}\"\n",
        "    else:\n",
        "        tooltip = None  # No tooltip for other shapes\n",
        "\n",
        "    geojson = folium.GeoJson(\n",
        "        data={\n",
        "            'type': 'Feature',\n",
        "            'geometry': mapping(row['geometry']),\n",
        "            'properties': {'idx': idx}  # Add the index to properties\n",
        "        },\n",
        "        style_function=style_function,\n",
        "        tooltip=tooltip  # Attach the tooltip with min/max value\n",
        "    )\n",
        "    geojson.add_to(m)\n",
        "\n",
        "# Display the map\n",
        "#m.save('map.html')\n",
        "m"
      ],
      "metadata": {
        "id": "BZsaoiSVInhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the GeoDataFrame to the same CRS as Folium (WGS84 - EPSG:4326)\n",
        "#gdf = gdf.to_crs(epsg=4326)\n",
        "\n",
        "# Create a base map\n",
        "m = folium.Map(location=[gdf.geometry.centroid.y.mean(), gdf.geometry.centroid.x.mean()], zoom_start=10)\n",
        "\n",
        "# Function to style the shapes\n",
        "def style_function(feature):\n",
        "    index = feature['properties']['idx']  # Use 'idx' which we'll add to properties\n",
        "    if index == min_index_be:\n",
        "        return {'fillColor': 'blue', 'color': 'blue', 'weight': 2, 'fillOpacity': 0.7}\n",
        "    elif index == max_index_be:\n",
        "        return {'fillColor': 'red', 'color': 'red', 'weight': 2, 'fillOpacity': 0.7}\n",
        "    else:\n",
        "        return {'fillColor': 'transparent', 'color': 'black', 'weight': 1, 'fillOpacity': 0.0}\n",
        "\n",
        "# Add each shape to the map with its index in properties\n",
        "for idx, row in gdf.iterrows():\n",
        "    # Add the appropriate tooltip based on whether it's min, max, or neither\n",
        "    if idx == min_index_be:\n",
        "        tooltip = f\"Min Value: {min_be}\"\n",
        "    elif idx == max_index_be:\n",
        "        tooltip = f\"Max Value: {max_be}\"\n",
        "    else:\n",
        "        tooltip = None  # No tooltip for other shapes\n",
        "\n",
        "    geojson = folium.GeoJson(\n",
        "        data={\n",
        "            'type': 'Feature',\n",
        "            'geometry': mapping(row['geometry']),\n",
        "            'properties': {'idx': idx}  # Add the index to properties\n",
        "        },\n",
        "        style_function=style_function,\n",
        "        tooltip=tooltip  # Attach the tooltip with min/max value\n",
        "    )\n",
        "    geojson.add_to(m)\n",
        "\n",
        "# Display the map\n",
        "#m.save('map.html')\n",
        "m"
      ],
      "metadata": {
        "id": "I8gQ3UvCJx1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the values so that 0 maps to white, negatives to blue, and positives to red\n",
        "norm = mcolors.TwoSlopeNorm(vmin=min_ki, vcenter=0, vmax=max_ki)\n",
        "\n",
        "# Use a colormap that transitions from blue (negative) to white (neutral) to red (positive)\n",
        "cmap = mcolors.LinearSegmentedColormap.from_list(\"pos_neg_cmap\", ['blue', 'white', 'red'])\n",
        "\n",
        "# Convert the filtered GeoDataFrame to the same CRS as Folium (WGS84 - EPSG:4326)\n",
        "gdf_filtered = gdf.to_crs(epsg=4326)\n",
        "\n",
        "# Function to get the color based on the normalized value\n",
        "def get_color(value):\n",
        "    norm_value = norm(value)  # Normalize the value with center at 0\n",
        "    return mcolors.to_hex(cmap(norm_value))  # Convert color to hex for folium\n",
        "\n",
        "# Create a base map\n",
        "m = folium.Map(location=[gdf_filtered.geometry.centroid.y.mean(), gdf_filtered.geometry.centroid.x.mean()], zoom_start=10)\n",
        "\n",
        "# Add each shape to the map with its color based on the value\n",
        "for idx, row in gdf_filtered.iterrows():\n",
        "    value = ki.iloc[idx, 2]  # Get the corresponding value from the DataFrame\n",
        "    color = get_color(value)  # Get the color based on the value\n",
        "\n",
        "    tooltip = f\"Value: {value}\"  # Tooltip to display the value on hover\n",
        "\n",
        "    geojson = folium.GeoJson(\n",
        "        data={\n",
        "            'type': 'Feature',\n",
        "            'geometry': mapping(row['geometry']),\n",
        "            'properties': {'idx': idx}\n",
        "        },\n",
        "        style_function=lambda x, color=color: {\n",
        "            'fillColor': color,\n",
        "            'color': 'black',\n",
        "            'weight': 1,\n",
        "            'fillOpacity': 0.7\n",
        "        },\n",
        "        tooltip=tooltip  # Attach the tooltip with the value\n",
        "    )\n",
        "    geojson.add_to(m)\n",
        "\n",
        "# Save the map to an HTML file and display it\n",
        "#m.save('map.html')\n",
        "m"
      ],
      "metadata": {
        "id": "unZdnDtbm4de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the values so that 0 maps to white, negatives to blue, and positives to red\n",
        "norm = mcolors.TwoSlopeNorm(vmin=min_be, vcenter=0, vmax=max_be)\n",
        "\n",
        "# Use a colormap that transitions from blue (negative) to white (neutral) to red (positive)\n",
        "cmap = mcolors.LinearSegmentedColormap.from_list(\"pos_neg_cmap\", ['blue', 'white', 'red'])\n",
        "\n",
        "# Convert the filtered GeoDataFrame to the same CRS as Folium (WGS84 - EPSG:4326)\n",
        "gdf_filtered = gdf.to_crs(epsg=4326)\n",
        "\n",
        "# Function to get the color based on the normalized value\n",
        "def get_color(value):\n",
        "    norm_value = norm(value)  # Normalize the value with center at 0\n",
        "    return mcolors.to_hex(cmap(norm_value))  # Convert color to hex for folium\n",
        "\n",
        "# Create a base map\n",
        "m = folium.Map(location=[gdf_filtered.geometry.centroid.y.mean(), gdf_filtered.geometry.centroid.x.mean()], zoom_start=10)\n",
        "\n",
        "# Add each shape to the map with its color based on the value\n",
        "for idx, row in gdf_filtered.iterrows():\n",
        "    value = ki.iloc[idx, 2]  # Get the corresponding value from the DataFrame\n",
        "    color = get_color(value)  # Get the color based on the value\n",
        "\n",
        "    tooltip = f\"Value: {value}\"  # Tooltip to display the value on hover\n",
        "\n",
        "    geojson = folium.GeoJson(\n",
        "        data={\n",
        "            'type': 'Feature',\n",
        "            'geometry': mapping(row['geometry']),\n",
        "            'properties': {'idx': idx}\n",
        "        },\n",
        "        style_function=lambda x, color=color: {\n",
        "            'fillColor': color,\n",
        "            'color': 'black',\n",
        "            'weight': 1,\n",
        "            'fillOpacity': 0.7\n",
        "        },\n",
        "        tooltip=tooltip  # Attach the tooltip with the value\n",
        "    )\n",
        "    geojson.add_to(m)\n",
        "\n",
        "# Save the map to an HTML file and display it\n",
        "#m.save('map.html')\n",
        "m"
      ],
      "metadata": {
        "id": "L5MBLnKN9gY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the values to the range [0, 1] for color mapping\n",
        "norm = plt.Normalize(vmin=min_ki, vmax=max_ki)\n",
        "\n",
        "# Use the 'viridis' colormap for a balanced color gradient\n",
        "cmap = mcolors.LinearSegmentedColormap.from_list(\"weight_cmap\", ['blue','green','yellow', 'orange', 'red'])\n",
        "\n",
        "# Convert the filtered GeoDataFrame to the same CRS as Folium (WGS84 - EPSG:4326)\n",
        "gdf_filtered = gdf.to_crs(epsg=4326)\n",
        "\n",
        "# Create a base map\n",
        "m = folium.Map(location=[gdf_filtered.geometry.centroid.y.mean(), gdf_filtered.geometry.centroid.x.mean()], zoom_start=10)\n",
        "\n",
        "# Add each shape to the map with its color based on the value\n",
        "for idx, row in gdf_filtered.iterrows():\n",
        "    value = be.iloc[idx, 2]  # Get the corresponding value from the DataFrame\n",
        "    color = get_color(value)  # Get the color based on the value\n",
        "\n",
        "    tooltip = f\"Value: {value}\"  # Tooltip to display the value on hover\n",
        "\n",
        "    geojson = folium.GeoJson(\n",
        "        data={\n",
        "            'type': 'Feature',\n",
        "            'geometry': mapping(row['geometry']),\n",
        "            'properties': {'idx': idx}\n",
        "        },\n",
        "        style_function=lambda x, color=color: {\n",
        "            'fillColor': color,\n",
        "            'color': 'black',\n",
        "            'weight': 1,\n",
        "            'fillOpacity': 0.7\n",
        "        },\n",
        "        tooltip=tooltip  # Attach the tooltip with the value\n",
        "    )\n",
        "    geojson.add_to(m)\n",
        "\n",
        "# Save the map to an HTML file and display it\n",
        "#m.save('map.html')\n",
        "m"
      ],
      "metadata": {
        "id": "rYvvY36I9glk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the values to the range [0, 1] for color mapping\n",
        "norm = plt.Normalize(vmin=min_be, vmax=max_be)\n",
        "\n",
        "# Use the 'viridis' colormap for a balanced color gradient\n",
        "cmap = mcolors.LinearSegmentedColormap.from_list(\"weight_cmap\", ['blue','green','yellow', 'orange', 'red'])\n",
        "\n",
        "# Convert the filtered GeoDataFrame to the same CRS as Folium (WGS84 - EPSG:4326)\n",
        "gdf_filtered = gdf.to_crs(epsg=4326)\n",
        "\n",
        "# Create a base map\n",
        "m = folium.Map(location=[gdf_filtered.geometry.centroid.y.mean(), gdf_filtered.geometry.centroid.x.mean()], zoom_start=10)\n",
        "\n",
        "# Add each shape to the map with its color based on the value\n",
        "for idx, row in gdf_filtered.iterrows():\n",
        "    value = be.iloc[idx, 2]  # Get the corresponding value from the DataFrame\n",
        "    color = get_color(value)  # Get the color based on the value\n",
        "\n",
        "    tooltip = f\"Value: {value}\"  # Tooltip to display the value on hover\n",
        "\n",
        "    geojson = folium.GeoJson(\n",
        "        data={\n",
        "            'type': 'Feature',\n",
        "            'geometry': mapping(row['geometry']),\n",
        "            'properties': {'idx': idx}\n",
        "        },\n",
        "        style_function=lambda x, color=color: {\n",
        "            'fillColor': color,\n",
        "            'color': 'black',\n",
        "            'weight': 1,\n",
        "            'fillOpacity': 0.7\n",
        "        },\n",
        "        tooltip=tooltip  # Attach the tooltip with the value\n",
        "    )\n",
        "    geojson.add_to(m)\n",
        "\n",
        "# Save the map to an HTML file and display it\n",
        "#m.save('map.html')\n",
        "m"
      ],
      "metadata": {
        "id": "vItmi7Snm4fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume be.iloc[:, 2] contains the values you want to visualize\n",
        "values = ki.iloc[:, 2]\n",
        "\n",
        "# Calculate percentiles\n",
        "percentiles = np.percentile(values, np.arange(0, 101, 10))  # Dividing into 10 percentiles (0-10, 10-20, ..., 90-100)\n",
        "\n",
        "# Create a blue-to-red colormap\n",
        "cmap = mcolors.LinearSegmentedColormap.from_list(\"blue_red_cmap\", [\"blue\",\"white\", \"red\"])\n",
        "\n",
        "# Function to assign colors based on the percentile range\n",
        "def get_percentile_color(value):\n",
        "    for i in range(len(percentiles) - 1):\n",
        "        if percentiles[i] <= value < percentiles[i + 1]:\n",
        "            color = cmap(i / (len(percentiles) - 1))  # Use blue-to-red colormap\n",
        "            return mcolors.to_hex(color)\n",
        "    return mcolors.to_hex(cmap(1.0))  # For the top 100th percentile\n",
        "\n",
        "# Convert the filtered GeoDataFrame to the same CRS as Folium (WGS84 - EPSG:4326)\n",
        "gdf_filtered = gdf.to_crs(epsg=4326)\n",
        "\n",
        "# Add percentiles as a new column to the GeoDataFrame for convenience (optional)\n",
        "gdf_filtered['percentile'] = np.searchsorted(percentiles, values, side='right')\n",
        "\n",
        "# Create a base map\n",
        "m = folium.Map(location=[gdf_filtered.geometry.centroid.y.mean(), gdf_filtered.geometry.centroid.x.mean()], zoom_start=10)\n",
        "\n",
        "# Add each shape to the map with its color based on the percentile\n",
        "for idx, row in gdf_filtered.iterrows():\n",
        "    value = values[idx]\n",
        "    color = get_percentile_color(value)  # Get the color based on the percentile\n",
        "\n",
        "    tooltip = f\"Value: {value}, Percentile: {gdf_filtered['percentile'][idx]}\"  # Tooltip with value and percentile\n",
        "\n",
        "    geojson = folium.GeoJson(\n",
        "        data={\n",
        "            'type': 'Feature',\n",
        "            'geometry': mapping(row['geometry']),\n",
        "            'properties': {'idx': idx}\n",
        "        },\n",
        "        style_function=lambda x, color=color: {\n",
        "            'fillColor': color,\n",
        "            'color': 'black',\n",
        "            'weight': 1,\n",
        "            'fillOpacity': 0.7\n",
        "        },\n",
        "        tooltip=tooltip  # Attach the tooltip with the value and percentile\n",
        "    )\n",
        "    geojson.add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m"
      ],
      "metadata": {
        "id": "19yD4k_RAj-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values = be.iloc[:, 2]\n",
        "\n",
        "# Calculate percentiles\n",
        "percentiles = np.percentile(values, np.arange(0, 101, 10))  # Dividing into 10 percentiles (0-10, 10-20, ..., 90-100)\n",
        "\n",
        "# Create a blue-to-red colormap\n",
        "cmap = mcolors.LinearSegmentedColormap.from_list(\"blue_red_cmap\", [\"blue\",\"white\", \"red\"])\n",
        "\n",
        "# Function to assign colors based on the percentile range\n",
        "def get_percentile_color(value):\n",
        "    for i in range(len(percentiles) - 1):\n",
        "        if percentiles[i] <= value < percentiles[i + 1]:\n",
        "            color = cmap(i / (len(percentiles) - 1))  # Use blue-to-red colormap\n",
        "            return mcolors.to_hex(color)\n",
        "    return mcolors.to_hex(cmap(1.0))  # For the top 100th percentile\n",
        "\n",
        "# Convert the filtered GeoDataFrame to the same CRS as Folium (WGS84 - EPSG:4326)\n",
        "gdf_filtered = gdf.to_crs(epsg=4326)\n",
        "\n",
        "# Add percentiles as a new column to the GeoDataFrame for convenience (optional)\n",
        "gdf_filtered['percentile'] = np.searchsorted(percentiles, values, side='right')\n",
        "\n",
        "# Create a base map\n",
        "m = folium.Map(location=[gdf_filtered.geometry.centroid.y.mean(), gdf_filtered.geometry.centroid.x.mean()], zoom_start=10)\n",
        "\n",
        "# Add each shape to the map with its color based on the percentile\n",
        "for idx, row in gdf_filtered.iterrows():\n",
        "    value = values[idx]\n",
        "    color = get_percentile_color(value)  # Get the color based on the percentile\n",
        "\n",
        "    tooltip = f\"Value: {value}, Percentile: {gdf_filtered['percentile'][idx]}\"  # Tooltip with value and percentile\n",
        "\n",
        "    geojson = folium.GeoJson(\n",
        "        data={\n",
        "            'type': 'Feature',\n",
        "            'geometry': mapping(row['geometry']),\n",
        "            'properties': {'idx': idx}\n",
        "        },\n",
        "        style_function=lambda x, color=color: {\n",
        "            'fillColor': color,\n",
        "            'color': 'black',\n",
        "            'weight': 1,\n",
        "            'fillOpacity': 0.7\n",
        "        },\n",
        "        tooltip=tooltip  # Attach the tooltip with the value and percentile\n",
        "    )\n",
        "    geojson.add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m"
      ],
      "metadata": {
        "id": "YbFQyERpzaGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Every_OD"
      ],
      "metadata": {
        "id": "GGKhjqmRIQZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dijkstra_max_capacity(G, source):\n",
        "    \"\"\"Modified Dijkstra's algorithm to find paths with the maximum minimum edge weight, ensuring positive capacity.\"\"\"\n",
        "    # Priority queue: stores (-capacity, path length, current node, path)\n",
        "    queue = [(-float('inf'), 0, source, [])]\n",
        "    visited = {source: (-float('inf'), 0)}  # Stores the best (capacity, path length) found so far\n",
        "    paths = {}  # Store the path to reach each node\n",
        "\n",
        "    while queue:\n",
        "        # Pop the node with the highest capacity path\n",
        "        max_cap, length, node, path = heapq.heappop(queue)\n",
        "        max_cap = -max_cap\n",
        "        current_path = path + [node]\n",
        "\n",
        "        if node in paths:\n",
        "            continue\n",
        "\n",
        "        paths[node] = current_path\n",
        "\n",
        "        # Explore neighbors\n",
        "        for neighbor in G.neighbors(node):\n",
        "            edge_weight = G[node][neighbor]['weight']\n",
        "\n",
        "            # Skip edges with non-positive capacity\n",
        "            if edge_weight <= 0:\n",
        "                continue\n",
        "\n",
        "            # The capacity of the new path is the min of the current path capacity and the new edge weight\n",
        "            new_cap = min(max_cap, edge_weight)\n",
        "            new_length = length + 1\n",
        "\n",
        "            # If this path to neighbor has better capacity or is shorter with the same capacity\n",
        "            if neighbor not in visited or (new_cap > visited[neighbor][0]) or (new_cap == visited[neighbor][0] and new_length < visited[neighbor][1]):\n",
        "                visited[neighbor] = (new_cap, new_length)\n",
        "                heapq.heappush(queue, (-new_cap, new_length, neighbor, current_path))\n",
        "\n",
        "    return paths, visited"
      ],
      "metadata": {
        "id": "x_CMFPF-iIbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store the final results\n",
        "all_routes = []\n",
        "\n",
        "# Get all nodes in the graph\n",
        "nodes = list(G4.nodes())\n",
        "\n",
        "# Iterate over all pairs of nodes\n",
        "for start_node in nodes:\n",
        "    # Run the modified Dijkstra's algorithm from the start node\n",
        "    paths, capacities = dijkstra_max_capacity(G4, start_node)\n",
        "\n",
        "    # Store the result for each end node\n",
        "    for end_node in nodes:\n",
        "        if end_node != start_node:\n",
        "            start_coord = start_node\n",
        "            end_coord = end_node\n",
        "            if end_node in capacities:\n",
        "                max_capacity = capacities[end_node][0]   # The max capacity for this path\n",
        "            else:\n",
        "                # Handle the case where there is no path from start_node to end_node\n",
        "                max_capacity = 0  # or some other default value indicating no path\n",
        "\n",
        "            all_routes.append([start_coord, end_coord, max_capacity])"
      ],
      "metadata": {
        "id": "LY7cONQwKyVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dijkstra_max_capacity_to_target(G, source, target):\n",
        "    \"\"\"Find the shortest path with the maximum minimum edge weight to a specific target, ensuring positive capacity.\"\"\"\n",
        "    # Priority queue: stores (-capacity, path length, current node, path)\n",
        "    queue = [(-float('inf'), 0, source, [])]\n",
        "    visited = {source: (-float('inf'), 0)}  # Stores the best (capacity, path length) found so far\n",
        "\n",
        "    while queue:\n",
        "        # Pop the node with the highest capacity path\n",
        "        max_cap, length, node, path = heapq.heappop(queue)\n",
        "        max_cap = -max_cap\n",
        "        current_path = path + [node]\n",
        "\n",
        "        if node == target:\n",
        "            return current_path, max_cap  # Return the path and its max capacity when target is reached\n",
        "\n",
        "        if node in visited and visited[node][0] >= max_cap:\n",
        "            continue\n",
        "\n",
        "        visited[node] = (max_cap, length)\n",
        "\n",
        "        # Explore neighbors\n",
        "        for neighbor in G.neighbors(node):\n",
        "            edge_weight = G[node][neighbor]['weight']\n",
        "\n",
        "            # Skip edges with non-positive capacity\n",
        "            if edge_weight <= 0:\n",
        "                continue\n",
        "\n",
        "            # The capacity of the new path is the min of the current path capacity and the new edge weight\n",
        "            new_cap = min(max_cap, edge_weight)\n",
        "            new_length = length + 1\n",
        "\n",
        "            # If this path to neighbor has better capacity or is shorter with the same capacity\n",
        "            if neighbor not in visited or (new_cap > visited[neighbor][0]) or (new_cap == visited[neighbor][0] and new_length < visited[neighbor][1]):\n",
        "                heapq.heappush(queue, (-new_cap, new_length, neighbor, current_path))\n",
        "\n",
        "    return None, None  # Return None if the target is not reachable\n",
        "\n",
        "def compute_all_pairs_max_capacity(G):\n",
        "    \"\"\"Compute maximum minimum capacity paths for every possible pair of nodes in the graph.\"\"\"\n",
        "    nodes = list(G.nodes)\n",
        "    all_results = []\n",
        "\n",
        "    # Iterate through all unique pairs of nodes\n",
        "    for source, target in combinations(nodes, 2):\n",
        "        path, capacity = dijkstra_max_capacity_to_target(G, source, target)\n",
        "        if path:\n",
        "            all_results.append([source, target, capacity])\n",
        "\n",
        "    return all_results"
      ],
      "metadata": {
        "id": "b0JT91Tj8gG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store the final results\n",
        "all_routes2 = compute_all_pairs_max_capacity(G4)"
      ],
      "metadata": {
        "id": "dBehLQRX8gNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(all_routes))\n",
        "print(len(G4.nodes()))\n",
        "print(all_routes[1236])"
      ],
      "metadata": {
        "id": "s42CR5s7KyXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = list(G4.nodes())\n",
        "print(type(nodes[2]))\n",
        "print(type(all_routes[2][0]))"
      ],
      "metadata": {
        "id": "0Z3uyo4tDdD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_routes_df = pd.DataFrame(all_routes, columns=['first_coord', 'second_coord', 'capacity'])\n",
        "geometry = [LineString([first, second]) for first, second in zip(all_routes_df['first_coord'], all_routes_df['second_coord'])]\n",
        "all_routes_gdf = gpd.GeoDataFrame(all_routes_df, geometry=geometry)\n",
        "all_routes_gdf.set_crs(epsg=4326, inplace=True)"
      ],
      "metadata": {
        "id": "lHhYWD-RKyZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if all_routes_gdf.crs is None:\n",
        "    # Replace 'EPSG:4326' with the correct CRS if known\n",
        "    all_routes_gdf.set_crs(epsg=4326, inplace=True)\n",
        "\n",
        "# Define the path to the shapefile\n",
        "shapefile_path = 'zone_zone.SHP'\n",
        "\n",
        "# Check if the shapefile exists\n",
        "if not os.path.exists(shapefile_path):\n",
        "    raise FileNotFoundError(f\"Shapefile not found: {shapefile_path}\")\n",
        "\n",
        "# Load the shapefile using Fiona\n",
        "with fiona.open(shapefile_path) as shapefile:\n",
        "    # Ensure shapefile has a CRS\n",
        "    if shapefile.crs is None:\n",
        "        # Replace 'EPSG:4326' with the correct CRS if known\n",
        "        shapefile_crs = 'EPSG:4326'\n",
        "    else:\n",
        "        shapefile_crs = shapefile.crs\n",
        "\n",
        "    # Convert the shapefile features to GeoDataFrame\n",
        "    shapefile_gdf = gpd.GeoDataFrame.from_features(shapefile, crs=shapefile_crs)\n",
        "\n",
        "    # Filter the shapefile GeoDataFrame based on the 'NO' attribute\n",
        "    shapefile_filtered = shapefile_gdf[(shapefile_gdf['NO'] >= 1000) & (shapefile_gdf['NO'] <= 23999)]\n",
        "\n",
        "    # Ensure CRS match\n",
        "    if shapefile_filtered.crs != all_routes_gdf.crs:\n",
        "        shapefile_filtered = shapefile_filtered.to_crs(geom.crs)\n",
        "\n",
        "    # Initialize the new columns with None values\n",
        "    all_routes_gdf['first_coord_shape_index'] = None\n",
        "    all_routes_gdf['second_coord_shape_index'] = None\n",
        "\n",
        "    # Convert filtered shapes to Shapely geometries\n",
        "    filtered_shapes = [(shape(geom_shape), shape_id) for geom_shape, shape_id in zip(shapefile_filtered.geometry, shapefile_filtered.index)]\n",
        "\n",
        "    # Iterate through the rows of the GeoDataFrame\n",
        "    for idx, row in all_routes_gdf.iterrows():\n",
        "        # Extract the coordinates from the geometry column\n",
        "        coords = list(row.geometry.coords)\n",
        "        first_coord = Point(coords[0])\n",
        "        second_coord = Point(coords[-1])\n",
        "        \"\"\"if idx <= 10:\n",
        "          print(coords)\n",
        "          print(first_coord)\n",
        "          print(second_coord)\"\"\"\n",
        "\n",
        "        # Check which shape the first coordinate falls within\n",
        "        first_match = next((shape_id for geom_shape, shape_id in filtered_shapes if geom_shape.contains(first_coord)), None)\n",
        "        all_routes_gdf.at[idx, 'first_coord_shape_index'] = first_match\n",
        "\n",
        "        # Check which shape the second coordinate falls within\n",
        "        second_match = next((shape_id for geom_shape, shape_id in filtered_shapes if geom_shape.contains(second_coord)), None)\n",
        "        all_routes_gdf.at[idx, 'second_coord_shape_index'] = second_match\n",
        "\n",
        "        if idx%1000000 == 0:\n",
        "            print(idx)"
      ],
      "metadata": {
        "id": "yR1qjObviL_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_routes_gdf2 = copy.deepcopy(all_routes_gdf)\n",
        "all_routes_gdf2.set_crs(epsg=4326, inplace=True)\n",
        "\n",
        "# Define the path to the shapefile\n",
        "shapefile_path = 'zone_zone.SHP'\n",
        "\n",
        "# Check if the shapefile exists\n",
        "if not os.path.exists(shapefile_path):\n",
        "    raise FileNotFoundError(f\"Shapefile not found: {shapefile_path}\")\n",
        "\n",
        "# Load the shapefile using Fiona\n",
        "with fiona.open(shapefile_path) as shapefile:\n",
        "    # Ensure the shapefile has a CRS\n",
        "    shapefile_crs = shapefile.crs if shapefile.crs else 'EPSG:4326'\n",
        "\n",
        "    # Convert the shapefile features to a GeoDataFrame\n",
        "    shapefile_gdf = gpd.GeoDataFrame.from_features(shapefile, crs=shapefile_crs)\n",
        "\n",
        "    # Filter the shapefile GeoDataFrame based on the 'NO' attribute\n",
        "    shapefile_filtered = shapefile_gdf[(shapefile_gdf['NO'] >= 1000) & (shapefile_gdf['NO'] <= 23999)]\n",
        "\n",
        "    # Ensure CRS match\n",
        "    if shapefile_filtered.crs != all_routes_gdf2.crs:\n",
        "        shapefile_filtered = shapefile_filtered.to_crs(all_routes_gdf2.crs)\n",
        "\n",
        "    # Initialize the new columns with None values\n",
        "    all_routes_gdf2['first_coord_shape_index'] = None\n",
        "    all_routes_gdf2['second_coord_shape_index'] = None\n",
        "\n",
        "    # Convert filtered shapes to Shapely geometries\n",
        "    filtered_shapes = [(shape(geom_shape), shape_id) for geom_shape, shape_id in zip(shapefile_filtered.geometry, shapefile_filtered.index)]\n",
        "\n",
        "    # Initialize a dictionary to cache the shape indices for coordinates\n",
        "    coord_shape_index_cache = {}\n",
        "\n",
        "    # Iterate through the rows of the GeoDataFrame\n",
        "    for idx, row in all_routes_gdf2.iterrows():\n",
        "        # Extract the coordinates from the geometry column\n",
        "        coords = list(row.geometry.coords)\n",
        "        first_coord = Point(coords[0][1], coords[0][0])  # Extract as tuple (x, y)\n",
        "        second_coord = Point(coords[-1][1], coords[-1][0])\n",
        "\n",
        "        # Check if the first coordinate is already in the cache\n",
        "        if first_coord in coord_shape_index_cache:\n",
        "            first_match = coord_shape_index_cache[first_coord]\n",
        "        else:\n",
        "            # Determine which shape the first coordinate falls within\n",
        "            first_match = next((shape_id for geom_shape, shape_id in filtered_shapes if geom_shape.contains(first_coord)), None)\n",
        "            coord_shape_index_cache[first_coord] = first_match\n",
        "\n",
        "        all_routes_gdf2.at[idx, 'first_coord_shape_index'] = first_match\n",
        "\n",
        "        # Check if the second coordinate is already in the cache\n",
        "        if second_coord in coord_shape_index_cache:\n",
        "            second_match = coord_shape_index_cache[second_coord]\n",
        "        else:\n",
        "            # Determine which shape the second coordinate falls within\n",
        "            second_match = next((shape_id for geom_shape, shape_id in filtered_shapes if geom_shape.contains(second_coord)), None)\n",
        "            coord_shape_index_cache[second_coord] = second_match\n",
        "\n",
        "        all_routes_gdf2.at[idx, 'second_coord_shape_index'] = second_match\n",
        "\n",
        "        if idx%1000000 == 0:\n",
        "            print(idx)"
      ],
      "metadata": {
        "id": "ab6TNf4Ee42P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the new columns with None values in the GeoDataFrame\n",
        "all_routes_gdf['first_coord_shape_index'] = None\n",
        "all_routes_gdf['second_coord_shape_index'] = None\n",
        "\n",
        "# Iterate through the rows of the GeoDataFrame\n",
        "for idx, row in all_routes_gdf.iterrows():\n",
        "    # Extract the coordinates as tuples from the geometry column\n",
        "    coords = list(row.geometry.coords)\n",
        "    first_coord = (coords[0][1], coords[0][0])  # Extract as tuple (x, y)\n",
        "    second_coord = (coords[-1][1], coords[-1][0])  # Extract as tuple (x, y)\n",
        "\n",
        "    # Lookup the shape indices in node_shape_dict\n",
        "    first_shape_index = node_shape_dict.get(first_coord, None)\n",
        "    second_shape_index = node_shape_dict.get(second_coord, None)\n",
        "\n",
        "    # Assign the shape indices to the corresponding columns in the GeoDataFrame\n",
        "    all_routes_gdf.at[idx, 'first_coord_shape_index'] = first_shape_index\n",
        "    all_routes_gdf.at[idx, 'second_coord_shape_index'] = second_shape_index\n",
        "\n",
        "    if idx%1000000 == 0:\n",
        "            print(idx)"
      ],
      "metadata": {
        "id": "6_b91lz4JjNM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_routes_gdf2.head()"
      ],
      "metadata": {
        "id": "W2NtYT4SXZN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_gdf = all_routes_gdf.head(1_000_000).copy()\n",
        "\n",
        "test_gdf.set_crs(epsg=4326, inplace=True)\n",
        "\n",
        "# Define the path to the shapefile\n",
        "shapefile_path = 'zone_zone.SHP'\n",
        "\n",
        "# Check if the shapefile exists\n",
        "if not os.path.exists(shapefile_path):\n",
        "    raise FileNotFoundError(f\"Shapefile not found: {shapefile_path}\")\n",
        "\n",
        "# Load the shapefile using Fiona\n",
        "with fiona.open(shapefile_path) as shapefile:\n",
        "    # Ensure the shapefile has a CRS\n",
        "    shapefile_crs = shapefile.crs if shapefile.crs else 'EPSG:4326'\n",
        "\n",
        "    # Convert the shapefile features to a GeoDataFrame\n",
        "    shapefile_gdf = gpd.GeoDataFrame.from_features(shapefile, crs=shapefile_crs)\n",
        "\n",
        "    # Filter the shapefile GeoDataFrame based on the 'NO' attribute\n",
        "    shapefile_filtered = shapefile_gdf[(shapefile_gdf['NO'] >= 1000) & (shapefile_gdf['NO'] <= 23999)]\n",
        "\n",
        "    # Ensure CRS match\n",
        "    if shapefile_filtered.crs != test_gdf.crs:\n",
        "        shapefile_filtered = shapefile_filtered.to_crs(test_gdf.crs)\n",
        "\n",
        "    # Initialize the new columns with None values\n",
        "    test_gdf['first_coord_shape_index'] = None\n",
        "    test_gdf['second_coord_shape_index'] = None\n",
        "\n",
        "    # Convert filtered shapes to Shapely geometries\n",
        "    filtered_shapes = [(shape(geom_shape), shape_id) for geom_shape, shape_id in zip(shapefile_filtered.geometry, shapefile_filtered.index)]\n",
        "\n",
        "    # Initialize a dictionary to cache the shape indices for coordinates\n",
        "    coord_shape_index_cache = {}\n",
        "\n",
        "    # Iterate through the rows of the GeoDataFrame\n",
        "    for idx, row in test_gdf.iterrows():\n",
        "        # Extract the coordinates from the geometry column\n",
        "        coords = list(row.geometry.coords)\n",
        "        first_coord = Point(coords[0])\n",
        "        second_coord = Point(coords[-1])\n",
        "\n",
        "        # Check if the first coordinate is already in the cache\n",
        "        if first_coord in coord_shape_index_cache:\n",
        "            first_match = coord_shape_index_cache[first_coord]\n",
        "        else:\n",
        "            # Determine which shape the first coordinate falls within\n",
        "            first_match = next((shape_id for geom_shape, shape_id in filtered_shapes if geom_shape.contains(first_coord)), None)\n",
        "            coord_shape_index_cache[first_coord] = first_match\n",
        "\n",
        "        test_gdf.at[idx, 'first_coord_shape_index'] = first_match\n",
        "\n",
        "        # Check if the second coordinate is already in the cache\n",
        "        if second_coord in coord_shape_index_cache:\n",
        "            second_match = coord_shape_index_cache[second_coord]\n",
        "        else:\n",
        "            # Determine which shape the second coordinate falls within\n",
        "            second_match = next((shape_id for geom_shape, shape_id in filtered_shapes if geom_shape.contains(second_coord)), None)\n",
        "            coord_shape_index_cache[second_coord] = second_match\n",
        "\n",
        "        test_gdf.at[idx, 'second_coord_shape_index'] = second_match\n",
        "\n",
        "        if idx%100000 == 0:\n",
        "            print(idx)"
      ],
      "metadata": {
        "id": "sFLvvgj4EVIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_gdf = all_routes_gdf.head(1_000_000).copy()\n",
        "# Initialize the new columns with None values in the GeoDataFrame\n",
        "test_gdf['first_coord_shape_index'] = None\n",
        "test_gdf['second_coord_shape_index'] = None\n",
        "\n",
        "# Iterate through the rows of the GeoDataFrame\n",
        "for idx, row in test_gdf.iterrows():\n",
        "    # Extract the coordinates as tuples from the geometry column\n",
        "    coords = list(row.geometry.coords)\n",
        "    first_coord = (coords[0][1], coords[0][0])  # Extract as tuple (x, y)\n",
        "    second_coord = (coords[-1][1], coords[-1][0])  # Extract as tuple (x, y)\n",
        "\n",
        "    # Lookup the shape indices in node_shape_dict\n",
        "    first_shape_index = node_shape_dict.get(first_coord, None)\n",
        "    second_shape_index = node_shape_dict.get(second_coord, None)\n",
        "\n",
        "    # Assign the shape indices to the corresponding columns in the GeoDataFrame\n",
        "    test_gdf.at[idx, 'first_coord_shape_index'] = first_shape_index\n",
        "    test_gdf.at[idx, 'second_coord_shape_index'] = second_shape_index\n",
        "\n",
        "    if idx%100000 == 0:\n",
        "            print(idx)"
      ],
      "metadata": {
        "id": "9KES7rxVTgPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_gdf.head()\n",
        "#print(coord_shape_index_cache)"
      ],
      "metadata": {
        "id": "rlZ8PfxcBA8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ODhoz = copy.deepcopy(all_routes_gdf)\n",
        "ODhoz.drop(columns = ['first_coord','second_coord'])\n",
        "output_file_path = 'OD_tablahoz.xlsx'\n",
        "ODhoz.to_excel(output_file_path, index=False)"
      ],
      "metadata": {
        "id": "oehr9gROqoBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OD_matrix = np.zeros((matrix_size, matrix_size))\n",
        "# Iterate through all_routes_gdf to update the matrix\n",
        "for idx, row in all_routes_gdf.iterrows():\n",
        "    first_index = row['first_coord_shape_index']\n",
        "    second_index = row['second_coord_shape_index']\n",
        "    capacity = row['capacity']\n",
        "\n",
        "    # Ensure the indices are valid (not None or out of range)\n",
        "    if first_index is not None and second_index is not None:\n",
        "        OD_matrix[first_index][second_index] += capacity\n",
        "OD_matrix"
      ],
      "metadata": {
        "id": "P-nB8PY1Al72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OD_matrix2 = np.zeros((matrix_size, matrix_size))\n",
        "# Iterate through all_routes_gdf to update the matrix\n",
        "for idx, row in all_routes_gdf2.iterrows():\n",
        "    first_index = row['first_coord_shape_index']\n",
        "    second_index = row['second_coord_shape_index']\n",
        "    capacity = row['capacity']\n",
        "\n",
        "    # Ensure the indices are valid (not None or out of range)\n",
        "    if first_index is not None and second_index is not None:\n",
        "        OD_matrix2[first_index][second_index] += capacity\n",
        "OD_matrix2"
      ],
      "metadata": {
        "id": "AhpdUZDUU4cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def are_matrices_equal(matrix1, matrix2):\n",
        "    # Check if the dimensions of the matrices are the same\n",
        "    if len(matrix1) != len(matrix2) or len(matrix1[0]) != len(matrix2[0]):\n",
        "        return False\n",
        "\n",
        "    # Iterate through each element and compare\n",
        "    for i in range(len(matrix1)):\n",
        "        for j in range(len(matrix1[0])):\n",
        "            if matrix1[i][j] != matrix2[i][j]:\n",
        "                return False\n",
        "\n",
        "    return True\n",
        "if are_matrices_equal(OD_matrix, OD_matrix2):\n",
        "    print(\"The matrices are equal.\")\n",
        "else:\n",
        "    print(\"The matrices are not equal.\")"
      ],
      "metadata": {
        "id": "725D4L1WZR4L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}